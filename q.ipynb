{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp312-cp312-win_amd64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.13.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.3.1-cp312-cp312-win_amd64.whl (159.7 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.13.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-win_amd64.whl (17 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: tbb, mpmath, intel-openmp, typing-extensions, sympy, networkx, mkl, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-2.1.5 filelock-3.15.4 fsspec-2024.6.1 intel-openmp-2021.4.0 jinja2-3.1.4 mkl-2021.4.0 mpmath-1.3.0 networkx-3.3 sympy-1.13.1 tbb-2021.13.0 torch-2.3.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import json\n",
    "from transformers import BertTokenizer, BertModel, DPRQuestionEncoderTokenizer, DPRQuestionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, DPRQuestionEncoderTokenizer, DPRQuestionEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "def initialize_bert():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer, model\n",
    "\n",
    "def initialize_dpr():\n",
    "    dpr_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "    dpr_model = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "    return dpr_tokenizer, dpr_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\aaaaa\\bbot1\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--facebook--dpr-question_encoder-single-nq-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-07-23 20:00:57,936 - INFO - Processing node: {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]}\n",
      "2024-07-23 20:00:57,970 - WARNING - Node {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]} does not have 'content' or content is empty.\n",
      "2024-07-23 20:00:58,082 - ERROR - Error processing node {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]}: 'dict' object has no attribute 'children'\n",
      "2024-07-23 20:00:58,120 - INFO - Processing node: {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]}\n",
      "2024-07-23 20:00:58,152 - WARNING - Node {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]} does not have 'content' or content is empty.\n",
      "2024-07-23 20:00:58,200 - ERROR - Error processing node {'id': 0, 'title': 'Root', 'content': None, 'children': [{'id': 1, 'title': 'Cover', 'content': '', 'children': []}, {'id': 2, 'title': 'Copyright', 'content': '978-1-492-03264-9\\n[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\\nby Aurélien Géron\\nCopyright © 2019 Aurélien Géron. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com .\\nEditor:  Nicole Tache\\nInterior Designer:  David FutatoCover Designer:  Karen Montgomery\\nIllustrator:  Rebecca Demarest\\nJune 2019:  Second Edition\\nRevision History for the Early Release\\n2018-11-05: First Release\\n2019-01-24: Second Release\\n2019-03-07: Third Release\\n2019-03-29: Fourth Release\\n2019-04-22: Fifth Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\\nScikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\\nMedia, Inc.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\\nor reliance on this work. Use of the information and instructions contained in this work is at your own\\nrisk. If any code samples or other technology this work contains or describes is subject to open source\\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'children': []}, {'id': 3, 'title': 'Table of Contents', 'content': 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\nPart I. The Fundamentals of Machine Learning\\n1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\\nWhat Is Machine Learning?                                                                                           4\\nWhy Use Machine Learning?                                                                                         4\\nTypes of Machine Learning Systems                                                                             8\\nSupervised/Unsupervised Learning                                                                           8\\nBatch and Online Learning                                                                                       15\\nInstance-Based Versus Model-Based Learning                                                      18\\nMain Challenges of Machine Learning                                                                       24\\nInsufficient Quantity of Training Data                                                                   24\\nNonrepresentative Training Data                                                                            26\\nPoor-Quality Data                                                                                                      27\\nIrrelevant Features                                                                                                     27\\nOverfitting the Training Data                                                                                   28\\nUnderfitting the Training Data                                                                                30\\nStepping Back                                                                                                             30\\nTesting and Validating                                                                                                   31\\nHyperparameter Tuning and Model Selection                                                       32\\nData Mismatch                                                                                                           33\\nExercises                                                                                                                          34\\n2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\\nWorking with Real Data                                                                                                38\\nLook at the Big Picture                                                                                                  39\\niii', 'children': []}, {'id': 4, 'title': 'Preface', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': [{'id': 5, 'title': 'The Machine Learning Tsunami', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 6, 'title': 'Machine Learning in Your Projects', 'content': '1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\\n2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\\nsince the 1990s, although they were not as general purpose.Preface\\nThe Machine Learning Tsunami\\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\\n(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\\nwas widely considered impossible at the time,2 and most researchers had abandoned\\nthe idea since the 1990s. This paper revived the interest of the scientific community\\nand before long many new papers demonstrated that Deep Learning was not only\\npossible, but capable of mind-blowing achievements that no other Machine Learning\\n(ML) technique could hope to match (with the help of tremendous computing power\\nand great amounts of data). This enthusiasm soon extended to many other areas of\\nMachine Learning.\\nFast-forward 10 years and Machine Learning has conquered the industry: it is now at\\nthe heart of much of the magic in today’s high-tech products, ranking your web\\nsearch results, powering your smartphone’s speech recognition, recommending vid‐\\neos, and beating the world champion at the game of Go. Before you know it, it will be\\ndriving your car.\\nMachine Learning in Your Projects\\nSo naturally you are excited about Machine Learning and you would love to join the\\nparty!\\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec‐\\nognize faces? Or learn to walk around?\\nxi', 'children': []}, {'id': 7, 'title': 'Objective and Approach', 'content': 'Or maybe your company has tons of data (user logs, financial data, production data,\\nmachine sensor data, hotline stats, HR reports, etc.), and more than likely you could\\nunearth some hidden gems if you just knew where to look; for example:\\n•Segment customers and find the best marketing strategy for each group\\n•Recommend products for each client based on what similar clients bought\\n•Detect which transactions are likely to be fraudulent\\n•Forecast next year’s revenue\\n•And more\\nWhatever the reason, you have decided to learn Machine Learning and implement it\\nin your projects. Great idea!\\nObjective and Approach\\nThis book assumes that you know close to nothing about Machine Learning. Its goal\\nis to give you the concepts, the intuitions, and the tools you need to actually imple‐\\nment programs capable of learning from data .\\nWe will cover a large number of techniques, from the simplest and most commonly\\nused (such as linear regression) to some of the Deep Learning techniques that regu‐\\nlarly win competitions.\\nRather than implementing our own toy versions of each algorithm, we will be using\\nactual production-ready Python frameworks:\\n•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\\nrithms efficiently, so it makes for a great entry point to learn Machine Learning.\\n•TensorFlow  is a more complex library for distributed numerical computation. It\\nmakes it possible to train and run very large neural networks efficiently by dis‐\\ntributing the computations across potentially hundreds of multi-GPU servers.\\nTensorFlow was created at Google and supports many of their large-scale\\nMachine Learning applications. It was open sourced in November 2015.\\n•Keras  is a high level Deep Learning API that makes it very simple to train and\\nrun neural networks. It can run on top of either TensorFlow, Theano or Micro‐\\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\\nown implementation of this API, called tf.keras , which provides support for some\\nadvanced TensorFlow features (e.g., to efficiently load data).\\nThe book favors a hands-on approach, growing an intuitive understanding of\\nMachine Learning through concrete working examples and just a little bit of theory.\\nWhile you can read this book without picking up your laptop, we highly recommend\\nxii | Preface', 'children': []}, {'id': 8, 'title': 'Prerequisites', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 9, 'title': 'Roadmap', 'content': 'you experiment with the code examples available online as Jupyter notebooks at\\nhttps://github.com/ageron/handson-ml2 .\\nPrerequisites\\nThis book assumes that you have some Python programming experience and that you\\nare familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\\nMatplotlib .\\nAlso, if you care about what’s under the hood you should have a reasonable under‐\\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta‐\\ntistics).\\nIf you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\\ncial tutorial on python.org  is also quite good.\\nIf you have never used Jupyter, Chapter 2  will guide you through installation and the\\nbasics: it is a great tool to have in your toolbox.\\nIf you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra.\\nRoadmap\\nThis book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\\ncovers the following topics:\\n•What is Machine Learning? What problems does it try to solve? What are the\\nmain categories and fundamental concepts of Machine Learning systems?\\n•The main steps in a typical Machine Learning project.\\n•Learning by fitting a model to data.\\n•Optimizing a cost function.\\n•Handling, cleaning, and preparing data.\\n•Selecting and engineering features.\\n•Selecting a model and tuning hyperparameters using cross-validation.\\n•The main challenges of Machine Learning, in particular underfitting and overfit‐\\nting (the bias/variance tradeoff).\\n•Reducing the dimensionality of the training data to fight the curse of dimension‐\\nality.\\n•Other unsupervised learning techniques, including clustering, density estimation\\nand anomaly detection.\\nPreface | xiii', 'children': []}, {'id': 10, 'title': 'Other Resources', 'content': 'Part II, Neural Networks and Deep Learning , covers the following topics:\\n•What are neural nets? What are they good for?\\n•Building and training neural nets using TensorFlow and Keras.\\n•The most important neural net architectures: feedforward neural nets, convolu‐\\ntional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\\nand generative adversarial networks (GANs).\\n•Techniques for training deep neural nets.\\n•Scaling neural networks for large datasets.\\n•Learning strategies with Reinforcement Learning.\\n•Handling uncertainty with Bayesian Deep Learning.\\nThe first part is based mostly on Scikit-Learn while the second part uses TensorFlow\\nand Keras.\\nDon’t jump into deep waters too hastily: while Deep Learning is no\\ndoubt one of the most exciting areas in Machine Learning, you\\nshould master the fundamentals first. Moreover, most problems\\ncan be solved quite well using simpler techniques such as Random\\nForests and Ensemble methods (discussed in Part I ). Deep Learn‐\\ning is best suited for complex problems such as image recognition,\\nspeech recognition, or natural language processing, provided you\\nhave enough data, computing power, and patience.\\nOther Resources\\nMany resources are available to learn about Machine Learning. Andrew Ng’s ML\\ncourse on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\\nLearning  are amazing, although they both require a significant time investment\\n(think months).\\nThere are also many interesting websites about Machine Learning, including of\\ncourse Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora .\\nFinally, the Deep Learning website  has a good list of resources to learn more.\\nOf course there are also many other introductory books about Machine Learning, in\\nparticular:\\n•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\\nmentals of Machine Learning, and implements some of the main algorithms in\\npure Python (from scratch, as the name suggests).\\nPreface | xv', 'children': []}, {'id': 11, 'title': 'Conventions Used in This Book', 'content': '•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\\nHall). This book is a great introduction to Machine Learning, covering a wide\\nrange of topics in depth, with code examples in Python (also from scratch, but\\nusing NumPy).\\n•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\\nintroduction to Machine Learning, this book leverages Python open source libra‐\\nries (Pylearn 2 and Theano).\\n•François Chollet, Deep Learning with Python  (Manning). A very practical book\\nthat covers a large range of topics in a clear and concise way, as you might expect\\nfrom the author of the excellent Keras library. It favors code examples over math‐\\nematical theory.\\n•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\\nData  (AMLBook). A rather theoretical approach to ML, this book provides deep\\ninsights, in particular on the bias/variance tradeoff (see Chapter 4 ).\\n•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\\nEdition  (Pearson). This is a great (and huge) book covering an incredible amount\\nof topics, including Machine Learning. It helps put ML into perspective.\\nFinally, a great way to learn is to join ML competition websites such as Kaggle.com\\nthis will allow you to practice your skills on real-world problems, with help and\\ninsights from some of the best ML professionals out there.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program ele‐\\nments such as variable or function names, databases, data types, environment\\nvariables, statements and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter‐\\nmined by context.\\nxvi | Preface', 'children': []}, {'id': 12, 'title': 'Code Examples', 'content': 'This element signifies a tip or suggestion.\\nThis element signifies a general note.\\nThis element indicates a warning or caution.\\nCode Examples\\nSupplemental material (code examples, exercises, etc.) is available for download at\\nhttps://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\\nSome of the code examples in the book leave out some repetitive sections, or details\\nthat are obvious or unrelated to Machine Learning. This keeps the focus on the\\nimportant parts of the code, and it saves space to cover more topics. However, if you\\nwant the full code examples, they are all available in the Jupyter notebooks.\\nNote that when the code examples display some outputs, then these code examples\\nare shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\\nguish the code from the outputs. For example, this code defines the square()  func‐\\ntion then it computes and displays the square of 3:\\n>>> def square(x):\\n...     return x ** 2\\n...\\n>>> result = square(3)\\n>>> result\\n9\\nWhen code does not display anything, prompts are not used. However, the result may\\nsometimes be shown as a comment like this:\\ndef square(x):\\n    return x ** 2\\nresult = square(3)  # result is 9\\nPreface | xvii', 'children': []}, {'id': 13, 'title': 'Using Code Examples', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 14, 'title': \"O'Reilly Safari\", 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 15, 'title': 'How to Contact Us', 'content': 'Using Code Examples\\nThis book is here to help you get your job done. In general, if example code is offered\\nwith this book, you may use it in your programs and documentation. Y ou do not\\nneed to contact us for permission unless you’re reproducing a significant portion of\\nthe code. For example, writing a program that uses several chunks of code from this\\nbook does not require permission. Selling or distributing a CD-ROM of examples\\nfrom O’Reilly books does require permission. Answering a question by citing this\\nbook and quoting example code does not require permission. Incorporating a signifi‐\\ncant amount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the\\ntitle, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\\nScikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\\nAurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\\nside fair use or the permission given above, feel free to contact us at permis‐\\nsions@oreilly.com .\\nO’Reilly Safari\\nSafari  (formerly Safari Books Online) is a membership-based\\ntraining and reference platform for enterprise, government,\\neducators, and individuals.\\nMembers have access to thousands of books, training videos, Learning Paths, interac‐\\ntive tutorials, and curated playlists from over 250 publishers, including O’Reilly\\nMedia, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\\nsional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\\nJohn Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\\nPress, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\\nCourse Technology, among others.\\nFor more information, please visit http://oreilly.com/safari .\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\nxviii | Preface', 'children': []}, {'id': 16, 'title': 'Changes in the Second Edition', 'content': '707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\\nwith-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\\nTo comment or ask technical questions about this book, send email to bookques‐\\ntions@oreilly.com .\\nFor more information about our books, courses, conferences, and news, see our web‐\\nsite at http://www.oreilly.com .\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on Y ouTube: http://www.youtube.com/oreillymedia\\nChanges in the Second Edition\\nThis second edition has five main objectives:\\n1.Cover additional topics: additional unsupervised learning techniques (including\\nclustering, anomaly detection, density estimation and mixture models), addi‐\\ntional techniques for training deep nets (including self-normalized networks),\\nadditional computer vision techniques (including the Xception, SENet, object\\ndetection with YOLO, and semantic segmentation using R-CNN), handling\\nsequences using CNNs (including WaveNet), natural language processing using\\nRNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\\nsorFlow models, and more.\\n2.Update the book to mention some of the latest results from Deep Learning\\nresearch.\\n3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\\nmentation of the Keras API (called tf.keras) whenever possible, to simplify the\\ncode examples.\\n4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\\ndas, Matplotlib and other libraries.\\n5.Clarify some sections and fix some errors, thanks to plenty of great feedback\\nfrom readers.\\nSome chapters were added, others were rewritten and a few were reordered. Table P-1\\nshows the mapping between the 1st edition chapters and the 2nd edition chapters:\\nPreface | xix', 'children': []}, {'id': 17, 'title': 'Acknowledgments', 'content': '3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\\nNever in my wildest dreams did I imagine that the first edition of this book would get\\nsuch a large audience. I received so many messages from readers, many asking ques‐\\ntions, some kindly pointing out errata, and most sending me encouraging words. I\\ncannot express how grateful I am to all these readers for their tremendous support.\\nThank you all so very much! Please do not hesitate to file issues on github  if you find\\nerrors in the code examples (or just to ask questions), or to submit errata  if you find\\nerrors in the text. Some readers also shared how this book helped them get their first\\njob, or how it helped them solve a concrete problem they were working on: I find\\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\\nyou could share your story with me, either privately (e.g., via LinkedIn ) or publicly\\n(e.g., in an Amazon review ).\\nI am also incredibly thankful to all the amazing people who took time out of their\\nbusy lives to review my book with such care. In particular, I would like to thank Fran‐\\nçois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\\nme some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\\nedition, having its author review the book was invaluable. I highly recommend Fran‐\\nçois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\\ndepth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\\nevery chapter of this 2nd edition and gave me excellent feedback.\\nThis book also benefited from plenty of help from members of the TensorFlow team,\\nin particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\\npatched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\\nSusano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\\nvan, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\\nmel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\\nRyan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\\nChargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\\nyou to all of you, and to all other members of the TensorFlow team. Not just for your\\nhelp, but also for making such a great library.\\nBig thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\\neral errors while he was writing the Korean translation of the 1st edition of this book.\\nHe also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\\ndocumentation. I do not speak Korean, but judging by the quality of his feedback, all\\nhis translations must be truly excellent! Moreover, he kindly contributed some of the\\nsolutions to the exercises in this book.\\nPreface | xxiii', 'children': []}]}, {'id': 18, 'title': 'Part I. The Fundamentals of Machine Learning', 'content': 'PART I\\nThe Fundamentals of\\nMachine Learning', 'children': [{'id': 19, 'title': 'Chapter 1. The Machine Learning Landscape', 'content': 'CHAPTER 1\\nThe Machine Learning Landscape\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 1 in the final\\nrelease of the book.\\nWhen most people hear “Machine Learning, ” they picture a robot: a dependable but‐\\nler or a deadly Terminator depending on who you ask. But Machine Learning is not\\njust a futuristic fantasy, it’s already here. In fact, it has been around for decades in\\nsome specialized applications, such as Optical Character Recognition  (OCR). But the\\nfirst ML application that really became mainstream, improving the lives of hundreds\\nof millions of people, took over the world back in the 1990s: it was the spam filter .\\nNot exactly a self-aware Skynet, but it does technically qualify as Machine Learning\\n(it has actually learned so well that you seldom need to flag an email as spam any‐\\nmore). It was followed by hundreds of ML applications that now quietly power hun‐\\ndreds of products and features that you use regularly, from better recommendations\\nto voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it\\nmean for a machine to learn  something? If I download a copy of Wikipedia, has my\\ncomputer really “learned” something? Is it suddenly smarter? In this chapter we will\\nstart by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a\\nlook at the map and learn about the main regions and the most notable landmarks:\\nsupervised versus unsupervised learning, online versus batch learning, instance-\\nbased versus model-based learning. Then we will look at the workflow of a typical ML\\nproject, discuss the main challenges you may face, and cover how to evaluate and\\nfine-tune a Machine Learning system.\\n3', 'children': [{'id': 20, 'title': 'What Is Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 21, 'title': 'Why Use Machine Learning?', 'content': 'This chapter introduces a lot of fundamental concepts (and jargon) that every data\\nscientist should know by heart. It will be a high-level overview (the only chapter\\nwithout much code), all rather simple, but you should make sure everything is\\ncrystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\\nget started!\\nIf you already know all the Machine Learning basics, you may want\\nto skip directly to Chapter 2 . If you are not sure, try to answer all\\nthe questions listed at the end of the chapter before moving on.\\nWhat Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can\\nlearn from data .\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn\\nwithout being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T\\nand some performance measure P , if its performance on T, as measured by P , improves\\nwith experience E.\\n—Tom Mitchell, 1997\\nFor example, your spam filter is a Machine Learning program that can learn to flag\\nspam given examples of spam emails (e.g., flagged by users) and examples of regular\\n(nonspam, also called “ham”) emails. The examples that the system uses to learn are\\ncalled the training set . Each training example is called a training instance  (or sample ).\\nIn this case, the task T is to flag spam for new emails, the experience E is the training\\ndata , and the performance measure P needs to be defined; for example, you can use\\nthe ratio of correctly classified emails. This particular performance measure is called\\naccuracy  and it is often used in classification tasks.\\nIf you just download a copy of Wikipedia, your computer has a lot more data, but it is\\nnot suddenly better at any task. Thus, it is not Machine Learning.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techni‐\\nques ( Figure 1-1 ):\\n4 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 22, 'title': 'Types of Machine Learning Systems', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 23, 'title': 'Supervised/Unsupervised Learning', 'content': 'Types of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to\\nclassify them in broad categories based on:\\n•Whether or not they are trained with human supervision (supervised, unsuper‐\\nvised, semisupervised, and Reinforcement Learning)\\n•Whether or not they can learn incrementally on the fly (online versus batch\\nlearning)\\n•Whether they work by simply comparing new data points to known data points,\\nor instead detect patterns in the training data and build a predictive model, much\\nlike scientists do (instance-based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For\\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\\nwork model trained using examples of spam and ham; this makes it an online, model-\\nbased, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of\\nsupervision they get during training. There are four major categories: supervised\\nlearning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\\ning.\\nSupervised learning\\nIn supervised learning , the training data you feed to the algorithm includes the desired\\nsolutions, called labels  (Figure 1-5 ).\\nFigure 1-5. A labeled training set for supervised learning (e.g., spam classification)\\n8 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 24, 'title': 'Batch and Online Learning', 'content': 'Figure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn\\nhow to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\\nat the game of Go. It learned its winning policy by analyzing millions of games, and\\nthen playing many games against itself. Note that learning was turned off during the\\ngames against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the\\nsystem can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning , the system is incapable of learning incrementally: it must be trained\\nusing all the available data. This will generally take a lot of time and computing\\nresources, so it is typically done offline. First the system is trained, and then it is\\nlaunched into production and runs without learning anymore; it just applies what it\\nhas learned. This is called offline  learning .\\nIf you want a batch learning system to know about new data (such as a new type of\\nspam), you need to train a new version of the system from scratch on the full dataset\\n(not just the new data, but also the old data), then stop the old system and replace it\\nwith the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine\\nLearning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\\nTypes of Machine Learning Systems | 15', 'children': []}, {'id': 25, 'title': 'Instance-Based Versus Model-Based Learning', 'content': 'results. To reduce this risk, you need to monitor your system closely and promptly\\nswitch learning off (and possibly revert to a previously working state) if you detect a\\ndrop in performance. Y ou may also want to monitor the input data and react to\\nabnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize .\\nMost Machine Learning tasks are about making predictions. This means that given a\\nnumber of training examples, the system needs to be able to generalize to examples it\\nhas never seen before. Having a good performance measure on the training data is\\ngood, but insufficient; the true goal is to perform well on new instances.\\nThere are two main approaches to generalization: instance-based learning and\\nmodel-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\\ncreate a spam filter this way, it would just flag all emails that are identical to emails\\nthat have already been flagged by users—not the worst solution, but certainly not the\\nbest.\\nInstead of just flagging emails that are identical to known spam emails, your spam\\nfilter could be programmed to also flag emails that are very similar to known spam\\nemails. This requires a measure of similarity  between two emails. A (very basic) simi‐\\nlarity measure between two emails could be to count the number of words they have\\nin common. The system would flag an email as spam if it has many words in com‐\\nmon with a known spam email.\\nThis is called instance-based learning : the system learns the examples by heart, then\\ngeneralizes to new cases by comparing them to the learned examples (or a subset of\\nthem), using a similarity measure. For example, in Figure 1-15  the new instance\\nwould be classified as a triangle because the majority of the most similar instances\\nbelong to that class.\\n18 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 26, 'title': 'Main Challenges of Machine Learning', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': [{'id': 27, 'title': 'Insufficient Quantity of Training Data', 'content': 'Main Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some\\ndata, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\\nwith examples of bad data.\\nInsufficient  Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\\nsay “apple” (possibly repeating this procedure a few times). Now the child is able to\\nrecognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\\ning algorithms to work properly. Even for very simple problems you typically need\\nthousands of examples, and for complex problems such as image or speech recogni‐\\ntion you may need millions of examples (unless you can reuse parts of an existing\\nmodel).\\n24 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 28, 'title': 'Nonrepresentative Training Data', 'content': 'Nonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the\\nnew cases you want to generalize to. This is true whether you use instance-based\\nlearning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not\\nperfectly representative; a few countries were missing. Figure 1-21  shows what the\\ndata looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is\\nrepresented by the dotted line. As you can see, not only does adding a few missing\\ncountries significantly alter the model, but it makes it clear that such a simple linear\\nmodel is probably never going to work well. It seems that very rich countries are not\\nhappier than moderately rich countries (in fact they seem unhappier), and conversely\\nsome poor countries seem happier than many rich countries.\\nBy using a nonrepresentative training set, we trained a model that is unlikely to make\\naccurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to general‐\\nize to. This is often harder than it sounds: if the sample is too small, you will have\\nsampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\\nsamples can be nonrepresentative if the sampling method is flawed. This is called\\nsampling bias .\\nA Famous Example of Sampling Bias\\nPerhaps the most famous example of sampling bias happened during the US presi‐\\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\\nanswers, and predicted with high confidence that Landon would get 57% of the votes.\\n26 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 29, 'title': 'Poor-Quality Data', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 30, 'title': 'Irrelevant Features', 'content': 'Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\\nsampling method:\\n•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\\nphone directories, lists of magazine subscribers, club membership lists, and the\\nlike. All of these lists tend to favor wealthier people, who are more likely to vote\\nRepublican (hence Landon).\\n•Second, less than 25% of the people who received the poll answered. Again, this\\nintroduces a sampling bias, by ruling out people who don’t care much about poli‐\\ntics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\\ncial type of sampling bias called nonresponse bias .\\nHere is another example: say you want to build a system to recognize funk music vid‐\\neos. One way to build your training set is to search “funk music” on Y ouTube and use\\nthe resulting videos. But this assumes that Y ouTube’s search engine returns a set of\\nvideos that are representative of all the funk music videos on Y ouTube. In reality, the\\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\\nyou will get a lot of “funk carioca” videos, which sound nothing like James Brown).\\nOn the other hand, how else can you get a large training set?\\nPoor-Quality Data\\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\\nquality measurements), it will make it harder for the system to detect the underlying\\npatterns, so your system is less likely to perform well. It is often well worth the effort\\nto spend time cleaning up your training data. The truth is, most data scientists spend\\na significant part of their time doing just that. For example:\\n•If some instances are clearly outliers, it may help to simply discard them or try to\\nfix the errors manually.\\n•If some instances are missing a few features (e.g., 5% of your customers did not\\nspecify their age), you must decide whether you want to ignore this attribute alto‐\\ngether, ignore these instances, fill in the missing values (e.g., with the median\\nage), or train one model with the feature and one model without it, and so on.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\\ning if the training data contains enough relevant features and not too many irrelevant\\nones. A critical part of the success of a Machine Learning project is coming up with a\\ngood set of features to train on. This process, called feature engineering , involves:\\nMain Challenges of Machine Learning | 27', 'children': []}, {'id': 31, 'title': 'Overfitting the Training Data', 'content': '•Feature selection : selecting the most useful features to train on among existing\\nfeatures.\\n•Feature extraction : combining existing features to produce a more useful one (as\\nwe saw earlier, dimensionality reduction algorithms can help).\\n•Creating new features by gathering new data.\\nNow that we have looked at many examples of bad data, let’s look at a couple of exam‐\\nples of bad algorithms.\\nOverfitting  the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. Y ou might be\\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\\nsomething that we humans do all too often, and unfortunately machines can fall into\\nthe same trap if we are not careful. In Machine Learning this is called overfitting : it\\nmeans that the model performs well on the training data, but it does not generalize\\nwell.\\nFigure 1-22  shows an example of a high-degree polynomial life satisfaction model\\nthat strongly overfits the training data. Even though it performs much better on the\\ntraining data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting  the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data,\\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\\nwill not generalize to new instances. For example, say you feed your life satisfaction\\nmodel many more attributes, including uninformative ones such as the country’s\\nname. In that case, a complex model may detect patterns like the fact that all coun‐\\ntries in the training data with a w in their name have a life satisfaction greater than 7:\\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\\n28 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 32, 'title': 'Underfitting the Training Data', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 33, 'title': 'Stepping Back', 'content': 'Figure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyper‐\\nparameter . A hyperparameter is a parameter of a learning algorithm (not of the\\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\\nto training and remains constant during training. If you set the regularization hyper‐\\nparameter to a very large value, you will get an almost flat model (a slope close to\\nzero); the learning algorithm will almost certainly not overfit the training data, but it\\nwill be less likely to find a good solution. Tuning hyperparameters is an important\\npart of building a Machine Learning system (you will see a detailed example in the\\nnext chapter).\\nUnderfitting  the Training Data\\nAs you might guess, underfitting  is the opposite of overfitting: it occurs when your\\nmodel is too simple to learn the underlying structure of the data. For example, a lin‐\\near model of life satisfaction is prone to underfit; reality is just more complex than\\nthe model, so its predictions are bound to be inaccurate, even on the training exam‐\\nples.\\nThe main options to fix this problem are:\\n•Selecting a more powerful model, with more parameters\\n•Feeding better features to the learning algorithm (feature engineering)\\n•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\\nparameter)\\nStepping Back\\nBy now you already know a lot about Machine Learning. However, we went through\\nso many concepts that you may be feeling a little lost, so let’s step back and look at the\\nbig picture:\\n30 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 34, 'title': 'Testing and Validating', 'content': '•Machine Learning is about making machines get better at some task by learning\\nfrom data, instead of having to explicitly code rules.\\n•There are many different types of ML systems: supervised or not, batch or online,\\ninstance-based or model-based, and so on.\\n•In a ML project you gather data in a training set, and you feed the training set to\\na learning algorithm. If the algorithm is model-based it tunes some parameters to\\nfit the model to the training set (i.e., to make good predictions on the training set\\nitself), and then hopefully it will be able to make good predictions on new cases\\nas well. If the algorithm is instance-based, it just learns the examples by heart and\\ngeneralizes to new instances by comparing them to the learned instances using a\\nsimilarity measure.\\n•The system will not perform well if your training set is too small, or if the data is\\nnot representative, noisy, or polluted with irrelevant features (garbage in, garbage\\nout). Lastly, your model needs to be neither too simple (in which case it will\\nunderfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you\\ndon’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\\ntune it if necessary. Let’s see how.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try\\nit out on new cases. One way to do that is to put your model in production and moni‐\\ntor how well it performs. This works well, but if your model is horribly bad, your\\nusers will complain—not the best idea.\\nA better option is to split your data into two sets: the training set  and the test set . As\\nthese names imply, you train your model using the training set, and you test it using\\nthe test set. The error rate on new cases is called the generalization error  (or out-of-\\nsample error ), and by evaluating your model on the test set, you get an estimate of this\\nerror. This value tells you how well your model will perform on instances it has never\\nseen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set)\\nbut the generalization error is high, it means that your model is overfitting the train‐\\ning data.\\nIt is common to use 80% of the data for training and hold out  20%\\nfor testing. However, this depends on the size of the dataset: if it\\ncontains 10 million instances, then holding out 1% means your test\\nset will contain 100,000 instances: that’s probably more than\\nenough to get a good estimate of the generalization error.\\nTesting and Validating | 31', 'children': [{'id': 35, 'title': 'Hyperparameter Tuning and Model Selection', 'content': 'Hyperparameter Tuning and Model Selection\\nSo evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\\ntating between two models (say a linear model and a polynomial model): how can\\nyou decide? One option is to train both and compare how well they generalize using\\nthe test set.\\nNow suppose that the linear model generalizes better, but you want to apply some \\nregularization to avoid overfitting. The question is: how do you choose the value of\\nthe regularization hyperparameter? One option is to train 100 different models using\\n100 different values for this hyperparameter. Suppose you find the best hyperparame‐\\nter value that produces a model with the lowest generalization error, say just 5% error.\\nSo you launch this model into production, but unfortunately it does not perform as\\nwell as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test\\nset, and you adapted the model and hyperparameters to produce the best model for\\nthat particular set . This means that the model is unlikely to perform as well on new\\ndata.\\nA common solution to this problem is called holdout validation : you simply hold out\\npart of the training set to evaluate several candidate models and select the best one.\\nThe new heldout set is called the validation set  (or sometimes the development set , or\\ndev set ). More specifically, you train multiple models with various hyperparameters\\non the reduced training set (i.e., the full training set minus the validation set), and\\nyou select the model that performs best on the validation set. After this holdout vali‐\\ndation process, you train the best model on the full training set (including the valida‐\\ntion set), and this gives you the final model. Lastly, you evaluate this final model on\\nthe test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then\\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\\nmistake. Conversely, if the validation set is too large, then the remaining training set\\nwill be much smaller than the full training set. Why is this bad? Well, since the final\\nmodel will be trained on the full training set, it is not ideal to compare candidate\\nmodels trained on a much smaller training set. It would be like selecting the fastest\\nsprinter to participate in a marathon. One way to solve this problem is to perform\\nrepeated cross-validation , using many small validation sets. Each model is evaluated\\nonce per validation set, after it is trained on the rest of the data. By averaging out all\\nthe evaluations of a model, we get a much more accurate measure of its performance.\\nHowever, there is a drawback: the training time is multiplied by the number of valida‐\\ntion sets.\\n32 | Chapter 1: The Machine Learning Landscape', 'children': []}, {'id': 36, 'title': 'Data Mismatch', 'content': '11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\\nIn some cases, it is easy to get a large amount of data for training, but it is not per‐\\nfectly representative of the data that will be used in production. For example, suppose\\nyou want to create a mobile app to take pictures of flowers and automatically deter‐\\nmine their species. Y ou can easily download millions of pictures of flowers on the\\nweb, but they won’t be perfectly representative of the pictures that will actually be\\ntaken using the app on a mobile device. Perhaps you only have 10,000 representative\\npictures (i.e., actually taken with the app). In this case, the most important rule to\\nremember is that the validation set and the test must be as representative as possible\\nof the data you expect to use in production, so they should be composed exclusively\\nof representative pictures: you can shuffle them and put half in the validation set, and\\nhalf in the test set (making sure that no duplicates or near-duplicates end up in both\\nsets). After training your model on the web pictures, if you observe that the perfor‐\\nmance of your model on the validation set is disappointing, you will not know\\nwhether this is because your model has overfit the training set, or whether this is just\\ndue to the mismatch between the web pictures and the mobile app pictures. One sol‐\\nution is to hold out part of the training pictures (from the web) in yet another set that\\nAndrew Ng calls the train-dev set . After the model is trained (on the training set, not\\non the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\\nthe model is not overfitting the training set, so if performs poorly on the validation\\nset, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\\nlem by preprocessing the web images to make them look more like the pictures that\\nwill be taken by the mobile app, and then retraining the model. Conversely, if the\\nmodel performs poorly on the train-dev set, then the model must have overfit the\\ntraining set, so you should try to simplify or regularize the model, get more training\\ndata and clean up the training data, as discussed earlier.\\nNo Free Lunch Theorem\\nA model is a simplified version of the observations. The simplifications are meant to\\ndiscard the superfluous details that are unlikely to generalize to new instances. How‐\\never, to decide what data to discard and what data to keep, you must make assump‐\\ntions . For example, a linear model makes the assumption that the data is\\nfundamentally linear and that the distance between the instances and the straight line\\nis just noise, which can safely be ignored.\\nIn a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\\nno assumption about the data, then there is no reason to prefer one model over any\\nother. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\\nTesting and Validating | 33', 'children': []}]}, {'id': 37, 'title': 'Exercises', 'content': 'model is a linear model, while for other datasets it is a neural network. There is no\\nmodel that is a priori  guaranteed to work better (hence the name of the theorem). The\\nonly way to know for sure which model is best is to evaluate them all. Since this is not\\npossible, in practice you make some reasonable assumptions about the data and you\\nevaluate only a few reasonable models. For example, for simple tasks you may evalu‐\\nate linear models with various levels of regularization, and for a complex problem you\\nmay evaluate various neural networks.\\nExercises\\nIn this chapter we have covered some of the most important concepts in Machine\\nLearning. In the next chapters we will dive deeper and write more code, but before we\\ndo, make sure you know how to answer the following questions:\\n1.How would you define Machine Learning?\\n2.Can you name four types of problems where it shines?\\n3.What is a labeled training set?\\n4.What are the two most common supervised tasks?\\n5.Can you name four common unsupervised tasks?\\n6.What type of Machine Learning algorithm would you use to allow a robot to\\nwalk in various unknown terrains?\\n7.What type of algorithm would you use to segment your customers into multiple\\ngroups?\\n8.Would you frame the problem of spam detection as a supervised learning prob‐\\nlem or an unsupervised learning problem?\\n9.What is an online learning system?\\n10.What is out-of-core learning?\\n11.What type of learning algorithm relies on a similarity measure to make predic‐\\ntions?\\n12.What is the difference between a model parameter and a learning algorithm’s\\nhyperparameter?\\n13.What do model-based learning algorithms search for? What is the most common\\nstrategy they use to succeed? How do they make predictions?\\n14.Can you name four of the main challenges in Machine Learning?\\n15.If your model performs great on the training data but generalizes poorly to new\\ninstances, what is happening? Can you name three possible solutions?\\n16.What is a test set and why would you want to use it?\\n34 | Chapter 1: The Machine Learning Landscape', 'children': []}]}, {'id': 38, 'title': 'Chapter 2. End-to-End Machine Learning Project', 'content': '1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\\nproject, not to learn anything about the real estate business.\\nCHAPTER 2\\nEnd-to-End Machine Learning Project\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 2 in the final\\nrelease of the book.\\nIn this chapter, you will go through an example project end to end, pretending to be a\\nrecently hired data scientist in a real estate company.1 Here are the main steps you will\\ngo through:\\n1.Look at the big picture.\\n2.Get the data.\\n3.Discover and visualize the data to gain insights.\\n4.Prepare the data for Machine Learning algorithms.\\n5.Select a model and train it.\\n6.Fine-tune your model.\\n7.Present your solution.\\n8.Launch, monitor, and maintain your system.\\n37', 'children': [{'id': 39, 'title': 'Working with Real Data', 'content': '2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\\n& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\\nWhen you are learning about Machine Learning it is best to actually experiment with\\nreal-world data, not just artificial datasets. Fortunately, there are thousands of open\\ndatasets to choose from, ranging across all sorts of domains. Here are a few places\\nyou can look to get data:\\n•Popular open data repositories:\\n—UC Irvine Machine Learning Repository\\n—Kaggle datasets\\n—Amazon’s AWS datasets\\n•Meta portals (they list open data repositories):\\n—http://dataportals.org/\\n—http://opendatamonitor.eu/\\n—http://quandl.com/\\n•Other pages listing many popular open data repositories:\\n—Wikipedia’s list of Machine Learning datasets\\n—Quora.com question\\n—Datasets subreddit\\nIn this chapter we chose the California Housing Prices dataset from the StatLib repos‐\\nitory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\\nsus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\\ntime), but it has many qualities for learning, so we will pretend it is recent data. We\\nalso added a categorical attribute and removed a few features for teaching purposes.\\n38 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 40, 'title': 'Look at the Big Picture', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': [{'id': 41, 'title': 'Frame the Problem', 'content': 'Figure 2-1. California housing prices\\nLook at the Big Picture\\nWelcome to Machine Learning Housing Corporation! The first task you are asked to\\nperform is to build a model of housing prices in California using the California cen‐\\nsus data. This data has metrics such as the population, median income, median hous‐\\ning price, and so on for each block group in California. Block groups are the smallest\\ngeographical unit for which the US Census Bureau publishes sample data (a block\\ngroup typically has a population of 600 to 3,000 people). We will just call them “dis‐\\ntricts” for short.\\nY our model should learn from this data and be able to predict the median housing\\nprice in any district, given all the other metrics.\\nSince you are a well-organized data scientist, the first thing you do\\nis to pull out your Machine Learning project checklist. Y ou can\\nstart with the one in ???; it should work reasonably well for most\\nMachine Learning projects but make sure to adapt it to your needs.\\nIn this chapter we will go through many checklist items, but we will\\nalso skip a few, either because they are self-explanatory or because\\nthey will be discussed in later chapters.\\nFrame the Problem\\nThe first question to ask your boss is what exactly is the business objective; building a\\nmodel is probably not the end goal. How does the company expect to use and benefit\\nLook at the Big Picture | 39', 'children': []}, {'id': 42, 'title': 'Select a Performance Measure', 'content': 'Select a Performance Measure\\nY our next step is to select a performance measure. A typical performance measure for\\nregression problems is the Root Mean Square Error (RMSE). It gives an idea of how\\nmuch error the system typically makes in its predictions, with a higher weight for\\nlarge errors. Equation 2-1  shows the mathematical formula to compute the RMSE.\\nEquation 2-1. Root Mean Square Error (RMSE)\\nRMSE X,h=1\\nm∑\\ni= 1m\\nhxi−yi2\\n42 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 43, 'title': 'Check the Assumptions', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}]}, {'id': 44, 'title': 'Get the Data', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': [{'id': 45, 'title': 'Create the Workspace', 'content': '5The latest version of Python 3 is recommended. Python 2.7+ may work too, but it is now deprecated, all major\\nscientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible.outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\\nvery well and is generally preferred.\\nCheck the Assumptions\\nLastly, it is good practice to list and verify the assumptions that were made so far (by\\nyou or others); this can catch serious issues early on. For example, the district prices\\nthat your system outputs are going to be fed into a downstream Machine Learning\\nsystem, and we assume that these prices are going to be used as such. But what if the\\ndownstream system actually converts the prices into categories (e.g., “cheap, ”\\n“medium, ” or “expensive”) and then uses those categories instead of the prices them‐\\nselves? In this case, getting the price perfectly right is not important at all; your sys‐\\ntem just needs to get the category right. If that’s so, then the problem should have\\nbeen framed as a classification task, not a regression task. Y ou don’t want to find this\\nout after working on a regression system for months.\\nFortunately, after talking with the team in charge of the downstream system, you are\\nconfident that they do indeed need the actual prices, not just categories. Great! Y ou’re\\nall set, the lights are green, and you can start coding now!\\nGet the Data\\nIt’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk\\nthrough the following code examples in a Jupyter notebook. The full Jupyter note‐\\nbook is available at https://github.com/ageron/handson-ml2 .\\nCreate the Workspace\\nFirst you will need to have Python installed. It is probably already installed on your\\nsystem. If not, you can get it at https://www.python.org/ .5\\nNext you need to create a workspace directory for your Machine Learning code and\\ndatasets. Open a terminal and type the following commands (after the $ prompts):\\n$ export ML_PATH=\"$HOME/ml\"      # You can change the path if you prefer\\n$ mkdir -p $ML_PATH\\nY ou will need a number of Python modules: Jupyter, NumPy, Pandas, Matplotlib, and\\nScikit-Learn. If you already have Jupyter running with all these modules installed,\\nyou can safely skip to “Download the Data” on page 49. If you don’t have them yet,\\nthere are many ways to install them (and their dependencies). Y ou can use your sys‐\\nGet the Data | 45', 'children': []}, {'id': 46, 'title': 'Download the Data', 'content': '10Y ou might also need to check legal constraints, such as private fields that should never be copied to unsafe\\ndatastores.\\n11In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\\nnotebook.Download the Data\\nIn typical environments your data would be available in a relational database (or\\nsome other common datastore) and spread across multiple tables/documents/files. To\\naccess it, you would first need to get your credentials and access authorizations,10 and\\nfamiliarize yourself with the data schema. In this project, however, things are much\\nsimpler: you will just download a single compressed file, housing.tgz , which contains a\\ncomma-separated value (CSV) file called housing.csv  with all the data.\\nY ou could use your web browser to download it, and run tar xzf housing.tgz  to\\ndecompress the file and extract the CSV file, but it is preferable to create a small func‐\\ntion to do that. It is useful in particular if data changes regularly, as it allows you to\\nwrite a small script that you can run whenever you need to fetch the latest data (or\\nyou can set up a scheduled job to do that automatically at regular intervals). Auto‐\\nmating the process of fetching the data is also useful if you need to install the dataset\\non multiple machines.\\nHere is the function to fetch the data:11\\nimport os\\nimport tarfile\\nfrom six.moves  import urllib\\nDOWNLOAD_ROOT  = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\\nHOUSING_PATH  = os.path.join(\"datasets\" , \"housing\" )\\nHOUSING_URL  = DOWNLOAD_ROOT  + \"datasets/housing/housing.tgz\"\\ndef fetch_housing_data (housing_url =HOUSING_URL , housing_path =HOUSING_PATH ):\\n    if not os.path.isdir(housing_path ):\\n        os.makedirs (housing_path )\\n    tgz_path  = os.path.join(housing_path , \"housing.tgz\" )\\n    urllib.request.urlretrieve (housing_url , tgz_path )\\n    housing_tgz  = tarfile.open(tgz_path )\\n    housing_tgz .extractall (path=housing_path )\\n    housing_tgz .close()\\nNow when you call fetch_housing_data() , it creates a datasets/housing  directory in\\nyour workspace, downloads the housing.tgz  file, and extracts the housing.csv  from it in\\nthis directory.\\nNow let’s load the data using Pandas. Once again you should write a small function to\\nload the data:\\nGet the Data | 49', 'children': []}, {'id': 47, 'title': 'Take a Quick Look at the Data Structure', 'content': 'import pandas as pd\\ndef load_housing_data (housing_path =HOUSING_PATH ):\\n    csv_path  = os.path.join(housing_path , \"housing.csv\" )\\n    return pd.read_csv (csv_path )\\nThis function returns a Pandas DataFrame object containing all the data.\\nTake a Quick Look at the Data Structure\\nLet’s take a look at the top five rows using the DataFrame’s head()  method (see\\nFigure 2-5 ).\\nFigure 2-5. Top five rows in the dataset\\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\\nscreenshot): longitude , latitude , housing_median_age , total_rooms , total_bed\\nrooms , population , households , median_income , median_house_value , and\\nocean_proximity .\\nThe info()  method is useful to get a quick description of the data, in particular the\\ntotal number of rows, and each attribute’s type and number of non-null values (see\\nFigure 2-6 ).\\n50 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 48, 'title': 'Create a Test Set', 'content': 'and it is not necessarily a problem, but you should try to understand how the\\ndata was computed.\\n2.The housing median age and the median house value were also capped. The lat‐\\nter may be a serious problem since it is your target attribute (your labels). Y our\\nMachine Learning algorithms may learn that prices never go beyond that limit.\\nY ou need to check with your client team (the team that will use your system’s out‐\\nput) to see if this is a problem or not. If they tell you that they need precise pre‐\\ndictions even beyond $500,000, then you have mainly two options:\\na.Collect proper labels for the districts whose labels were capped.\\nb.Remove those districts from the training set (and also from the test set, since\\nyour system should not be evaluated poorly if it predicts values beyond\\n$500,000).\\n3.These attributes have very different scales. We will discuss this later in this chap‐\\nter when we explore feature scaling.\\n4.Finally, many histograms are tail heavy : they extend much farther to the right of\\nthe median than to the left. This may make it a bit harder for some Machine\\nLearning algorithms to detect patterns. We will try transforming these attributes\\nlater on to have more bell-shaped distributions.\\nHopefully you now have a better understanding of the kind of data you are dealing\\nwith.\\nWait! Before you look at the data any further, you need to create a\\ntest set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\\nyou have only taken a quick glance at the data, and surely you should learn a whole\\nlot more about it before you decide what algorithms to use, right? This is true, but\\nyour brain is an amazing pattern detection system, which means that it is highly\\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\\ninteresting pattern in the test data that leads you to select a particular kind of\\nMachine Learning model. When you estimate the generalization error using the test\\nset, your estimate will be too optimistic and you will launch a system that will not\\nperform as well as expected. This is called data snooping  bias.\\nCreating a test set is theoretically quite simple: just pick some instances randomly,\\ntypically 20% of the dataset (or less if your dataset is very large), and set them aside:\\n54 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 49, 'title': 'Discover and Visualize the Data to Gain Insights', 'content': 'Let’s see if this worked as expected. Y ou can start by looking at the income category\\nproportions in the test set:\\n>>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\\n3    0.350533\\n2    0.318798\\n4    0.176357\\n5    0.114583\\n1    0.039729\\nName: income_cat, dtype: float64\\nWith similar code you can measure the income category proportions in the full data‐\\nset. Figure 2-10  compares the income category proportions in the overall dataset, in\\nthe test set generated with stratified sampling, and in a test set generated using purely\\nrandom sampling. As you can see, the test set generated using stratified sampling has\\nincome category proportions almost identical to those in the full dataset, whereas the\\ntest set generated using purely random sampling is quite skewed.\\nFigure 2-10. Sampling bias comparison of stratified  versus purely random sampling\\nNow you should remove the income_cat  attribute so the data is back to its original\\nstate:\\nfor set_ in (strat_train_set , strat_test_set ):\\n    set_.drop(\"income_cat\" , axis=1, inplace=True)\\nWe spent quite a bit of time on test set generation for a good reason: this is an often\\nneglected but critical part of a Machine Learning project. Moreover, many of these\\nideas will be useful later when we discuss cross-validation. Now it’s time to move on\\nto the next stage: exploring the data.\\nDiscover and Visualize the Data to Gain Insights\\nSo far you have only taken a quick glance at the data to get a general understanding of\\nthe kind of data you are manipulating. Now the goal is to go a little bit more in depth.\\nFirst, make sure you have put the test set aside and you are only exploring the train‐\\ning set. Also, if the training set is very large, you may want to sample an exploration\\n58 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 50, 'title': 'Visualizing Geographical Data', 'content': 'set, to make manipulations easy and fast. In our case, the set is quite small so you can\\njust work directly on the full set. Let’s create a copy so you can play with it without\\nharming the training set:\\nhousing = strat_train_set .copy()\\nVisualizing Geographical Data\\nSince there is geographical information (latitude and longitude), it is a good idea to\\ncreate a scatterplot of all districts to visualize the data ( Figure 2-11 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\\nFigure 2-11. A geographical scatterplot of the data\\nThis looks like California all right, but other than that it is hard to see any particular\\npattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\\nwhere there is a high density of data points ( Figure 2-12 ):\\nhousing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\\nDiscover and Visualize the Data to Gain Insights | 59', 'children': []}, {'id': 51, 'title': 'Looking for Correlations', 'content': 'This image tells you that the housing prices are very much related to the location\\n(e.g., close to the ocean) and to the population density, as you probably knew already.\\nIt will probably be useful to use a clustering algorithm to detect the main clusters, and\\nadd new features that measure the proximity to the cluster centers. The ocean prox‐\\nimity attribute may be useful as well, although in Northern California the housing\\nprices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation\\ncoefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\\nmethod:\\ncorr_matrix  = housing.corr()\\nNow let’s look at how much each attribute correlates with the median house value:\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value    1.000000\\nmedian_income         0.687170\\ntotal_rooms           0.135231\\nhousing_median_age    0.114220\\nhouseholds            0.064702\\ntotal_bedrooms        0.047865\\npopulation           -0.026699\\nlongitude            -0.047279\\nlatitude             -0.142826\\nName: median_house_value, dtype: float64\\nThe correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\\nthere is a strong positive correlation; for example, the median house value tends to go\\nup when the median income goes up. When the coefficient is close to –1, it means\\nthat there is a strong negative correlation; you can see a small negative correlation\\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\\ngo down when you go north). Finally, coefficients close to zero mean that there is no\\nlinear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\\ncient between their horizontal and vertical axes.\\n62 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 52, 'title': 'Experimenting with Attribute Combinations', 'content': 'Figure 2-16. Median income versus median house value\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the\\ndata and gain insights. Y ou identified a few data quirks that you may want to clean up\\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\\ncorrelations between attributes, in particular with the target attribute. Y ou also\\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans‐\\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\\nconsiderably with each project, but the general ideas are similar.\\nOne last thing you may want to do before actually preparing the data for Machine\\nLearning algorithms is to try out various attribute combinations. For example, the\\ntotal number of rooms in a district is not very useful if you don’t know how many\\nhouseholds there are. What you really want is the number of rooms per household.\\nSimilarly, the total number of bedrooms by itself is not very useful: you probably\\nwant to compare it to the number of rooms. And the population per household also\\nseems like an interesting attribute combination to look at. Let’s create these new\\nattributes:\\nhousing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\\nhousing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\\nhousing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\\nAnd now let’s look at the correlation matrix again:\\n>>> corr_matrix  = housing.corr()\\n>>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\\nmedian_house_value          1.000000\\nDiscover and Visualize the Data to Gain Insights | 65', 'children': []}]}, {'id': 53, 'title': 'Prepare the Data for Machine Learning Algorithms', 'content': 'median_income               0.687160\\nrooms_per_household         0.146285\\ntotal_rooms                 0.135097\\nhousing_median_age          0.114110\\nhouseholds                  0.064506\\ntotal_bedrooms              0.047689\\npopulation_per_household   -0.021985\\npopulation                 -0.026920\\nlongitude                  -0.047432\\nlatitude                   -0.142724\\nbedrooms_per_room          -0.259984\\nName: median_house_value, dtype: float64\\nHey, not bad! The new bedrooms_per_room  attribute is much more correlated with\\nthe median house value than the total number of rooms or bedrooms. Apparently\\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\\nrooms per household is also more informative than the total number of rooms in a\\ndistrict—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to\\nstart off on the right foot and quickly gain insights that will help you get a first rea‐\\nsonably good prototype. But this is an iterative process: once you get a prototype up\\nand running, you can analyze its output to gain more insights and come back to this\\nexploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of just\\ndoing this manually, you should write functions to do that, for several good reasons:\\n•This will allow you to reproduce these transformations easily on any dataset (e.g.,\\nthe next time you get a fresh dataset).\\n•Y ou will gradually build a library of transformation functions that you can reuse\\nin future projects.\\n•Y ou can use these functions in your live system to transform the new data before\\nfeeding it to your algorithms.\\n•This will make it possible for you to easily try various transformations and see\\nwhich combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying strat_train_set  once again),\\nand let’s separate the predictors and the labels since we don’t necessarily want to apply\\nthe same transformations to the predictors and the target values (note that drop()  \\ncreates a copy of the data and does not affect strat_train_set ):\\nhousing = strat_train_set .drop(\"median_house_value\" , axis=1)\\nhousing_labels  = strat_train_set [\"median_house_value\" ].copy()\\n66 | Chapter 2: End-to-End Machine Learning Project', 'children': [{'id': 54, 'title': 'Data Cleaning', 'content': 'Data Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create\\na few functions to take care of them. Y ou noticed earlier that the total_bedrooms\\nattribute has some missing values, so let’s fix this. Y ou have three options:\\n•Get rid of the corresponding districts.\\n•Get rid of the whole attribute.\\n•Set the values to some value (zero, the mean, the median, etc.).\\nY ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\\nmethods:\\nhousing.dropna(subset=[\"total_bedrooms\" ])    # option 1\\nhousing.drop(\"total_bedrooms\" , axis=1)       # option 2\\nmedian = housing[\"total_bedrooms\" ].median()  # option 3\\nhousing[\"total_bedrooms\" ].fillna(median, inplace=True)\\nIf you choose option 3, you should compute the median value on the training set, and\\nuse it to fill the missing values in the training set, but also don’t forget to save the\\nmedian value that you have computed. Y ou will need it later to replace missing values\\nin the test set when you want to evaluate your system, and also once the system goes\\nlive to replace missing values in new data.\\nScikit-Learn provides a handy class to take care of missing values: SimpleImputer .\\nHere is how to use it. First, you need to create a SimpleImputer  instance, specifying\\nthat you want to replace each attribute’s missing values with the median of that\\nattribute:\\nfrom sklearn.impute  import SimpleImputer\\nimputer = SimpleImputer (strategy =\"median\" )\\nSince the median can only be computed on numerical attributes, we need to create a\\ncopy of the data without the text attribute ocean_proximity :\\nhousing_num  = housing.drop(\"ocean_proximity\" , axis=1)\\nNow you can fit the imputer  instance to the training data using the fit()  method:\\nimputer.fit(housing_num )\\nThe imputer  has simply computed the median of each attribute and stored the result\\nin its statistics_  instance variable. Only the total_bedrooms  attribute had missing\\nvalues, but we cannot be sure that there won’t be any missing values in new data after\\nthe system goes live, so it is safer to apply the imputer  to all the numerical attributes:\\n>>> imputer.statistics_\\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\\nPrepare the Data for Machine Learning Algorithms | 67', 'children': []}, {'id': 55, 'title': 'Handling Text and Categorical Attributes', 'content': '18Some predictors also provide methods to measure the confidence of their predictions.\\n19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\\nPandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\\nalgorithms).18\\n•Inspection . All the estimator’s hyperparameters are accessible directly via public\\ninstance variables (e.g., imputer.strategy ), and all the estimator’s learned\\nparameters are also accessible via public instance variables with an underscore\\nsuffix (e.g., imputer.statistics_ ).\\n•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\\nsparse matrices, instead of homemade classes. Hyperparameters are just regular\\nPython strings or numbers.\\n•Composition . Existing building blocks are reused as much as possible. For\\nexample, it is easy to create a Pipeline  estimator from an arbitrary sequence of\\ntransformers followed by a final estimator, as we will see.\\n•Sensible defaults . Scikit-Learn provides reasonable default values for most\\nparameters, making it easy to create a baseline working system quickly.\\nHandling Text and Categorical Attributes\\nEarlier we left out the categorical attribute ocean_proximity  because it is a text\\nattribute so we cannot compute its median:\\n>>> housing_cat  = housing[[\"ocean_proximity\" ]]\\n>>> housing_cat .head(10)\\n      ocean_proximity\\n17606       <1H OCEAN\\n18632       <1H OCEAN\\n14650      NEAR OCEAN\\n3230           INLAND\\n3555        <1H OCEAN\\n19480          INLAND\\n8879        <1H OCEAN\\n13685          INLAND\\n4937        <1H OCEAN\\n4861        <1H OCEAN\\nMost Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\\nvert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\\nlEncoder  class19:\\n>>> from sklearn.preprocessing  import OrdinalEncoder\\n>>> ordinal_encoder  = OrdinalEncoder ()\\nPrepare the Data for Machine Learning Algorithms | 69', 'children': []}, {'id': 56, 'title': 'Custom Transformers', 'content': \"21See SciPy’s documentation for more details.\\nzero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\\nconvert it to a (dense) NumPy array, just call the toarray()  method:\\n>>> housing_cat_1hot .toarray()\\narray([[1., 0., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 0., 1.],\\n       ...,\\n       [0., 1., 0., 0., 0.],\\n       [1., 0., 0., 0., 0.],\\n       [0., 0., 0., 1., 0.]])\\nOnce again, you can get the list of categories using the encoder’s categories_\\ninstance variable:\\n>>> cat_encoder .categories_\\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\\n       dtype=object)]\\nIf a categorical attribute has a large number of possible categories\\n(e.g., country code, profession, species, etc.), then one-hot encod‐\\ning will result in a large number of input features. This may slow\\ndown training and degrade performance. If this happens, you may\\nwant to replace the categorical input with useful numerical features\\nrelated to the categories: for example, you could replace the\\nocean_proximity  feature with the distance to the ocean (similarly,\\na country code could be replaced with the country’s population and\\nGDP per capita). Alternatively, you could replace each category\\nwith a learnable low dimensional vector called an embedding . Each\\ncategory’s representation would be learned during training: this is\\nan example of representation learning  (see Chapter 13  and ??? for\\nmore details).\\nCustom Transformers\\nAlthough Scikit-Learn provides many useful transformers, you will need to write\\nyour own for tasks such as custom cleanup operations or combining specific\\nattributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\\nitance), all you need is to create a class and implement three methods: fit()\\n(returning self ), transform() , and fit_transform() . Y ou can get the last one for\\nfree by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\\ntor as a base class (and avoid *args  and **kargs  in your constructor) you will get\\ntwo extra methods ( get_params()  and set_params() ) that will be useful for auto‐\\nPrepare the Data for Machine Learning Algorithms | 71\", 'children': []}, {'id': 57, 'title': 'Feature Scaling', 'content': 'matic hyperparameter tuning. For example, here is a small transformer class that adds\\nthe combined attributes we discussed earlier:\\nfrom sklearn.base  import BaseEstimator , TransformerMixin\\nrooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\\nclass CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\\n    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\\n        self.add_bedrooms_per_room  = add_bedrooms_per_room\\n    def fit(self, X, y=None):\\n        return self  # nothing else to do\\n    def transform (self, X, y=None):\\n        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\\n        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\\n        if self.add_bedrooms_per_room :\\n            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\\n            return np.c_[X, rooms_per_household , population_per_household ,\\n                         bedrooms_per_room ]\\n        else:\\n            return np.c_[X, rooms_per_household , population_per_household ]\\nattr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\\nhousing_extra_attribs  = attr_adder .transform (housing.values)\\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room ,\\nset to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\\nmeter will allow you to easily find out whether adding this attribute helps the\\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\\nto gate any data preparation step that you are not 100% sure about. The more you\\nautomate these data preparation steps, the more combinations you can automatically\\ntry out, making it much more likely that you will find a great combination (and sav‐\\ning you a lot of time).\\nFeature Scaling\\nOne of the most important transformations you need to apply to your data is feature\\nscaling . With few exceptions, Machine Learning algorithms don’t perform well when\\nthe input numerical attributes have very different scales. This is the case for the hous‐\\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\\nincomes only range from 0 to 15. Note that scaling the target values is generally not\\nrequired.\\nThere are two common ways to get all attributes to have the same scale: min-max\\nscaling  and standardization .\\nMin-max scaling (many people call this normalization ) is quite simple: values are\\nshifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\\ning the min value and dividing by the max minus the min. Scikit-Learn provides a\\n72 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 58, 'title': 'Transformation Pipelines', 'content': 'transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\\nthat lets you change the range if you don’t want 0–1 for some reason.\\nStandardization is quite different: first it subtracts the mean value (so standardized\\nvalues always have a zero mean), and then it divides by the standard deviation so that\\nthe resulting distribution has unit variance. Unlike min-max scaling, standardization\\ndoes not bound values to a specific range, which may be a problem for some algo‐\\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\\never, standardization is much less affected by outliers. For example, suppose a district\\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\\nall the other values from 0–15 down to 0–0.15, whereas standardization would not be\\nmuch affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\\nardization.\\nAs with all the transformations, it is important to fit the scalers to\\nthe training data only, not to the full dataset (including the test set).\\nOnly then can you use them to transform the training set and the\\ntest set (and new data).\\nTransformation Pipelines\\nAs you can see, there are many data transformation steps that need to be executed in\\nthe right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\\nsuch sequences of transformations. Here is a small pipeline for the numerical\\nattributes:\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import StandardScaler\\nnum_pipeline  = Pipeline ([\\n        (\\'imputer\\' , SimpleImputer (strategy =\"median\" )),\\n        (\\'attribs_adder\\' , CombinedAttributesAdder ()),\\n        (\\'std_scaler\\' , StandardScaler ()),\\n    ])\\nhousing_num_tr  = num_pipeline .fit_transform (housing_num )\\nThe Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\\nsteps. All but the last estimator must be transformers (i.e., they must have a\\nfit_transform()  method). The names can be anything you like (as long as they are\\nunique and don’t contain double underscores “ __”): they will come in handy later for\\nhyperparameter tuning.\\nWhen you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\\nall transformers, passing the output of each call as the parameter to the next call, until\\nit reaches the final estimator, for which it just calls the fit()  method.\\nPrepare the Data for Machine Learning Algorithms | 73', 'children': []}]}, {'id': 59, 'title': 'Select and Train a Model', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': [{'id': 60, 'title': 'Training and Evaluating on the Training Set', 'content': 'Instead of a transformer, you can specify the string \"drop\"  if you\\nwant the columns to be dropped. Or you can specify \"pass\\nthrough\"  if you want the columns to be left untouched. By default,\\nthe remaining columns (i.e., the ones that were not listed) will be\\ndropped, but you can set the remainder  hyperparameter to any\\ntransformer (or to \"passthrough\" ) if you want these columns to be\\nhandled differently.\\nIf you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\\nsklearn-pandas , or roll out your own custom transformer to get the same function‐\\nality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\\nwhich can also apply different transformers and concatenate their outputs, but you\\ncannot specify different columns for each transformer, they all apply to the whole\\ndata. It is possible to work around this limitation using a custom transformer for col‐\\numn selection (see the Jupyter notebook for an example).\\nSelect and Train a Model\\nAt last! Y ou framed the problem, you got the data and explored it, you sampled a\\ntraining set and a test set, and you wrote transformation pipelines to clean up and\\nprepare your data for Machine Learning algorithms automatically. Y ou are now ready\\nto select and train a Machine Learning model.\\nTraining and Evaluating on the Training Set\\nThe good news is that thanks to all these previous steps, things are now going to be\\nmuch simpler than you might think. Let’s first train a Linear Regression model, like\\nwe did in the previous chapter:\\nfrom sklearn.linear_model  import LinearRegression\\nlin_reg = LinearRegression ()\\nlin_reg.fit(housing_prepared , housing_labels )\\nDone! Y ou now have a working Linear Regression model. Let’s try it out on a few\\ninstances from the training set:\\n>>> some_data  = housing.iloc[:5]\\n>>> some_labels  = housing_labels .iloc[:5]\\n>>> some_data_prepared  = full_pipeline .transform (some_data )\\n>>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\\nPredictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\\n>>> print(\"Labels:\" , list(some_labels ))\\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\\nSelect and Train a Model | 75', 'children': []}, {'id': 61, 'title': 'Better Evaluation Using Cross-Validation', 'content': 'It works, although the predictions are not exactly accurate (e.g., the first prediction is\\noff by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\\ning set using Scikit-Learn’s mean_squared_error  function:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> housing_predictions  = lin_reg.predict(housing_prepared )\\n>>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\\n>>> lin_rmse  = np.sqrt(lin_mse)\\n>>> lin_rmse\\n68628.19819848922\\nOkay, this is better than nothing but clearly not a great score: most districts’\\nmedian_housing_values  range between $120,000 and $265,000, so a typical predic‐\\ntion error of $68,628 is not very satisfying. This is an example of a model underfitting\\nthe training data. When this happens it can mean that the features do not provide\\nenough information to make good predictions, or that the model is not powerful\\nenough. As we saw in the previous chapter, the main ways to fix underfitting are to\\nselect a more powerful model, to feed the training algorithm with better features, or\\nto reduce the constraints on the model. This model is not regularized, so this rules\\nout the last option. Y ou could try to add more features (e.g., the log of the popula‐\\ntion), but first let’s try a more complex model to see how it does.\\nLet’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\\ndetail in Chapter 6 ). The code should look familiar by now:\\nfrom sklearn.tree  import DecisionTreeRegressor\\ntree_reg  = DecisionTreeRegressor ()\\ntree_reg .fit(housing_prepared , housing_labels )\\nNow that the model is trained, let’s evaluate it on the training set:\\n>>> housing_predictions  = tree_reg .predict(housing_prepared )\\n>>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\\n>>> tree_rmse  = np.sqrt(tree_mse )\\n>>> tree_rmse\\n0.0\\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\\nit is much more likely that the model has badly overfit the data. How can you be sure?\\nAs we saw earlier, you don’t want to touch the test set until you are ready to launch a\\nmodel you are confident about, so you need to use part of the training set for train‐\\ning, and part for model validation.\\nBetter Evaluation Using Cross-Validation\\nOne way to evaluate the Decision Tree model would be to use the train_test_split\\nfunction to split the training set into a smaller training set and a validation set, then\\n76 | Chapter 2: End-to-End Machine Learning Project', 'children': []}]}, {'id': 62, 'title': 'Fine-Tune Your Model', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': [{'id': 63, 'title': 'Grid Search', 'content': 'Y ou should save every model you experiment with, so you can\\ncome back easily to any model you want. Make sure you save both\\nthe hyperparameters and the trained parameters, as well as the\\ncross-validation scores and perhaps the actual predictions as well.\\nThis will allow you to easily compare scores across model types,\\nand compare the types of errors they make. Y ou can easily save\\nScikit-Learn models by using Python’s pickle  module, or using\\nsklearn.externals.joblib , which is more efficient at serializing \\nlarge NumPy arrays:\\nfrom sklearn.externals  import joblib\\njoblib.dump(my_model , \"my_model.pkl\" )\\n# and later...\\nmy_model_loaded  = joblib.load(\"my_model.pkl\" )\\nFine-Tune Your Model\\nLet’s assume that you now have a shortlist of promising models. Y ou now need to\\nfine-tune them. Let’s look at a few ways you can do that.\\nGrid Search\\nOne way to do that would be to fiddle with the hyperparameters manually, until you\\nfind a great combination of hyperparameter values. This would be very tedious work,\\nand you may not have time to explore many combinations.\\nInstead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\\ndo is tell it which hyperparameters you want it to experiment with, and what values to\\ntry out, and it will evaluate all the possible combinations of hyperparameter values,\\nusing cross-validation. For example, the following code searches for the best combi‐\\nnation of hyperparameter values for the RandomForestRegressor :\\nfrom sklearn.model_selection  import GridSearchCV\\nparam_grid  = [\\n    {\\'n_estimators\\' : [3, 10, 30], \\'max_features\\' : [2, 4, 6, 8]},\\n    {\\'bootstrap\\' : [False], \\'n_estimators\\' : [3, 10], \\'max_features\\' : [2, 3, 4]},\\n  ]\\nforest_reg  = RandomForestRegressor ()\\ngrid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\\n                           scoring=\\'neg_mean_squared_error\\' ,\\n                           return_train_score =True)\\ngrid_search .fit(housing_prepared , housing_labels )\\nFine-Tune Your Model | 79', 'children': []}, {'id': 64, 'title': 'Randomized Search', 'content': \"...     print(np.sqrt(-mean_score ), params)\\n...\\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\\nIn this example, we obtain the best solution by setting the max_features  hyperpara‐\\nmeter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\\ncombination is 49,682, which is slightly better than the score you got earlier using the\\ndefault hyperparameter values (which was 50,182). Congratulations, you have suc‐\\ncessfully fine-tuned your best model!\\nDon’t forget that you can treat some of the data preparation steps as\\nhyperparameters. For example, the grid search will automatically\\nfind out whether or not to add a feature you were not sure about\\n(e.g., using the add_bedrooms_per_room  hyperparameter of your\\nCombinedAttributesAdder  transformer). It may similarly be used\\nto automatically find the best way to handle outliers, missing fea‐\\ntures, feature selection, and more.\\nRandomized Search\\nThe grid search approach is fine when you are exploring relatively few combinations,\\nlike in the previous example, but when the hyperparameter search space  is large, it is\\noften preferable to use RandomizedSearchCV  instead. This class can be used in much\\nthe same way as the GridSearchCV  class, but instead of trying out all possible combi‐\\nnations, it evaluates a given number of random combinations by selecting a random\\nvalue for each hyperparameter at every iteration. This approach has two main bene‐\\nfits:\\nFine-Tune Your Model | 81\", 'children': []}, {'id': 65, 'title': 'Ensemble Methods', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 66, 'title': 'Analyze the Best Models and Their Errors', 'content': '•If you let the randomized search run for, say, 1,000 iterations, this approach will\\nexplore 1,000 different values for each hyperparameter (instead of just a few val‐\\nues per hyperparameter with the grid search approach).\\n•Y ou have more control over the computing budget you want to allocate to hyper‐\\nparameter search, simply by setting the number of iterations.\\nEnsemble Methods\\nAnother way to fine-tune your system is to try to combine the models that perform\\nbest. The group (or “ensemble”) will often perform better than the best individual\\nmodel (just like Random Forests perform better than the individual Decision Trees\\nthey rely on), especially if the individual models make very different types of errors.\\nWe will cover this topic in more detail in Chapter 7 .\\nAnalyze the Best Models and Their Errors\\nY ou will often gain good insights on the problem by inspecting the best models. For\\nexample, the RandomForestRegressor  can indicate the relative importance of each\\nattribute for making accurate predictions:\\n>>> feature_importances  = grid_search .best_estimator_ .feature_importances_\\n>>> feature_importances\\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\\n       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\\n       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\\n       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\\nLet’s display these importance scores next to their corresponding attribute names:\\n>>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\\n>>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\\n>>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\\n>>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\\n>>> sorted(zip(feature_importances , attributes ), reverse=True)\\n[(0.3661589806181342, \\'median_income\\'),\\n (0.1647809935615905, \\'INLAND\\'),\\n (0.10879295677551573, \\'pop_per_hhold\\'),\\n (0.07334423551601242, \\'longitude\\'),\\n (0.0629090704826203, \\'latitude\\'),\\n (0.05641917918195401, \\'rooms_per_hhold\\'),\\n (0.05335107734767581, \\'bedrooms_per_room\\'),\\n (0.041143798478729635, \\'housing_median_age\\'),\\n (0.014874280890402767, \\'population\\'),\\n (0.014672685420543237, \\'total_rooms\\'),\\n (0.014257599323407807, \\'households\\'),\\n (0.014106483453584102, \\'total_bedrooms\\'),\\n (0.010311488326303787, \\'<1H OCEAN\\'),\\n (0.002856474637320158, \\'NEAR OCEAN\\'),\\n82 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 67, 'title': 'Evaluate Your System on the Test Set', 'content': ' (0.00196041559947807, \\'NEAR BAY\\'),\\n (6.028038672736599e-05, \\'ISLAND\\')]\\nWith this information, you may want to try dropping some of the less useful features\\n(e.g., apparently only one ocean_proximity  category is really useful, so you could try\\ndropping the others).\\nY ou should also look at the specific errors that your system makes, then try to under‐\\nstand why it makes them and what could fix the problem (adding extra features or, on\\nthe contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\\nEvaluate Your System on the Test Set\\nAfter tweaking your models for a while, you eventually have a system that performs\\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\\nnothing special about this process; just get the predictors and the labels from your\\ntest set, run your full_pipeline  to transform the data (call transform() , not\\nfit_transform() , you do not want to fit the test set!), and evaluate the final model\\non the test set:\\nfinal_model  = grid_search .best_estimator_\\nX_test = strat_test_set .drop(\"median_house_value\" , axis=1)\\ny_test = strat_test_set [\"median_house_value\" ].copy()\\nX_test_prepared  = full_pipeline .transform (X_test)\\nfinal_predictions  = final_model .predict(X_test_prepared )\\nfinal_mse  = mean_squared_error (y_test, final_predictions )\\nfinal_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\\nIn some cases, such a point estimate of the generalization error will not be quite\\nenough to convince you to launch: what if it is just 0.1% better than the model cur‐\\nrently in production? Y ou might want to have an idea of how precise this estimate is.\\nFor this, you can compute a 95% confidence  interval  for the generalization error using\\nscipy.stats.t.interval() :\\n>>> from scipy import stats\\n>>> confidence  = 0.95\\n>>> squared_errors  = (final_predictions  - y_test) ** 2\\n>>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\\n...                          loc=squared_errors .mean(),\\n...                          scale=stats.sem(squared_errors )))\\n...\\narray([45685.10470776, 49691.25001878])\\nThe performance will usually be slightly worse than what you measured using cross-\\nvalidation if you did a lot of hyperparameter tuning (because your system ends up\\nfine-tuned to perform well on the validation data, and will likely not perform as well\\nFine-Tune Your Model | 83', 'children': []}]}, {'id': 68, 'title': 'Launch, Monitor, and Maintain Your System', 'content': 'on unknown datasets). It is not the case in this example, but when this happens you\\nmust resist the temptation to tweak the hyperparameters to make the numbers look\\ngood on the test set; the improvements would be unlikely to generalize to new data.\\nNow comes the project prelaunch phase: you need to present your solution (high‐\\nlighting what you have learned, what worked and what did not, what assumptions\\nwere made, and what your system’s limitations are), document everything, and create\\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\\n“the median income is the number one predictor of housing prices”). In this Califor‐\\nnia housing example, the final performance of the system is not better than the\\nexperts’ , but it may still be a good idea to launch it, especially if this frees up some\\ntime for the experts so they can work on more interesting and productive tasks.\\nLaunch, Monitor, and Maintain Your System\\nPerfect, you got approval to launch! Y ou need to get your solution ready for produc‐\\ntion, in particular by plugging the production input data sources into your system\\nand writing tests.\\nY ou also need to write monitoring code to check your system’s live performance at\\nregular intervals and trigger alerts when it drops. This is important to catch not only\\nsudden breakage, but also performance degradation. This is quite common because\\nmodels tend to “rot” as data evolves over time, unless the models are regularly trained\\non fresh data.\\nEvaluating your system’s performance will require sampling the system’s predictions\\nand evaluating them. This will generally require a human analysis. These analysts\\nmay be field experts, or workers on a crowdsourcing platform (such as Amazon\\nMechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\\ntion pipeline into your system.\\nY ou should also make sure you evaluate the system’s input data quality. Sometimes\\nperformance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\\ntioning sensor sending random values, or another team’s output becoming stale), but\\nit may take a while before your system’s performance degrades enough to trigger an\\nalert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\\ninputs is particularly important for online learning systems.\\nFinally, you will generally want to train your models on a regular basis using fresh\\ndata. Y ou should automate this process as much as possible. If you don’t, you are very\\nlikely to refresh your model only every six months (at best), and your system’s perfor‐\\nmance may fluctuate severely over time. If your system is an online learning system,\\nyou should make sure you save snapshots of its state at regular intervals so you can\\neasily roll back to a previously working state.\\n84 | Chapter 2: End-to-End Machine Learning Project', 'children': []}, {'id': 69, 'title': 'Try It Out!', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}, {'id': 70, 'title': 'Exercises', 'content': 'Try It Out!\\nHopefully this chapter gave you a good idea of what a Machine Learning project\\nlooks like, and showed you some of the tools you can use to train a great system. As\\nyou can see, much of the work is in the data preparation step, building monitoring\\ntools, setting up human evaluation pipelines, and automating regular model training.\\nThe Machine Learning algorithms are also important, of course, but it is probably\\npreferable to be comfortable with the overall process and know three or four algo‐\\nrithms well rather than to spend all your time exploring advanced algorithms and not\\nenough time on the overall process.\\nSo, if you have not already done so, now is a good time to pick up a laptop, select a\\ndataset that you are interested in, and try to go through the whole process from A to\\nZ. A good place to start is on a competition website such as http://kaggle.com/ : you\\nwill have a dataset to play with, a clear goal, and people to share the experience with.\\nExercises\\nUsing this chapter’s housing dataset:\\n1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\\nparameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\\nmeter) or kernel=\"rbf\"  (with various values for the C and gamma\\nhyperparameters). Don’t worry about what these hyperparameters mean for now.\\nHow does the best SVR predictor perform?\\n2.Try replacing GridSearchCV  with RandomizedSearchCV .\\n3.Try adding a transformer in the preparation pipeline to select only the most\\nimportant attributes.\\n4.Try creating a single pipeline that does the full data preparation plus the final\\nprediction.\\n5.Automatically explore some preparation options using GridSearchCV .\\nSolutions to these exercises are available in the online Jupyter notebooks at https://\\ngithub.com/ageron/handson-ml2 .\\nTry It Out! | 85', 'children': []}]}, {'id': 71, 'title': 'Chapter 3. Classification', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': [{'id': 72, 'title': 'MNIST', 'content': '1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\\nCHAPTER 3\\nClassification\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 3 in the final\\nrelease of the book.\\nIn Chapter 1  we mentioned that the most common supervised learning tasks are\\nregression (predicting values) and classification (predicting classes). In Chapter 2  we\\nexplored a regression task, predicting housing values, using various algorithms such\\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\\nin further detail in later chapters). Now we will turn our attention to classification\\nsystems.\\nMNIST\\nIn this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\\nimages of digits handwritten by high school students and employees of the US Cen‐\\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\\nied so much that it is often called the “Hello World” of Machine Learning: whenever\\npeople come up with a new classification algorithm, they are curious to see how it\\nwill perform on MNIST. Whenever someone learns Machine Learning, sooner or\\nlater they tackle MNIST.\\nScikit-Learn provides many helper functions to download popular datasets. MNIST is\\none of them. The following code fetches the MNIST dataset:1\\n87', 'children': []}, {'id': 73, 'title': 'Training a Binary Classifier', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': []}, {'id': 74, 'title': 'Performance Measures', 'content': '2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\\nstock market prices or weather conditions). We will explore this in the next chapters.\\ninstances, and they perform poorly if they get many similar instances in a row. Shuf‐\\nfling the dataset ensures that this won’t happen.2\\nTraining a Binary Classifier\\nLet’s simplify the problem for now and only try to identify one digit—for example,\\nthe number 5. This “5-detector” will be an example of a binary classifier , capable of\\ndistinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\\nthis classification task:\\ny_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\\ny_test_5  = (y_test == 5)\\nOkay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\\nGradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\\nsifier has the advantage of being capable of handling very large datasets efficiently.\\nThis is in part because SGD deals with training instances independently, one at a time\\n(which also makes SGD well suited for online learning ), as we will see later. Let’s create\\nan SGDClassifier  and train it on the whole training set:\\nfrom sklearn.linear_model  import SGDClassifier\\nsgd_clf = SGDClassifier (random_state =42)\\nsgd_clf.fit(X_train, y_train_5 )\\nThe SGDClassifier  relies on randomness during training (hence\\nthe name “stochastic”). If you want reproducible results, you\\nshould set the random_state  parameter.\\nNow you can use it to detect images of the number 5:\\n>>> sgd_clf.predict([some_digit ])\\narray([ True])\\nThe classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\\nin this particular case! Now, let’s evaluate this model’s performance.\\nPerformance Measures\\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\\nwill spend a large part of this chapter on this topic. There are many performance\\n90 | Chapter 3: Classification', 'children': [{'id': 75, 'title': 'Measuring Accuracy Using Cross-Validation', 'content': 'measures available, so grab another coffee and get ready to learn many new concepts\\nand acronyms!\\nMeasuring Accuracy Using Cross-Validation\\nA good way to evaluate a model is to use cross-validation, just as you did in Chap‐\\nter 2 .\\nImplementing Cross-Validation\\nOccasionally you will need more control over the cross-validation process than what\\nScikit-Learn provides off-the-shelf. In these cases, you can implement cross-\\nvalidation yourself; it is actually fairly straightforward. The following code does\\nroughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \\nsame result:\\nfrom sklearn.model_selection  import StratifiedKFold\\nfrom sklearn.base  import clone\\nskfolds = StratifiedKFold (n_splits =3, random_state =42)\\nfor train_index , test_index  in skfolds.split(X_train, y_train_5 ):\\n    clone_clf  = clone(sgd_clf)\\n    X_train_folds  = X_train[train_index ]\\n    y_train_folds  = y_train_5 [train_index ]\\n    X_test_fold  = X_train[test_index ]\\n    y_test_fold  = y_train_5 [test_index ]\\n    clone_clf .fit(X_train_folds , y_train_folds )\\n    y_pred = clone_clf .predict(X_test_fold )\\n    n_correct  = sum(y_pred == y_test_fold )\\n    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\\nThe StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\\nto produce folds that contain a representative ratio of each class. At each iteration the\\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\\npredictions on the test fold. Then it counts the number of correct predictions and\\noutputs the ratio of correct predictions.\\nLet’s use the cross_val_score()  function to evaluate your SGDClassifier  model\\nusing K-fold cross-validation, with three folds. Remember that K-fold cross-\\nvalidation means splitting the training set into K-folds (in this case, three), then mak‐\\ning predictions and evaluating them on each fold using a model trained on the\\nremaining folds (see Chapter 2 ):\\nPerformance Measures | 91', 'children': []}, {'id': 76, 'title': 'Confusion Matrix', 'content': '>>> from sklearn.model_selection  import cross_val_score\\n>>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.96355, 0.93795, 0.95615])\\nWow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \\nThis looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\\ndumb classifier that just classifies every single image in the “not-5” class:\\nfrom sklearn.base  import BaseEstimator\\nclass Never5Classifier (BaseEstimator ):\\n    def fit(self, X, y=None):\\n        pass\\n    def predict(self, X):\\n        return np.zeros((len(X), 1), dtype=bool)\\nCan you guess this model’s accuracy? Let’s find out:\\n>>> never_5_clf  = Never5Classifier ()\\n>>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\\narray([0.91125, 0.90855, 0.90915])\\nThat’s right, it has over 90% accuracy! This is simply because only about 10% of the\\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\\n90% of the time. Beats Nostradamus.\\nThis demonstrates why accuracy is generally not the preferred performance measure\\nfor classifiers, especially when you are dealing with skewed datasets  (i.e., when some\\nclasses are much more frequent than others).\\nConfusion Matrix\\nA much better way to evaluate the performance of a classifier is to look at the confu‐\\nsion matrix . The general idea is to count the number of times instances of class A are\\nclassified as class B. For example, to know the number of times the classifier confused\\nimages of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\\nmatrix.\\nTo compute the confusion matrix, you first need to have a set of predictions, so they\\ncan be compared to the actual targets. Y ou could make predictions on the test set, but\\nlet’s keep it untouched for now (remember that you want to use the test set only at the\\nvery end of your project, once you have a classifier that you are ready to launch).\\nInstead, you can use the cross_val_predict()  function:\\nfrom sklearn.model_selection  import cross_val_predict\\ny_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\\nJust like the cross_val_score()  function, cross_val_predict()  performs K-fold\\ncross-validation, but instead of returning the evaluation scores, it returns the predic‐\\n92 | Chapter 3: Classification', 'children': []}, {'id': 77, 'title': 'Precision and Recall', 'content': '(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\\n(Equation 3-2 ).\\nEquation 3-2. Recall\\nrecall =TP\\nTP+FN\\nFN is of course the number of false negatives.\\nIf you are confused about the confusion matrix, Figure 3-2  may help.\\nFigure 3-2. An illustrated confusion matrix\\nPrecision and Recall\\nScikit-Learn provides several functions to compute classifier metrics, including preci‐\\nsion and recall:\\n>>> from sklearn.metrics  import precision_score , recall_score\\n>>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\\n0.7290850836596654\\n>>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\\n0.7555801512636044\\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy.\\nWhen it claims an image represents a 5, it is correct only 72.9% of the time. More‐\\nover, it only detects 75.6% of the 5s.\\nIt is often convenient to combine precision and recall into a single metric called the F1\\nscore , in particular if you need a simple way to compare two classifiers. The F1 score is \\nthe harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\\n94 | Chapter 3: Classification', 'children': []}, {'id': 78, 'title': 'Precision/Recall Tradeoff', 'content': 'treats all values equally, the harmonic mean gives much more weight to low values.\\nAs a result, the classifier will only get a high F1 score if both recall and precision are\\nhigh.\\nEquation 3-3. F1\\nF1=2\\n1\\nprecision+1\\nrecall= 2 ×precision × recall\\nprecision + recall=TP\\nTP+FN+FP\\n2\\nTo compute the F1 score, simply call the f1_score()  function:\\n>>> from sklearn.metrics  import f1_score\\n>>> f1_score (y_train_5 , y_train_pred )\\n0.7420962043663375\\nThe F1 score favors classifiers that have similar precision and recall. This is not always\\nwhat you want: in some contexts you mostly care about precision, and in other con‐\\ntexts you really care about recall. For example, if you trained a classifier to detect vid‐\\neos that are safe for kids, you would probably prefer a classifier that rejects many\\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\\nsifier that has a much higher recall but lets a few really bad videos show up in your\\nproduct (in such cases, you may even want to add a human pipeline to check the clas‐\\nsifier’s video selection). On the other hand, suppose you train a classifier to detect\\nshoplifters on surveillance images: it is probably fine if your classifier has only 30%\\nprecision as long as it has 99% recall (sure, the security guards will get a few false\\nalerts, but almost all shoplifters will get caught).\\nUnfortunately, you can’t have it both ways: increasing precision reduces recall, and\\nvice versa. This is called the precision/recall tradeoff .\\nPrecision/Recall Tradeoff\\nTo understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\\ntion decisions. For each instance, it computes a score based on a decision function , \\nand if that score is greater than a threshold, it assigns the instance to the positive\\nclass, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\\nfrom the lowest score on the left to the highest score on the right. Suppose the deci‐\\nsion threshold  is positioned at the central arrow (between the two 5s): you will find 4\\ntrue positives (actual 5s) on the right of that threshold, and one false positive (actually\\na 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\\nraise the threshold (move it to the arrow on the right), the false positive (the 6)\\nbecomes a true negative, thereby increasing precision (up to 100% in this case), but\\none true positive becomes a false negative, decreasing recall down to 50%. Conversely,\\nlowering the threshold increases recall and reduces precision.\\nPerformance Measures | 95', 'children': []}, {'id': 79, 'title': 'The ROC Curve', 'content': \"If someone says “let’s reach 99% precision, ” you should ask, “at\\nwhat recall?”\\nThe ROC Curve\\nThe receiver operating characteristic  (ROC) curve is another common tool used with\\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\\nting precision versus recall, the ROC curve plots the true positive rate  (another name\\nfor recall) against the false positive rate . The FPR is the ratio of negative instances that\\nare incorrectly classified as positive. It is equal to one minus the true negative rate , \\nwhich is the ratio of negative instances that are correctly classified as negative. The\\nTNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\\n1 – specificity .\\nTo plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\\nhold values, using the roc_curve()  function:\\nfrom sklearn.metrics  import roc_curve\\nfpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\\nThen you can plot the FPR against the TPR using Matplotlib. This code produces the\\nplot in Figure 3-6 :\\ndef plot_roc_curve (fpr, tpr, label=None):\\n    plt.plot(fpr, tpr, linewidth =2, label=label)\\n    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\\n    [...] # Add axis labels and grid\\nplot_roc_curve (fpr, tpr)\\nplt.show()\\nPerformance Measures | 99\", 'children': []}]}, {'id': 80, 'title': 'Multiclass Classification', 'content': 'As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\\nbetter than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\\nresult, its ROC AUC score is also significantly better:\\n>>> roc_auc_score (y_train_5 , y_scores_forest )\\n0.9983436731328145\\nTry measuring the precision and recall scores: you should find 99.0% precision and\\n86.6% recall. Not too bad!\\nHopefully you now know how to train binary classifiers, choose the appropriate met‐\\nric for your task, evaluate your classifiers using cross-validation, select the precision/\\nrecall tradeoff that fits your needs, and compare various models using ROC curves\\nand ROC AUC scores. Now let’s try to detect more than just the 5s.\\nMulticlass Classification\\nWhereas binary classifiers distinguish between two classes, multiclass classifiers  (also\\ncalled multinomial classifiers ) can distinguish between more than two classes.\\nSome algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\\ncapable of handling multiple classes directly. Others (such as Support Vector Machine\\nclassifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\\nous strategies that you can use to perform multiclass classification using multiple\\nbinary classifiers.\\nFor example, one way to create a system that can classify the digit images into 10\\nclasses (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\\n1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\\nthe decision score from each classifier for that image and you select the class whose\\nclassifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \\n(also called one-versus-the-rest ).\\nAnother strategy is to train a binary classifier for every pair of digits: one to distin‐\\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\\nThis is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\\ntrain N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\\nbinary classifiers! When you want to classify an image, you have to run the image\\nthrough all 45 classifiers and see which class wins the most duels. The main advan‐\\ntage of OvO is that each classifier only needs to be trained on the part of the training\\nset for the two classes that it must distinguish.\\nSome algorithms (such as Support Vector Machine classifiers) scale poorly with the\\nsize of the training set, so for these algorithms OvO is preferred since it is faster to\\ntrain many classifiers on small training sets than training few classifiers on large\\ntraining sets. For most binary classification algorithms, however, OvA is preferred.\\n102 | Chapter 3: Classification', 'children': []}, {'id': 81, 'title': 'Error Analysis', 'content': 'array([5], dtype=uint8)\\n>>> len(ovo_clf.estimators_ )\\n45\\nTraining a RandomForestClassifier  is just as easy:\\n>>> forest_clf .fit(X_train, y_train)\\n>>> forest_clf .predict([some_digit ])\\narray([5], dtype=uint8)\\nThis time Scikit-Learn did not have to run OvA or OvO because Random Forest\\nclassifiers  can directly classify instances into multiple classes. Y ou can call\\npredict_proba()  to get the list of probabilities that the classifier assigned to each\\ninstance for each class:\\n>>> forest_clf .predict_proba ([some_digit ])\\narray([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\\nY ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\\nindex in the array means that the model estimates a 90% probability that the image\\nrepresents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\\ntively with 1%, 8% and 1% probability.\\nNow of course you want to evaluate these classifiers. As usual, you want to use cross-\\nvalidation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\\nfunction:\\n>>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\\narray([0.8489802 , 0.87129356, 0.86988048])\\nIt gets over 84% on all test folds. If you used a random classifier, you would get 10%\\naccuracy, so this is not such a bad score, but you can still do much better. For exam‐\\nple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\\n89%:\\n>>> from sklearn.preprocessing  import StandardScaler\\n>>> scaler = StandardScaler ()\\n>>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\\n>>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\\narray([0.89707059, 0.8960948 , 0.90693604])\\nError Analysis\\nOf course, if this were a real project, you would follow the steps in your Machine\\nLearning project checklist (see ???): exploring data preparation options, trying out\\nmultiple models, shortlisting the best ones and fine-tuning their hyperparameters\\nusing GridSearchCV , and automating as much as possible, as you did in the previous\\nchapter. Here, we will assume that you have found a promising model and you want\\nto find ways to improve it. One way to do this is to analyze the types of errors it\\nmakes.\\n104 | Chapter 3: Classification', 'children': []}, {'id': 82, 'title': 'Multilabel Classification', 'content': 'The main difference between 3s and 5s is the position of the small line that joins the\\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\\nwould be to preprocess the images to ensure that they are well centered and not too\\nrotated. This will probably help reduce other errors as well.\\nMultilabel Classification\\nUntil now each instance has always been assigned to just one class. In some cases you\\nmay want your classifier to output multiple classes for each instance. For example,\\nconsider a face-recognition classifier: what should it do if it recognizes several people\\non the same picture? Of course it should attach one tag per person it recognizes. Say\\nthe classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\\nwhen it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\\n“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\\nbinary tags is called a multilabel classification  system.\\nWe won’t go into face recognition just yet, but let’s look at a simpler example, just for\\nillustration purposes:\\nfrom sklearn.neighbors  import KNeighborsClassifier\\ny_train_large  = (y_train >= 7)\\ny_train_odd  = (y_train % 2 == 1)\\ny_multilabel  = np.c_[y_train_large , y_train_odd ]\\nknn_clf = KNeighborsClassifier ()\\nknn_clf.fit(X_train, y_multilabel )\\nThis code creates a y_multilabel  array containing two target labels for each digit\\nimage: the first indicates whether or not the digit is large (7, 8, or 9) and the second\\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier  \\ninstance (which supports multilabel classification, but not all classifiers do) and we\\ntrain it using the multiple targets array. Now you can make a prediction, and notice\\nthat it outputs two labels:\\n>>> knn_clf.predict([some_digit ])\\narray([[False,  True]])\\nAnd it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\\nThere are many ways to evaluate a multilabel classifier, and selecting the right metric\\nreally depends on your project. For example, one approach is to measure the F1 score\\nfor each individual label (or any other binary classifier metric discussed earlier), then\\nsimply compute the average score. This code computes the average F1 score across all\\nlabels:\\n108 | Chapter 3: Classification', 'children': []}, {'id': 83, 'title': 'Multioutput Classification', 'content': '4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\\nmore details.\\n>>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\\n>>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\\n0.976410265560605\\nThis assumes that all labels are equally important, which may not be the case. In par‐\\nticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\\nto give more weight to the classifier’s score on pictures of Alice. One simple option is\\nto give each label a weight equal to its support  (i.e., the number of instances with that\\ntarget label). To do this, simply set average=\"weighted\"  in the preceding code.4\\nMultioutput Classification\\nThe last type of classification task we are going to discuss here is called multioutput-\\nmulticlass classification  (or simply multioutput classification ). It is simply a generaliza‐\\ntion of multilabel classification where each label can be multiclass (i.e., it can have\\nmore than two possible values).\\nTo illustrate this, let’s build a system that removes noise from images. It will take as\\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\\nclassifier’s output is multilabel (one label per pixel) and each label can have multiple\\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\\nclassification system.\\nThe line between classification and regression is sometimes blurry,\\nsuch as in this example. Arguably, predicting pixel intensity is more\\nakin to regression than to classification. Moreover, multioutput\\nsystems are not limited to classification tasks; you could even have\\na system that outputs multiple labels per instance, including both\\nclass labels and value labels.\\nLet’s start by creating the training and test sets by taking the MNIST images and\\nadding noise to their pixel intensities using NumPy’s randint()  function. The target\\nimages will be the original images:\\nnoise = np.random.randint(0, 100, (len(X_train), 784))\\nX_train_mod  = X_train + noise\\nnoise = np.random.randint(0, 100, (len(X_test), 784))\\nX_test_mod  = X_test + noise\\ny_train_mod  = X_train\\ny_test_mod  = X_test\\nMultioutput Classification  | 109', 'children': []}, {'id': 84, 'title': 'Exercises', 'content': '5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\\nshift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\\nyou should be frowning right now):\\nOn the left is the noisy input image, and on the right is the clean target image. Now\\nlet’s train the classifier and make it clean this image:\\nknn_clf.fit(X_train_mod , y_train_mod )\\nclean_digit  = knn_clf.predict([X_test_mod [some_index ]])\\nplot_digit (clean_digit )\\nLooks close enough to the target! This concludes our tour of classification. Hopefully\\nyou should now know how to select good metrics for classification tasks, pick the\\nappropriate precision/recall tradeoff, compare classifiers, and more generally build\\ngood classification systems for a variety of tasks.\\nExercises\\n1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\\non the test set. Hint: the KNeighborsClassifier  works quite well for this task;\\nyou just need to find good hyperparameter values (try a grid search on the\\nweights  and n_neighbors  hyperparameters).\\n2.Write a function that can shift an MNIST image in any direction (left, right, up,\\nor down) by one pixel.5 Then, for each image in the training set, create four shif‐\\n110 | Chapter 3: Classification', 'children': []}]}, {'id': 85, 'title': 'Chapter 4. Training Models', 'content': 'CHAPTER 4\\nTraining Models\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 4 in the final\\nrelease of the book.\\nSo far we have treated Machine Learning models and their training algorithms mostly\\nlike black boxes. If you went through some of the exercises in the previous chapters,\\nyou may have been surprised by how much you can get done without knowing any‐\\nthing about what’s under the hood: you optimized a regression system, you improved\\na digit image classifier, and you even built a spam classifier from scratch—all this\\nwithout knowing how they actually work. Indeed, in many situations you don’t really\\nneed to know the implementation details.\\nHowever, having a good understanding of how things work can help you quickly\\nhome in on the appropriate model, the right training algorithm to use, and a good set\\nof hyperparameters for your task. Understanding what’s under the hood will also help\\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top‐\\nics discussed in this chapter will be essential in understanding, building, and training\\nneural networks (discussed in Part II  of this book).\\nIn this chapter, we will start by looking at the Linear Regression model, one of the\\nsimplest models there is. We will discuss two very different ways to train it:\\n•Using a direct “closed-form” equation that directly computes the model parame‐\\nters that best fit the model to the training set (i.e., the model parameters that\\nminimize the cost function over the training set).\\n113', 'children': [{'id': 86, 'title': 'Linear Regression', 'content': '•Using an iterative optimization approach, called Gradient Descent (GD), that\\ngradually tweaks the model parameters to minimize the cost function over the\\ntraining set, eventually converging to the same set of parameters as the first\\nmethod. We will look at a few variants of Gradient Descent that we will use again\\nand again when we study neural networks in Part II : Batch GD, Mini-batch GD,\\nand Stochastic GD.\\nNext we will look at Polynomial Regression, a more complex model that can fit non‐\\nlinear datasets. Since this model has more parameters than Linear Regression, it is\\nmore prone to overfitting the training data, so we will look at how to detect whether\\nor not this is the case, using learning curves, and then we will look at several regulari‐\\nzation techniques that can reduce the risk of overfitting the training set.\\nFinally, we will look at two more models that are commonly used for classification\\ntasks: Logistic Regression and Softmax Regression.\\nThere will be quite a few math equations in this chapter, using basic\\nnotions of linear algebra and calculus. To understand these equa‐\\ntions, you will need to know what vectors and matrices are, how to\\ntranspose them, multiply them, and inverse them, and what partial\\nderivatives are. If you are unfamiliar with these concepts, please go\\nthrough the linear algebra and calculus introductory tutorials avail‐\\nable as Jupyter notebooks in the online supplemental material. For\\nthose who are truly allergic to mathematics, you should still go\\nthrough this chapter and simply skip the equations; hopefully, the\\ntext will be sufficient to help you understand most of the concepts.\\nLinear Regression\\nIn Chapter 1 , we looked at a simple regression model of life satisfaction: life_satisfac‐\\ntion = θ0 + θ1 × GDP_per_capita .\\nThis model is just a linear function of the input feature GDP_per_capita . θ0 and θ1 are\\nthe model’s parameters.\\nMore generally, a linear model makes a prediction by simply computing a weighted\\nsum of the input features, plus a constant called the bias term  (also called the intercept\\nterm ), as shown in Equation 4-1 .\\nEquation 4-1. Linear Regression model prediction\\ny=θ0+θ1x1+θ2x2+⋯+θnxn\\n•ŷ is the predicted value.\\n114 | Chapter 4: Training Models', 'children': [{'id': 87, 'title': 'The Normal Equation', 'content': '1It is often the case that a learning algorithm will try to optimize a different function than the performance\\nmeasure used to evaluate the final model. This is generally because that function is easier to compute, because\\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\\nthe model during training, as we will see when we discuss regularization.\\n2The demonstration that this returns the value of θ that minimizes the cost function is outside the scope of this\\nbook.than the RMSE, and it leads to the same result (because the value that minimizes a\\nfunction also minimizes its square root).1\\nThe MSE of a Linear Regression hypothesis hθ on a training set X is calculated using\\nEquation 4-3 .\\nEquation 4-3. MSE cost function for a Linear Regression model\\nMSE X,hθ=1\\nm∑\\ni= 1m\\nθTxi−yi2\\nMost of these notations were presented in Chapter 2  (see “Notations”  on page 43).\\nThe only difference is that we write hθ instead of just h in order to make it clear that\\nthe model is parametrized by the vector θ. To simplify notations, we will just write\\nMSE( θ) instead of MSE( X, hθ).\\nThe Normal Equation\\nTo find the value of θ that minimizes the cost function, there is a closed-form solution\\n—in other words, a mathematical equation that gives the result directly. This is called\\nthe Normal Equation  (Equation 4-4 ).2\\nEquation 4-4. Normal Equation\\nθ=XTX−1\\xa0XT\\xa0y\\n•θ is the value of θ that minimizes the cost function.\\n•y is the vector of target values containing y(1) to y(m).\\nLet’s generate some linear-looking data to test this equation on ( Figure 4-1 ):\\nimport numpy as np\\nX = 2 * np.random.rand(100, 1)\\ny = 4 + 3 * X + np.random.randn(100, 1)\\n116 | Chapter 4: Training Models', 'children': []}, {'id': 88, 'title': 'Computational Complexity', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': []}]}, {'id': 89, 'title': 'Gradient Descent', 'content': 'The pseudoinverse itself is computed using a standard matrix factorization technique \\ncalled Singular Value Decomposition  (SVD) that can decompose the training set\\nmatrix X into the matrix multiplication of three matrices U Σ VT (see\\nnumpy.linalg.svd() ). The pseudoinverse is computed as X+=VΣ+UT. To compute\\nthe matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny\\nthreshold value, then it replaces all the non-zero values with their inverse, and finally\\nit transposes the resulting matrix. This approach is more efficient than computing the\\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\\nnot work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some\\nfeatures are redundant, but the pseudoinverse is always defined.\\nComputational Complexity\\nThe Normal Equation computes the inverse of XT X, which is an ( n + 1) × ( n + 1)\\nmatrix (where n is the number of features). The computational complexity  of inverting\\nsuch a matrix is typically about O(n2.4) to O(n3) (depending on the implementation).\\nIn other words, if you double the number of features, you multiply the computation\\ntime by roughly 22.4 = 5.3 to 23 = 8.\\nThe SVD approach used by Scikit-Learn’s LinearRegression  class is about O(n2). If\\nyou double the number of features, you multiply the computation time by roughly 4.\\nBoth the Normal Equation and the SVD approach get very slow\\nwhen the number of features grows large (e.g., 100,000). On the\\npositive side, both are linear with regards to the number of instan‐\\nces in the training set (they are O(m)), so they handle large training\\nsets efficiently, provided they can fit in memory.\\nAlso, once you have trained your Linear Regression model (using the Normal Equa‐\\ntion or any other algorithm), predictions are very fast: the computational complexity\\nis linear with regards to both the number of instances you want to make predictions\\non and the number of features. In other words, making predictions on twice as many\\ninstances (or twice as many features) will just take roughly twice as much time.\\nNow we will look at very different ways to train a Linear Regression model, better\\nsuited for cases where there are a large number of features, or too many training\\ninstances to fit in memory.\\nGradient Descent\\nGradient Descent  is a very generic optimization algorithm capable of finding optimal\\nsolutions to a wide range of problems. The general idea of Gradient Descent is to\\ntweak parameters iteratively in order to minimize a cost function.\\nGradient Descent | 119', 'children': [{'id': 90, 'title': 'Batch Gradient Descent', 'content': 'As you can see, on the left the Gradient Descent algorithm goes straight toward the\\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\\nalmost orthogonal to the direction of the global minimum, and it ends with a long\\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\\ntake a long time.\\nWhen using Gradient Descent, you should ensure that all features\\nhave a similar scale (e.g., using Scikit-Learn’s StandardScaler\\nclass), or else it will take much longer to converge.\\nThis diagram also illustrates the fact that training a model means searching for a\\ncombination of model parameters that minimizes a cost function (over the training\\nset). It is a search in the model’s parameter space : the more parameters a model has,\\nthe more dimensions this space has, and the harder the search is: searching for a nee‐\\ndle in a 300-dimensional haystack is much trickier than in three dimensions. Fortu‐\\nnately, since the cost function is convex in the case of Linear Regression, the needle is\\nsimply at the bottom of the bowl.\\nBatch Gradient Descent\\nTo implement Gradient Descent, you need to compute the gradient of the cost func‐\\ntion with regards to each model parameter θj. In other words, you need to calculate\\nhow much the cost function will change if you change θj just a little bit. This is called \\na partial derivative . It is like asking “what is the slope of the mountain under my feet\\nif I face east?” and then asking the same question facing north (and so on for all other\\ndimensions, if you can imagine a universe with more than three dimensions). Equa‐\\ntion 4-5  computes the partial derivative of the cost function with regards to parame‐\\nter θj, noted ∂\\n∂θj MSE( θ).\\nEquation 4-5. Partial derivatives of the cost function\\n∂\\n∂θjMSE θ=2\\nm∑\\ni= 1m\\nθTxi−yixji\\nInstead of computing these partial derivatives individually, you can use Equation 4-6\\nto compute them all in one go. The gradient vector, noted ∇θMSE( θ), contains all the\\npartial derivatives of the cost function (one for each model parameter).\\nGradient Descent | 123', 'children': []}, {'id': 91, 'title': 'Stochastic Gradient Descent', 'content': '7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\\nWhen the cost function is convex and its slope does not change abruptly (as is the\\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\\nwill eventually converge to the optimal solution, but you may have to wait a while: it\\ncan take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\\nshape of the cost function. If you divide the tolerance by 10 to have a more precise\\nsolution, then the algorithm may have to run about 10 times longer.\\nStochastic Gradient Descent\\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\\ntraining set to compute the gradients at every step, which makes it very slow when\\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent  just\\npicks a random instance in the training set at every step and computes the gradients\\nbased only on that single instance. Obviously this makes the algorithm much faster\\nsince it has very little data to manipulate at every iteration. It also makes it possible to\\ntrain on huge training sets, since only one instance needs to be in memory at each\\niteration (SGD can be implemented as an out-of-core algorithm.7)\\nOn the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\\nthe minimum, the cost function will bounce up and down, decreasing only on aver‐\\nage. Over time it will end up very close to the minimum, but once it gets there it will\\ncontinue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\\nrithm stops, the final parameter values are good, but not optimal.\\nFigure 4-9. Stochastic Gradient Descent\\n126 | Chapter 4: Training Models', 'children': []}, {'id': 92, 'title': 'Mini-batch Gradient Descent', 'content': 'Once again, you find a solution quite close to the one returned by the Normal Equa‐\\ntion:\\n>>> sgd_reg.intercept_ , sgd_reg.coef_\\n(array([4.24365286]), array([2.8250878]))\\nMini-batch Gradient Descent\\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\\nDescent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\\nent Descent: at each step, instead of computing the gradients based on the full train‐\\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\\nbatch GD computes the gradients on small random sets of instances called mini-\\nbatches . The main advantage of Mini-batch GD over Stochastic GD is that you can\\nget a performance boost from hardware optimization of matrix operations, especially\\nwhen using GPUs.\\nThe algorithm’s progress in parameter space is less erratic than with SGD, especially\\nwith fairly large mini-batches. As a result, Mini-batch GD will end up walking\\naround a bit closer to the minimum than SGD. But, on the other hand, it may be\\nharder for it to escape from local minima (in the case of problems that suffer from\\nlocal minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\\npaths taken by the three Gradient Descent algorithms in parameter space during\\ntraining. They all end up near the minimum, but Batch GD’s path actually stops at the\\nminimum, while both Stochastic GD and Mini-batch GD continue to walk around.\\nHowever, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\\ntic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\\ning schedule.\\nFigure 4-11. Gradient Descent paths in parameter space\\nGradient Descent | 129', 'children': []}]}, {'id': 93, 'title': 'Polynomial Regression', 'content': '8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\\nused to train many other models, as we will see.\\n9A quadratic equation is of the form y = ax2 + bx + c.\\nLet’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\\nm is the number of training instances and n is the number of features); see Table 4-1 .\\nTable 4-1. Comparison of algorithms for Linear Regression\\nAlgorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\\nNormal Equation Fast No Slow 0 No n/a\\nSVD Fast No Slow 0 No LinearRegression\\nBatch GD Slow No Fast 2 Yes SGDRegressor\\nStochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\\nMini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\\nThere is almost no difference after training: all these algorithms\\nend up with very similar models and make predictions in exactly \\nthe same way.\\nPolynomial Regression\\nWhat if your data is actually more complex than a simple straight line? Surprisingly,\\nyou can actually use a linear model to fit nonlinear data. A simple way to do this is to\\nadd powers of each feature as new features, then train a linear model on this extended\\nset of features. This technique is called Polynomial Regression .\\nLet’s look at an example. First, let’s generate some nonlinear data, based on a simple\\nquadratic equation9 (plus some noise; see Figure 4-12 ):\\nm = 100\\nX = 6 * np.random.rand(m, 1) - 3\\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\\n130 | Chapter 4: Training Models', 'children': []}, {'id': 94, 'title': 'Learning Curves', 'content': 'Figure 4-13. Polynomial Regression model predictions\\nNot bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\\nfunction was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\\nNote that when there are multiple features, Polynomial Regression is capable of find‐\\ning relationships between features (which is something a plain Linear Regression\\nmodel cannot do). This is made possible by the fact that PolynomialFeatures  also\\nadds all combinations of features up to the given degree. For example, if there were\\ntwo features a and b, PolynomialFeatures  with degree=3  would not only add the\\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\\nPolynomialFeatures(degree=d)  transforms an array containing n\\nfeatures into an array containing n+d!\\nd!n! features, where n! is the\\nfactorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\\nrial explosion of the number of features!\\nLearning Curves\\nIf you perform high-degree Polynomial Regression, you will likely fit the training\\ndata much better than with plain Linear Regression. For example, Figure 4-14  applies\\na 300-degree polynomial model to the preceding training data, and compares the\\nresult with a pure linear model and a quadratic model (2nd-degree polynomial).\\nNotice how the 300-degree polynomial model wiggles around to get as close as possi‐\\nble to the training instances.\\n132 | Chapter 4: Training Models', 'children': []}, {'id': 95, 'title': 'Regularized Linear Models', 'content': '10This notion of bias is not to be confused with the bias term of linear models.\\nOne way to improve an overfitting model is to feed it more training\\ndata until the validation error reaches the training error.\\nThe Bias/Variance Tradeoff\\nAn important theoretical result of statistics and Machine Learning is the fact that a\\nmodel’s generalization error can be expressed as the sum of three very different\\nerrors:\\nBias\\nThis part of the generalization error is due to wrong assumptions, such as assum‐\\ning that the data is linear when it is actually quadratic. A high-bias model is most\\nlikely to underfit the training data.10\\nVariance\\nThis part is due to the model’s excessive sensitivity to small variations in the\\ntraining data. A model with many degrees of freedom (such as a high-degree pol‐\\nynomial model) is likely to have high variance, and thus to overfit the training\\ndata.\\nIrreducible error\\nThis part is due to the noisiness of the data itself. The only way to reduce this\\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\\nsensors, or detect and remove outliers).\\nIncreasing a model’s complexity will typically increase its variance and reduce its bias.\\nConversely, reducing a model’s complexity increases its bias and reduces its variance. \\nThis is why it is called a tradeoff.\\nRegularized Linear Models\\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\\nfor it to overfit the data. For example, a simple way to regularize a polynomial model\\nis to reduce the number of polynomial degrees.\\nFor a linear model, regularization is typically achieved by constraining the weights of\\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\\nwhich implement three different ways to constrain the weights.\\n136 | Chapter 4: Training Models', 'children': [{'id': 96, 'title': 'Ridge Regression', 'content': '11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis‐\\ncussed.\\n12Norms are discussed in Chapter 2 .\\nRidge Regression\\nRidge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\\near Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \\nThis forces the learning algorithm to not only fit the data but also keep the model\\nweights as small as possible. Note that the regularization term should only be added\\nto the cost function during training. Once the model is trained, you want to evaluate\\nthe model’s performance using the unregularized performance measure.\\nIt is quite common for the cost function used during training to be\\ndifferent from the performance measure used for testing. Apart\\nfrom regularization, another reason why they might be different is\\nthat a good training cost function should have optimization-\\nfriendly derivatives, while the performance measure used for test‐\\ning should be as close as possible to the final objective. A good\\nexample of this is a classifier trained using a cost function such as\\nthe log loss (discussed in a moment) but evaluated using precision/\\nrecall.\\nThe hyperparameter α controls how much you want to regularize the model. If α = 0\\nthen Ridge Regression is just Linear Regression. If α is very large, then all weights end\\nup very close to zero and the result is a flat line going through the data’s mean. Equa‐\\ntion 4-8  presents the Ridge Regression cost function.11\\nEquation 4-8. Ridge Regression cost function\\nJθ= MSE θ+α1\\n2∑i= 1nθi2\\nNote that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\\ndefine w as the vector of feature weights ( θ1 to θn), then the regularization term is\\nsimply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\\nFor Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\\nIt is important to scale the data (e.g., using a StandardScaler ) \\nbefore performing Ridge Regression, as it is sensitive to the scale of\\nthe input features. This is true of most regularized models.\\nRegularized Linear Models | 137', 'children': []}, {'id': 97, 'title': 'Lasso Regression', 'content': '14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\\nFor more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\\nrithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\\narray([[1.55071465]])\\nAnd using Stochastic Gradient Descent:14\\n>>> sgd_reg = SGDRegressor (penalty=\"l2\")\\n>>> sgd_reg.fit(X, y.ravel())\\n>>> sgd_reg.predict([[1.5]])\\narray([1.47012588])\\nThe penalty  hyperparameter sets the type of regularization term to use. Specifying\\n\"l2\"  indicates that you want SGD to add a regularization term to the cost function \\nequal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\\nRegression.\\nLasso Regression\\nLeast Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\\nRegression ) is another regularized version of Linear Regression: just like Ridge\\nRegression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\\nof the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\\nEquation 4-10. Lasso Regression cost function\\nJθ= MSE θ+α∑i= 1nθi\\nFigure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\\nLasso models and uses smaller α values.\\nRegularized Linear Models | 139', 'children': []}, {'id': 98, 'title': 'Elastic Net', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}, {'id': 99, 'title': 'Early Stopping', 'content': 'Here is a small Scikit-Learn example using the Lasso  class. Note that you could\\ninstead use an SGDRegressor(penalty=\"l1\") .\\n>>> from sklearn.linear_model  import Lasso\\n>>> lasso_reg  = Lasso(alpha=0.1)\\n>>> lasso_reg .fit(X, y)\\n>>> lasso_reg .predict([[1.5]])\\narray([1.53788174])\\nElastic Net\\nElastic Net is a middle ground between Ridge Regression and Lasso Regression. The\\nregularization term is a simple mix of both Ridge and Lasso’s regularization terms,\\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\\nEquation 4-12. Elastic Net cost function\\nJθ= MSE θ+rα∑i= 1nθi+1 −r\\n2α∑i= 1nθi2\\nSo when should you use plain Linear Regression (i.e., without any regularization),\\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\\ndefault, but if you suspect that only a few features are actually useful, you should pre‐\\nfer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\\nzero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\\nmay behave erratically when the number of features is greater than the number of\\ntraining instances or when several features are strongly correlated.\\nHere is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\\nthe mix ratio r):\\n>>> from sklearn.linear_model  import ElasticNet\\n>>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\\n>>> elastic_net .fit(X, y)\\n>>> elastic_net .predict([[1.5]])\\narray([1.54333232])\\nEarly Stopping\\nA very different way to regularize iterative learning algorithms such as Gradient\\nDescent is to stop training as soon as the validation error reaches a minimum. This is\\ncalled early stopping . Figure 4-20  shows a complex model (in this case a high-degree\\nPolynomial Regression model) being trained using Batch Gradient Descent. As the\\nepochs go by, the algorithm learns and its prediction error (RMSE) on the training set\\nnaturally goes down, and so does its prediction error on the validation set. However,\\n142 | Chapter 4: Training Models', 'children': []}]}, {'id': 100, 'title': 'Logistic Regression', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': [{'id': 101, 'title': 'Estimating Probabilities', 'content': 'minimum_val_error  = float(\"inf\")\\nbest_epoch  = None\\nbest_model  = None\\nfor epoch in range(1000):\\n    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\\n    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\\n    val_error  = mean_squared_error (y_val, y_val_predict )\\n    if val_error  < minimum_val_error :\\n        minimum_val_error  = val_error\\n        best_epoch  = epoch\\n        best_model  = clone(sgd_reg)\\nNote that with warm_start=True , when the fit()  method is called, it just continues\\ntraining where it left off instead of restarting from scratch.\\nLogistic Regression\\nAs we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\\ntion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\\nmonly used to estimate the probability that an instance belongs to a particular class\\n(e.g., what is the probability that this email is spam?). If the estimated probability is\\ngreater than 50%, then the model predicts that the instance belongs to that class\\n(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\\nbelongs to the negative class, labeled “0”). This makes it a binary classifier.\\nEstimating Probabilities\\nSo how does it work? Just like a Linear Regression model, a Logistic Regression\\nmodel computes a weighted sum of the input features (plus a bias term), but instead\\nof outputting the result directly like the Linear Regression model does, it outputs the\\nlogistic  of this result (see Equation 4-13 ).\\nEquation 4-13. Logistic Regression model estimated probability (vectorized form)\\np=hθx=σxTθ\\nThe logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\\nbetween 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\\nEquation 4-14. Logistic function\\nσt=1\\n1 + exp −t\\n144 | Chapter 4: Training Models', 'children': []}, {'id': 102, 'title': 'Training and Cost Function', 'content': 'Figure 4-21. Logistic function\\nOnce the Logistic Regression model has estimated the probability p = hθ(x) that an\\ninstance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\\ntion 4-15 ).\\nEquation 4-15. Logistic Regression model prediction\\ny=0 if p< 0 . 5\\n1 if p≥ 0 . 5\\nNotice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\\nmodel predicts 1 if xT θ is positive, and 0 if it is negative.\\nThe score t is often called the logit : this name comes from the fact\\nthat the logit function, defined as logit( p) = log( p / (1 - p)), is the\\ninverse of the logistic function. Indeed, if you compute the logit of\\nthe estimated probability p, you will find that the result is t. The\\nlogit is also called the log-odds , since it is the log of the ratio\\nbetween the estimated probability for the positive class and the\\nestimated probability for the negative class.\\nTraining and Cost Function\\nGood, now you know how a Logistic Regression model estimates probabilities and\\nmakes predictions. But how is it trained? The objective of training is to set the param‐\\neter vector θ so that the model estimates high probabilities for positive instances ( y =\\n1) and low probabilities for negative instances ( y = 0). This idea is captured by the\\ncost function shown in Equation 4-16  for a single training instance x.\\nEquation 4-16. Cost function of a single training instance\\ncθ=−log p if\\xa0y= 1\\n−log 1 −pif\\xa0y= 0\\nLogistic Regression | 145', 'children': []}, {'id': 103, 'title': 'Decision Boundaries', 'content': 'This cost function makes sense because – log( t) grows very large when t approaches\\n0, so the cost will be large if the model estimates a probability close to 0 for a positive\\ninstance, and it will also be very large if the model estimates a probability close to 1\\nfor a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\\ninstance or close to 1 for a positive instance, which is precisely what we want.\\nThe cost function over the whole training set is simply the average cost over all train‐\\ning instances. It can be written in a single expression (as you can verify easily), called \\nthe log loss , shown in Equation 4-17 .\\nEquation 4-17. Logistic Regression cost function (log loss)\\nJθ= −1\\nm∑i= 1myilogpi+1 −yilog1 −pi\\nThe bad news is that there is no known closed-form equation to compute the value of\\nθ that minimizes this cost function (there is no equivalent of the Normal Equation).\\nBut the good news is that this cost function is convex, so Gradient Descent (or any\\nother optimization algorithm) is guaranteed to find the global minimum (if the learn‐\\ning rate is not too large and you wait long enough). The partial derivatives of the cost\\nfunction with regards to the jth model parameter θj is given by Equation 4-18 .\\nEquation 4-18. Logistic cost function partial derivatives\\n∂\\n∂θjJθ=1\\nm∑\\ni= 1m\\nσθTxi−yixji\\nThis equation looks very much like Equation 4-5 : for each instance it computes the\\nprediction error and multiplies it by the jth feature value, and then it computes the\\naverage over all training instances. Once you have the gradient vector containing all\\nthe partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\\nit: you now know how to train a Logistic Regression model. For Stochastic GD you\\nwould of course just take one instance at a time, and for Mini-batch GD you would\\nuse a mini-batch at a time.\\nDecision Boundaries\\nLet’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\\ncontains the sepal and petal length and width of 150 iris flowers of three different\\nspecies: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\\n146 | Chapter 4: Training Models', 'children': []}, {'id': 104, 'title': 'Softmax Regression', 'content': 'Figure 4-24. Linear decision boundary\\nJust like the other linear models, Logistic Regression models can be regularized using \\nℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\\nThe hyperparameter controlling the regularization strength of a\\nScikit-Learn LogisticRegression  model is not alpha  (as in other\\nlinear models), but its inverse: C. The higher the value of C, the less\\nthe model is regularized.\\nSoftmax Regression\\nThe Logistic Regression model can be generalized to support multiple classes directly,\\nwithout having to train and combine multiple binary classifiers (as discussed in\\nChapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\\nThe idea is quite simple: when given an instance x, the Softmax Regression model\\nfirst computes a score sk(x) for each class k, then estimates the probability of each\\nclass by applying the softmax  function  (also called the normalized exponential ) to the\\nscores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\\ntion for Linear Regression prediction (see Equation 4-19 ).\\nEquation 4-19. Softmax  score for class k\\nskx=xTθk\\nNote that each class has its own dedicated parameter vector θ(k). All these vectors are\\ntypically stored as rows in a parameter matrix  Θ.\\nOnce you have computed the score of every class for the instance x, you can estimate\\nthe probability pk that the instance belongs to class k by running the scores through\\nthe softmax function ( Equation 4-20 ): it computes the exponential of every score,\\nLogistic Regression | 149', 'children': []}]}, {'id': 105, 'title': 'Exercises', 'content': 'Exercises\\n1.What Linear Regression training algorithm can you use if you have a training set\\nwith millions of features?\\n2.Suppose the features in your training set have very different scales. What algo‐\\nrithms might suffer from this, and how? What can you do about it?\\n3.Can Gradient Descent get stuck in a local minimum when training a Logistic\\nRegression model?\\n4.Do all Gradient Descent algorithms lead to the same model provided you let\\nthem run long enough?\\n5.Suppose you use Batch Gradient Descent and you plot the validation error at\\nevery epoch. If you notice that the validation error consistently goes up, what is\\nlikely going on? How can you fix this?\\n6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\\ndation error goes up?\\n7.Which Gradient Descent algorithm (among those we discussed) will reach the\\nvicinity of the optimal solution the fastest? Which will actually converge? How\\ncan you make the others converge as well?\\n8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\\nyou notice that there is a large gap between the training error and the validation\\nerror. What is happening? What are three ways to solve this?\\n9.Suppose you are using Ridge Regression and you notice that the training error\\nand the validation error are almost equal and fairly high. Would you say that the\\nmodel suffers from high bias or high variance? Should you increase the regulari‐\\nzation hyperparameter α or reduce it?\\n10.Why would you want to use:\\n•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\\nzation)?\\n•Lasso instead of Ridge Regression?\\n•Elastic Net instead of Lasso?\\n11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\\nShould you implement two Logistic Regression classifiers or one Softmax Regres‐\\nsion classifier?\\n12.Implement Batch Gradient Descent with early stopping for Softmax Regression \\n(without using Scikit-Learn).\\nSolutions to these exercises are available in ???.\\nExercises | 153', 'children': []}]}, {'id': 106, 'title': 'Chapter 5. Support Vector Machines', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 107, 'title': 'Linear SVM Classification', 'content': 'CHAPTER 5\\nSupport Vector Machines\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 5 in the final\\nrelease of the book.\\nA Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\\nmodel, capable of performing linear or nonlinear classification, regression, and even\\noutlier detection. It is one of the most popular models in Machine Learning, and any‐\\none interested in Machine Learning should have it in their toolbox. SVMs are partic‐\\nularly well suited for classification of complex but small- or medium-sized datasets.\\nThis chapter will explain the core concepts of SVMs, how to use them, and how they\\nwork.\\nLinear SVM Classification\\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\\nshows part of the iris dataset that was introduced at the end of Chapter 4 . The two\\nclasses can clearly be separated easily with a straight line (they are linearly separable ).\\nThe left plot shows the decision boundaries of three possible linear classifiers. The\\nmodel whose decision boundary is represented by the dashed line is so bad that it\\ndoes not even separate the classes properly. The other two models work perfectly on\\nthis training set, but their decision boundaries come so close to the instances that\\nthese models will probably not perform as well on new instances. In contrast, the\\nsolid line in the plot on the right represents the decision boundary of an SVM classi‐\\nfier; this line not only separates the two classes but also stays as far away from the\\nclosest training instances as possible. Y ou can think of an SVM classifier as fitting the\\n155', 'children': [{'id': 108, 'title': 'Soft Margin Classification', 'content': 'widest possible street (represented by the parallel dashed lines) between the classes.\\nThis is called large margin classification .\\nFigure 5-1. Large margin classification\\nNotice that adding more training instances “off the street” will not affect the decision\\nboundary at all: it is fully determined (or “supported”) by the instances located on the\\nedge of the street. These instances are called the support vectors  (they are circled in\\nFigure 5-1 ).\\nSVMs are sensitive to the feature scales, as you can see in\\nFigure 5-2 : on the left plot, the vertical scale is much larger than the\\nhorizontal scale, so the widest possible street is close to horizontal.\\nAfter feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \\nthe decision boundary looks much better (on the right plot).\\nFigure 5-2. Sensitivity to feature scales\\nSoft Margin Classification\\nIf we strictly impose that all instances be off the street and on the right side, this is\\ncalled hard margin classification . There are two main issues with hard margin classifi‐\\ncation. First, it only works if the data is linearly separable, and second it is quite sensi‐\\ntive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\\nthe left, it is impossible to find a hard margin, and on the right the decision boundary\\nends up very different from the one we saw in Figure 5-1  without the outlier, and it\\nwill probably not generalize as well.\\n156 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 109, 'title': 'Nonlinear SVM Classification', 'content': 'Nonlinear SVM Classification\\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\\ncases, many datasets are not even close to being linearly separable. One approach to\\nhandling nonlinear datasets is to add more features, such as polynomial features (as\\nyou did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\\nConsider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\\nx1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\\nture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\\nFigure 5-5. Adding features to make a dataset linearly separable\\nTo implement this idea using Scikit-Learn, you can create a Pipeline  containing a\\nPolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\\n130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\\ndataset: this is a toy dataset for binary classification in which the data points are sha‐\\nped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\\nusing the make_moons()  function:\\nfrom sklearn.datasets  import make_moons\\nfrom sklearn.pipeline  import Pipeline\\nfrom sklearn.preprocessing  import PolynomialFeatures\\npolynomial_svm_clf  = Pipeline ([\\n        (\"poly_features\" , PolynomialFeatures (degree=3)),\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\\n    ])\\npolynomial_svm_clf .fit(X, y)\\nNonlinear SVM Classification  | 159', 'children': [{'id': 110, 'title': 'Polynomial Kernel', 'content': 'Figure 5-6. Linear SVM classifier  using polynomial features\\nPolynomial Kernel\\nAdding polynomial features is simple to implement and can work great with all sorts\\nof Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\\ncannot deal with very complex datasets, and with a high polynomial degree it creates\\na huge number of features, making the model too slow.\\nFortunately, when using SVMs you can apply an almost miraculous mathematical\\ntechnique called the kernel trick  (it is explained in a moment). It makes it possible to\\nget the same result as if you added many polynomial features, even with very high-\\ndegree polynomials, without actually having to add them. So there is no combinato‐\\nrial explosion of the number of features since you don’t actually add any features. This\\ntrick is implemented by the SVC class. Let’s test it on the moons dataset:\\nfrom sklearn.svm  import SVC\\npoly_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\\n    ])\\npoly_kernel_svm_clf .fit(X, y)\\nThis code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\\nsented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\\ndegree polynomial kernel. Obviously, if your model is overfitting, you might want to\\n160 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 111, 'title': 'Adding Similarity Features', 'content': 'reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\\nit. The hyperparameter coef0  controls how much the model is influenced by high-\\ndegree polynomials versus low-degree polynomials.\\nFigure 5-7. SVM classifiers  with a polynomial kernel\\nA common approach to find the right hyperparameter values is to\\nuse grid search (see Chapter 2 ). It is often faster to first do a very\\ncoarse grid search, then a finer grid search around the best values\\nfound. Having a good sense of what each hyperparameter actually\\ndoes can also help you search in the right part of the hyperparame‐\\nter space.\\nAdding Similarity Features\\nAnother technique to tackle nonlinear problems is to add features computed using a\\nsimilarity function  that measures how much each instance resembles a particular\\nlandmark . For example, let’s take the one-dimensional dataset discussed earlier and\\nadd two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\\nlet’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\\nwith γ = 0.3 (see Equation 5-1 ).\\nEquation 5-1. Gaussian RBF\\nϕγx, ℓ= exp −γ∥x− ℓ∥2\\nIt is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\\nthe landmark). Now we are ready to compute the new features. For example, let’s look\\nat the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\\nfrom the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\\nand x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\\nformed dataset (dropping the original features). As you can see, it is now linearly\\nseparable.\\nNonlinear SVM Classification  | 161', 'children': []}, {'id': 112, 'title': 'Gaussian RBF Kernel', 'content': 'Figure 5-8. Similarity features using the Gaussian RBF\\nY ou may wonder how to select the landmarks. The simplest approach is to create a\\nlandmark at the location of each and every instance in the dataset. This creates many\\ndimensions and thus increases the chances that the transformed training set will be\\nlinearly separable. The downside is that a training set with m instances and n features\\ngets transformed into a training set with m instances and m features (assuming you\\ndrop the original features). If your training set is very large, you end up with an\\nequally large number of features.\\nGaussian RBF Kernel\\nJust like the polynomial features method, the similarity features method can be useful\\nwith any Machine Learning algorithm, but it may be computationally expensive to\\ncompute all the additional features, especially on large training sets. However, once\\nagain the kernel trick does its SVM magic: it makes it possible to obtain a similar\\nresult as if you had added many similarity features, without actually having to add\\nthem. Let’s try the Gaussian RBF kernel using the SVC class:\\nrbf_kernel_svm_clf  = Pipeline ([\\n        (\"scaler\" , StandardScaler ()),\\n        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\\n    ])\\nrbf_kernel_svm_clf .fit(X, y)\\nThis model is represented on the bottom left of Figure 5-9 . The other plots show\\nmodels trained with different values of hyperparameters gamma  (γ) and C. Increasing\\ngamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\\nresult each instance’s range of influence is smaller: the decision boundary ends up\\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma  \\nvalue makes the bell-shaped curve wider, so instances have a larger range of influ‐\\nence, and the decision boundary ends up smoother. So γ acts like a regularization\\nhyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\\nfitting, you should increase it (similar to the C hyperparameter).\\n162 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 113, 'title': 'Computational Complexity', 'content': '1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\\nFigure 5-9. SVM classifiers  using an RBF kernel\\nOther kernels exist but are used much more rarely. For example, some kernels are\\nspecialized for specific data structures. String kernels  are sometimes used when classi‐\\nfying text documents or DNA sequences (e.g., using the string subsequence kernel  or\\nkernels based on the Levenshtein distance ).\\nWith so many kernels to choose from, how can you decide which\\none to use? As a rule of thumb, you should always try the linear\\nkernel first (remember that LinearSVC  is much faster than SVC(ker\\nnel=\"linear\") ), especially if the training set is very large or if it\\nhas plenty of features. If the training set is not too large, you should\\ntry the Gaussian RBF kernel as well; it works well in most cases.\\nThen if you have spare time and computing power, you can also\\nexperiment with a few other kernels using cross-validation and grid\\nsearch, especially if there are kernels specialized for your training\\nset’s data structure.\\nComputational Complexity\\nThe LinearSVC  class is based on the liblinear  library, which implements an optimized\\nalgorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\\nNonlinear SVM Classification  | 163', 'children': []}]}, {'id': 114, 'title': 'SVM Regression', 'content': '2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\\ntime complexity is roughly O(m × n).\\nThe algorithm takes longer if you require a very high precision. This is controlled by\\nthe tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\\ntasks, the default tolerance is fine.\\nThe SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\\nports the kernel trick.2 The training time complexity is usually between O(m2 × n)\\nand O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\\nber of training instances gets large (e.g., hundreds of thousands of instances). This\\nalgorithm is perfect for complex but small or medium training sets. However, it scales\\nwell with the number of features, especially with sparse features  (i.e., when each\\ninstance has few nonzero features). In this case, the algorithm scales roughly with the\\naverage number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\\nSVM classification classes.\\nTable 5-1. Comparison of Scikit-Learn classes for SVM classification\\nClass Time complexity Out-of-core support Scaling required Kernel trick\\nLinearSVC O(m × n) No Yes No\\nSGDClassifier O(m × n) Yes Yes No\\nSVC O(m² × n) to O( m³ × n)No Yes Yes\\nSVM Regression\\nAs we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\\nport linear and nonlinear classification, but it also supports linear and nonlinear\\nregression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\\nsible street between two classes while limiting margin violations, SVM Regression\\ntries to fit as many instances as possible on the street while limiting margin violations\\n(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\\nter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\\nlinear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\\n0.5).\\n164 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 115, 'title': 'Under the Hood', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': [{'id': 116, 'title': 'Decision Function and Predictions', 'content': 'The following code produces the model represented on the left of Figure 5-11  using\\nScikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\\nsion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\\nof the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\\ning set (just like the LinearSVC  class), while the SVR class gets much too slow when\\nthe training set grows large (just like the SVC class).\\nfrom sklearn.svm  import SVR\\nsvm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\\nsvm_poly_reg .fit(X, y)\\nSVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\\numentation for more details.\\nUnder the Hood\\nThis section explains how SVMs make predictions and how their training algorithms\\nwork, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\\nexercises at the end of this chapter if you are just getting started with Machine Learn‐\\ning, and come back later when you want to get a deeper understanding of SVMs.\\nFirst, a word about notations: in Chapter 4  we used the convention of putting all the \\nmodel parameters in one vector θ, including the bias term θ0 and the input feature\\nweights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\\nuse a different convention, which is more convenient (and more common) when you\\nare dealing with SVMs: the bias term will be called b and the feature weights vector\\nwill be called w. No bias feature will be added to the input feature vectors.\\nDecision Function and Predictions\\nThe linear SVM classifier model predicts the class of a new instance x by simply com‐\\nputing the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\\nthe predicted class ŷ is the positive class (1), or else it is the negative class (0); see\\nEquation 5-2 .\\nEquation 5-2. Linear SVM classifier  prediction\\ny=0 if wTx+b< 0,\\n1 if wTx+b≥ 0\\n166 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 117, 'title': 'Training Objective', 'content': '3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\\nsion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\\nFigure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\\nwidth and petal length). The decision boundary is the set of points where the decision\\nfunction is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\\nresented by the thick solid line).3\\nFigure 5-12. Decision function for the iris dataset\\nThe dashed lines represent the points where the decision function is equal to 1 or –1:\\nthey are parallel and at equal distance to the decision boundary, forming a margin\\naround it. Training a linear SVM classifier means finding the value of w and b that\\nmake this margin as wide as possible while avoiding margin violations (hard margin)\\nor limiting them (soft margin).\\nTraining Objective\\nConsider the slope of the decision function: it is equal to the norm of the weight vec‐\\ntor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\\nto ±1 are going to be twice as far away from the decision boundary. In other words,\\ndividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\\nize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\\nUnder the Hood | 167', 'children': []}, {'id': 118, 'title': 'Quadratic Programming', 'content': '5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\\nberghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\\nseries of video lectures .off between these two objectives. This gives us the constrained optimization problem\\nin Equation 5-4 .\\nEquation 5-4. Soft margin linear SVM classifier  objective\\nminimizew,b,ζ1\\n2wTw+C∑\\ni= 1m\\nζi\\nsubject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\\nQuadratic Programming\\nThe hard margin and soft margin problems are both convex quadratic optimization\\nproblems with linear constraints. Such problems are known as Quadratic Program‐\\nming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\\nusing a variety of techniques that are outside the scope of this book.5 The general\\nproblem formulation is given by Equation 5-5 .\\nEquation 5-5. Quadratic Programming problem\\nMinimize\\np1\\n2pTHp + fTp\\nsubject to Ap≤b\\nwherepis an np‐dimensional vector ( np= number of parameters),\\nHis an np×npmatrix,\\nfis an np‐dimensional vector,\\nAis an nc×npmatrix ( nc= number of constraints),\\nbis an nc‐dimensional vector.\\nNote that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\\n2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\\nthe ith element of b.\\nY ou can easily verify that if you set the QP parameters in the following way, you get\\nthe hard margin linear SVM classifier objective:\\n•np = n + 1, where n is the number of features (the +1 is for the bias term).\\nUnder the Hood | 169', 'children': []}, {'id': 119, 'title': 'The Dual Problem', 'content': '6The objective function is convex, and the inequality constraints are continuously differentiable and convex\\nfunctions.•nc = m, where m is the number of training instances.\\n•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\\nthe bias term).\\n•f = 0, an np-dimensional vector full of 0s.\\n•b = –1, an nc-dimensional vector full of –1s.\\n•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\\nSo one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\\nQP solver by passing it the preceding parameters. The resulting vector p will contain\\nthe bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\\ncan use a QP solver to solve the soft margin problem (see the exercises at the end of\\nthe chapter).\\nHowever, to use the kernel trick we are going to look at a different constrained opti‐\\nmization problem.\\nThe Dual Problem\\nGiven a constrained optimization problem, known as the primal problem , it is possi‐\\nble to express a different but closely related problem, called its dual problem . The sol‐\\nution to the dual problem typically gives a lower bound to the solution of the primal\\nproblem, but under some conditions it can even have the same solutions as the pri‐\\nmal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\\ncan choose to solve the primal problem or the dual problem; both will have the same\\nsolution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\\ninterested in knowing how to derive the dual problem from the primal problem,\\nsee ???).\\nEquation 5-6. Dual form of the linear SVM objective\\nminimizeα1\\n2∑\\ni= 1m\\n∑\\nj= 1m\\nαiαjtitjxiTxj−∑\\ni= 1m\\nαi\\nsubject to αi≥ 0 for i= 1, 2,⋯,m\\n170 | Chapter 5: Support Vector Machines', 'children': []}, {'id': 120, 'title': 'Kernelized SVM', 'content': '7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\\ndot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\\ncompute w and b that minimize the primal problem by using Equation 5-7 .\\nEquation 5-7. From the dual solution to the primal solution\\nw=∑\\ni= 1m\\nαitixi\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTxi\\nThe dual problem is faster to solve than the primal when the number of training\\ninstances is smaller than the number of features. More importantly, it makes the ker‐\\nnel trick possible, while the primal does not. So what is this kernel trick anyway?\\nKernelized SVM\\nSuppose you want to apply a 2nd-degree polynomial transformation to a two-\\ndimensional training set (such as the moons training set), then train a linear SVM\\nclassifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\\nmial mapping function ϕ that you want to apply.\\nEquation 5-8. Second-degree polynomial mapping\\nϕx=ϕx1\\nx2=x12\\n2x1x2\\nx22\\nNotice that the transformed vector is three-dimensional instead of two-dimensional.\\nNow let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\\napply this 2nd-degree polynomial mapping and then compute the dot product7 of the\\ntransformed vectors (See Equation 5-9 ).\\nUnder the Hood | 171', 'children': []}, {'id': 121, 'title': 'Online SVMs', 'content': 'Equation 5-12. Computing the bias term using the kernel trick\\nb=1\\nns∑\\ni= 1\\nαi> 0m\\nti−wTϕxi=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1m\\nαjtjϕxjT\\nϕxi\\n=1\\nns∑\\ni= 1\\nαi> 0m\\nti−∑\\nj= 1\\nαj> 0m\\nαjtjKxi,xj\\nIf you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\\neffect of the kernel trick.\\nOnline SVMs\\nBefore concluding this chapter, let’s take a quick look at online SVM classifiers (recall\\nthat online learning means learning incrementally, typically as new instances arrive).\\nFor linear SVM classifiers, one method is to use Gradient Descent (e.g., using\\nSGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\\nfrom the primal problem. Unfortunately it converges much more slowly than the\\nmethods based on QP .\\nEquation 5-13. Linear SVM classifier  cost function\\nJw,b=1\\n2wTw + C∑\\ni= 1m\\nmax 0, 1 − tiwTxi+b\\nThe first sum in the cost function will push the model to have a small weight vector\\nw, leading to a larger margin. The second sum computes the total of all margin viola‐\\ntions. An instance’s margin violation is equal to 0 if it is located off the street and on\\nthe correct side, or else it is proportional to the distance to the correct side of the\\nstreet. Minimizing this term ensures that the model makes the margin violations as\\nsmall and as few as possible\\nHinge Loss\\nThe function max (0, 1 – t) is called the hinge loss  function (represented below). It is\\nequal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\\ndifferentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\\npage 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\\nvalue between –1 and 0).\\n174 | Chapter 5: Support Vector Machines', 'children': []}]}, {'id': 122, 'title': 'Exercises', 'content': '8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\\n9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\\nIt is also possible to implement online kernelized SVMs—for example, using “Incre‐\\nmental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\\nActive Learning. ”9 However, these are implemented in Matlab and C++. For large-\\nscale nonlinear problems, you may want to consider using neural networks instead \\n(see Part II ).\\nExercises\\n1.What is the fundamental idea behind Support Vector Machines?\\n2.What is a support vector?\\n3.Why is it important to scale the inputs when using SVMs?\\n4.Can an SVM classifier output a confidence score when it classifies an instance?\\nWhat about a probability?\\n5.Should you use the primal or the dual form of the SVM problem to train a model\\non a training set with millions of instances and hundreds of features?\\n6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\\ntraining set: should you increase or decrease γ (gamma )? What about C?\\n7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\\nlinear SVM classifier problem using an off-the-shelf QP solver?\\n8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\\nSGDClassifier  on the same dataset. See if you can get them to produce roughly\\nthe same model.\\n9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\\nclassifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\\nExercises | 175', 'children': []}]}, {'id': 123, 'title': 'Chapter 6. Decision Trees', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': [{'id': 124, 'title': 'Training and Visualizing a Decision Tree', 'content': 'CHAPTER 6\\nDecision Trees\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 6 in the final\\nrelease of the book.\\nLike SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\\nform both classification and regression tasks, and even multioutput tasks. They are\\nvery powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\\nter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\\nfitting it perfectly (actually overfitting it).\\nDecision Trees are also the fundamental components of Random Forests (see Chap‐\\nter 7 ), which are among the most powerful Machine Learning algorithms available\\ntoday.\\nIn this chapter we will start by discussing how to train, visualize, and make predic‐\\ntions with Decision Trees. Then we will go through the CART training algorithm\\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees.\\nTraining and Visualizing a Decision Tree\\nTo understand Decision Trees, let’s just build one and take a look at how it makes pre‐\\ndictions. The following code trains a DecisionTreeClassifier  on the iris dataset\\n(see Chapter 4 ):\\nfrom sklearn.datasets  import load_iris\\nfrom sklearn.tree  import DecisionTreeClassifier\\n177', 'children': []}, {'id': 125, 'title': 'Making Predictions', 'content': 'Figure 6-1. Iris Decision Tree\\nMaking Predictions\\nLet’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\\nan iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\\ntop): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\\nthen you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\\nnode  (i.e., it does not have any children nodes), so it does not ask any questions: you\\ncan simply look at the predicted class for that node and the Decision Tree predicts\\nthat your flower is an Iris-Setosa ( class=setosa ).\\nNow suppose you find another flower, but this time the petal length is greater than\\n2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\\nnot a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\\nit is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\\nan Iris-Virginica (depth 2, right). It’s really that simple.\\nOne of the many qualities of Decision Trees is that they require\\nvery little data preparation. In particular, they don’t require feature\\nscaling or centering at all.\\nMaking Predictions | 179', 'children': []}, {'id': 126, 'title': 'Estimating Class Probabilities', 'content': 'Figure 6-2. Decision Tree decision boundaries\\nModel Interpretation: White Box Versus Black Box\\nAs you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\\npret. Such models are often called white box models . In contrast, as we will see, Ran‐\\ndom Forests or neural networks are generally considered black box models . They\\nmake great predictions, and you can easily check the calculations that they performed\\nto make these predictions; nevertheless, it is usually hard to explain in simple terms\\nwhy the predictions were made. For example, if a neural network says that a particu‐\\nlar person appears on a picture, it is hard to know what actually contributed to this\\nprediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\\nshoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\\nnice and simple classification rules that can even be applied manually if need be (e.g.,\\nfor flower classification).\\nEstimating Class Probabilities\\nA Decision Tree can also estimate the probability that an instance belongs to a partic‐\\nular class k: first it traverses the tree to find the leaf node for this instance, and then it\\nreturns the ratio of training instances of class k in this node. For example, suppose\\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\\nsponding leaf node is the depth-2 left node, so the Decision Tree should output the\\nfollowing probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\\nand 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\\nshould output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\\nthis:\\n>>> tree_clf .predict_proba ([[5, 1.5]])\\narray([[0.        , 0.90740741, 0.09259259]])\\nEstimating Class Probabilities | 181', 'children': []}, {'id': 127, 'title': 'The CART Training Algorithm', 'content': '>>> tree_clf .predict([[5, 1.5]])\\narray([1])\\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\\nthe bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\\nVirginica in this case).\\nThe CART Training Algorithm\\nScikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\\nDecision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\\nrithm first splits the training set in two subsets using a single feature k and a thres‐\\nhold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\\npair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\\nthat the algorithm tries to minimize is given by Equation 6-2 .\\nEquation 6-2. CART cost function for classification\\nJk,tk=mleft\\nmGleft+mright\\nmGright\\nwhereGleft/rightmeasures the impurity of the left/right subset,\\nmleft/rightis the number of instances in the left/right subset.\\nOnce it has successfully split the training set in two, it splits the subsets using the\\nsame logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\\nches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\\nfind a split that will reduce impurity. A few other hyperparameters (described in a\\nmoment) control additional stopping conditions ( min_samples_split , min_sam\\nples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\\n182 | Chapter 6: Decision Trees', 'children': []}, {'id': 128, 'title': 'Computational Complexity', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 129, 'title': 'Gini Impurity or Entropy?', 'content': '2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\\ntion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\\nfound for any NP-Complete problem (except perhaps on a quantum computer).\\n3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\\n4A reduction of entropy is often called an information gain .\\nAs you can see, the CART algorithm is a greedy algorithm : it greed‐\\nily searches for an optimum split at the top level, then repeats the\\nprocess at each level. It does not check whether or not the split will\\nlead to the lowest possible impurity several levels down. A greedy\\nalgorithm often produces a reasonably good solution, but it is not\\nguaranteed to be the optimal solution.\\nUnfortunately, finding the optimal tree is known to be an NP-\\nComplete  problem:2 it requires O(exp( m)) time, making the prob‐\\nlem intractable even for fairly small training sets. This is why we\\nmust settle for a “reasonably good” solution.\\nComputational Complexity\\nMaking predictions requires traversing the Decision Tree from the root to a leaf.\\nDecision Trees are generally approximately balanced, so traversing the Decision Tree\\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\\nchecking the value of one feature, the overall prediction complexity is just O(log2(m)),\\nindependent of the number of features. So predictions are very fast, even when deal‐\\ning with large training sets.\\nHowever, the training algorithm compares all features (or less if max_features  is set)\\non all samples at each node. This results in a training complexity of O(n × m log(m)).\\nFor small training sets (less than a few thousand instances), Scikit-Learn can speed up\\ntraining by presorting the data (set presort=True ), but this slows down training con‐\\nsiderably for larger training sets.\\nGini Impurity or Entropy?\\nBy default, the Gini impurity measure is used, but you can select the entropy  impurity\\nmeasure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\\nof entropy originated in thermodynamics as a measure of molecular disorder:\\nentropy approaches zero when molecules are still and well ordered. It later spread to a\\nwide variety of domains, including Shannon’s information theory , where it measures\\nthe average information content of a message:4 entropy is zero when all messages are\\nidentical. In Machine Learning, it is frequently used as an impurity measure: a set’s\\nComputational Complexity | 183', 'children': []}, {'id': 130, 'title': 'Regularization Hyperparameters', 'content': '5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\\ndefinition of the entropy of the ith node. For example, the depth-2 left node in\\nFigure 6-1  has an entropy equal to −49\\n54log249\\n54−5\\n54log25\\n54 ≈ 0.445.\\nEquation 6-3. Entropy\\nHi= − ∑\\nk= 1\\npi,k≠ 0n\\npi,klog2pi,k\\nSo should you use Gini impurity or entropy? The truth is, most of the time it does not\\nmake a big difference: they lead to similar trees. Gini impurity is slightly faster to\\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\\nisolate the most frequent class in its own branch of the tree, while entropy tends to\\nproduce slightly more balanced trees.5\\nRegularization Hyperparameters\\nDecision Trees make very few assumptions about the training data (as opposed to lin‐\\near models, which obviously assume that the data is linear, for example). If left\\nunconstrained, the tree structure will adapt itself to the training data, fitting it very\\nclosely, and most likely overfitting it. Such a model is often called a nonparametric\\nmodel , not because it does not have any parameters (it often has a lot) but because the\\nnumber of parameters is not determined prior to training, so the model structure is\\nfree to stick closely to the data. In contrast, a parametric model  such as a linear model\\nhas a predetermined number of parameters, so its degree of freedom is limited,\\nreducing the risk of overfitting (but increasing the risk of underfitting).\\nTo avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\\nduring training. As you know by now, this is called regularization. The regularization\\nhyperparameters depend on the algorithm used, but generally you can at least restrict\\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\\nmax_depth  hyperparameter (the default value is None , which means unlimited).\\nReducing max_depth  will regularize the model and thus reduce the risk of overfitting.\\nThe DecisionTreeClassifier  class has a few other parameters that similarly restrict\\nthe shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\\nples a node must have before it can be split), min_samples_leaf  (the minimum num‐\\nber of samples a leaf node must have), min_weight_fraction_leaf  (same as\\nmin_samples_leaf  but expressed as a fraction of the total number of weighted\\n184 | Chapter 6: Decision Trees', 'children': []}, {'id': 131, 'title': 'Regression', 'content': 'instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\\n(maximum number of features that are evaluated for splitting at each node). Increas‐\\ning min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\\nmodel.\\nOther algorithms work by first training the Decision Tree without\\nrestrictions, then pruning  (deleting) unnecessary nodes. A node\\nwhose children are all leaf nodes is considered unnecessary if the\\npurity improvement it provides is not statistically significant . Stan‐\\ndard statistical tests, such as the χ2 test, are used to estimate the\\nprobability that the improvement is purely the result of chance\\n(which is called the null hypothesis ). If this probability, called the p-\\nvalue , is higher than a given threshold (typically 5%, controlled by\\na hyperparameter), then the node is considered unnecessary and its\\nchildren are deleted. The pruning continues until all unnecessary\\nnodes have been pruned.\\nFigure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\\nChapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\\n(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\\nples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\\nmodel on the right will probably generalize better.\\nFigure 6-3. Regularization using min_samples_leaf\\nRegression\\nDecision Trees are also capable of performing regression tasks. Let’s build a regres‐\\nsion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\\nquadratic dataset with max_depth=2 :\\nfrom sklearn.tree  import DecisionTreeRegressor\\nRegression | 185', 'children': []}, {'id': 132, 'title': 'Instability', 'content': '6It randomly selects the set of features to evaluate at each node.Instability\\nHopefully by now you are convinced that Decision Trees have a lot going for them:\\nthey are simple to understand and interpret, easy to use, versatile, and powerful.\\nHowever they do have a few limitations. First, as you may have noticed, Decision\\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\\nwhich makes them sensitive to training set rotation. For example, Figure 6-7  shows a\\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\\non the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\\nsarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\\nlikely that the model on the right will not generalize well. One way to limit this prob‐\\nlem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\\ntraining data.\\nFigure 6-7. Sensitivity to training set rotation\\nMore generally, the main issue with Decision Trees is that they are very sensitive to\\nsmall variations in the training data. For example, if you just remove the widest Iris-\\nVersicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\\nand train a new Decision Tree, you may get the model represented in Figure 6-8 . As\\nyou can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\\nActually, since the training algorithm used by Scikit-Learn is stochastic6 you may\\nget very different models even on the same training data (unless you set the\\nrandom_state  hyperparameter).\\n188 | Chapter 6: Decision Trees', 'children': []}, {'id': 133, 'title': 'Exercises', 'content': 'Figure 6-8. Sensitivity to training set details\\nRandom Forests can limit this instability by averaging predictions over many trees, as\\nwe will see in the next chapter.\\nExercises\\n1.What is the approximate depth of a Decision Tree trained (without restrictions)\\non a training set with 1 million instances?\\n2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\\nally lower/greater, or always  lower/greater?\\n3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\\nmax_depth ?\\n4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\\nthe input features?\\n5.If it takes one hour to train a Decision Tree on a training set containing 1 million\\ninstances, roughly how much time will it take to train another Decision Tree on a\\ntraining set containing 10 million instances?\\n6.If your training set contains 100,000 instances, will setting presort=True  speed\\nup training?\\n7.Train and fine-tune a Decision Tree for the moons dataset.\\na.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\\nb.Split it into a training set and a test set using train_test_split() .\\nExercises | 189', 'children': []}]}, {'id': 134, 'title': 'Chapter 7. Ensemble Learning and Random Forests', 'content': 'CHAPTER 7\\nEnsemble Learning and Random Forests\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 7 in the final\\nrelease of the book.\\nSuppose you ask a complex question to thousands of random people, then aggregate\\ntheir answers. In many cases you will find that this aggregated answer is better than\\nan expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\\nthe predictions of a group of predictors (such as classifiers or regressors), you will\\noften get better predictions than with the best individual predictor. A group of pre‐\\ndictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\\nEnsemble Learning algorithm is called an Ensemble method .\\nFor example, you can train a group of Decision Tree classifiers, each on a different\\nrandom subset of the training set. To make predictions, you just obtain the predic‐\\ntions of all individual trees, then predict the class that gets the most votes (see the last\\nexercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \\nand despite its simplicity, this is one of the most powerful Machine Learning algo‐\\nrithms available today.\\nMoreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\\nthe end of a project, once you have already built a few good predictors, to combine\\nthem into an even better predictor. In fact, the winning solutions in Machine Learn‐\\ning competitions often involve several Ensemble methods (most famously in the Net‐\\nflix Prize competition ).\\nIn this chapter we will discuss the most popular Ensemble methods, including bag‐\\nging, boosting , stacking , and a few others. We will also explore Random Forests.\\n191', 'children': [{'id': 135, 'title': 'Voting Classifiers', 'content': 'Voting Classifiers\\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy.\\nY ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\\nFigure 7-1. Training diverse classifiers\\nA very simple way to create an even better classifier is to aggregate the predictions of\\neach classifier and predict the class that gets the most votes. This majority-vote classi‐\\nfier is called a hard voting  classifier (see Figure 7-2 ).\\nFigure 7-2. Hard voting classifier  predictions\\n192 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 136, 'title': 'Bagging and Pasting', 'content': '1“Bagging Predictors, ” L. Breiman (1996).\\n2In statistics, resampling with replacement is called bootstrapping .\\n3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\\nachieves over 91.2% accuracy!\\nBagging and Pasting\\nOne way to get a diverse set of classifiers is to use very different training algorithms,\\nas just discussed. Another approach is to use the same training algorithm for every\\npredictor, but to train them on different random subsets of the training set. When\\nsampling is performed with  replacement, this method is called bagging1 (short for\\nbootstrap aggregating2). When sampling is performed without  replacement, it is called\\npasting .3\\nIn other words, both bagging and pasting allow training instances to be sampled sev‐\\neral times across multiple predictors, but only bagging allows training instances to be\\nsampled several times for the same predictor. This sampling and training process is\\nrepresented in Figure 7-4 .\\nFigure 7-4. Pasting/bagging training set sampling and training\\nOnce all predictors are trained, the ensemble can make a prediction for a new\\ninstance by simply aggregating the predictions of all predictors. The aggregation\\nfunction is typically the statistical mode  (i.e., the most frequent prediction, just like a\\nhard voting classifier) for classification, or the average for regression. Each individual\\nBagging and Pasting | 195', 'children': [{'id': 137, 'title': 'Bagging and Pasting in Scikit-Learn', 'content': '4Bias and variance were introduced in Chapter 4 .\\n5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\\nto sample is equal to the size of the training set times max_samples .\\npredictor has a higher bias than if it were trained on the original training set, but\\naggregation reduces both bias and variance.4 Generally, the net result is that the\\nensemble has a similar bias but a lower variance than a single predictor trained on the\\noriginal training set.\\nAs you can see in Figure 7-4 , predictors can all be trained in parallel, via different\\nCPU cores or even different servers. Similarly, predictions can be made in parallel.\\nThis is one of the reasons why bagging and pasting are such popular methods: they\\nscale very well.\\nBagging and Pasting in Scikit-Learn\\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\\nsifier  class (or BaggingRegressor  for regression). The following code trains an\\nensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\\ndomly sampled from the training set with replacement (this is an example of bagging,\\nbut if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\\neter tells Scikit-Learn the number of CPU cores to use for training and predictions\\n(–1 tells Scikit-Learn to use all available cores):\\nfrom sklearn.ensemble  import BaggingClassifier\\nfrom sklearn.tree  import DecisionTreeClassifier\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (), n_estimators =500,\\n    max_samples =100, bootstrap =True, n_jobs=-1)\\nbag_clf.fit(X_train, y_train)\\ny_pred = bag_clf.predict(X_test)\\nThe BaggingClassifier  automatically performs soft voting\\ninstead of hard voting if the base classifier can estimate class proba‐\\nbilities (i.e., if it has a predict_proba()  method), which is the case\\nwith Decision Trees classifiers.\\nFigure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\\ntrained on the moons dataset. As you can see, the ensemble’s predictions will likely\\ngeneralize much better than the single Decision Tree’s predictions: the ensemble has a\\ncomparable bias but a smaller variance (it makes roughly the same number of errors\\non the training set, but the decision boundary is less irregular).\\n196 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 138, 'title': 'Out-of-Bag Evaluation', 'content': '6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\\nFigure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\\ntrained on, so bagging ends up with a slightly higher bias than pasting, but this also\\nmeans that predictors end up being less correlated so the ensemble’s variance is\\nreduced. Overall, bagging often results in better models, which explains why it is gen‐\\nerally preferred. However, if you have spare time and CPU power you can use cross-\\nvalidation to evaluate both bagging and pasting and select the one that works best.\\nOut-of-Bag Evaluation\\nWith bagging, some instances may be sampled several times for any given predictor,\\nwhile others may not be sampled at all. By default a BaggingClassifier  samples m\\ntraining instances with replacement ( bootstrap=True ), where m is the size of the\\ntraining set. This means that only about 63% of the training instances are sampled on\\naverage for each predictor.6 The remaining 37% of the training instances that are not\\nsampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\\nfor all predictors.\\nSince a predictor never sees the oob instances during training, it can be evaluated on\\nthese instances, without the need for a separate validation set. Y ou can evaluate the\\nensemble itself by averaging out the oob evaluations of each predictor.\\nIn Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\\nrequest an automatic oob evaluation after training. The following code demonstrates\\nthis. The resulting evaluation score is available through the oob_score_  variable:\\n>>> bag_clf = BaggingClassifier (\\n...     DecisionTreeClassifier (), n_estimators =500,\\n...     bootstrap =True, n_jobs=-1, oob_score =True)\\n...\\n>>> bag_clf.fit(X_train, y_train)\\nBagging and Pasting | 197', 'children': []}]}, {'id': 139, 'title': 'Random Patches and Random Subspaces', 'content': '7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\\n8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\\n0.90133333333333332\\nAccording to this oob evaluation, this BaggingClassifier  is likely to achieve about\\n90.1% accuracy on the test set. Let’s verify this:\\n>>> from sklearn.metrics  import accuracy_score\\n>>> y_pred = bag_clf.predict(X_test)\\n>>> accuracy_score (y_test, y_pred)\\n0.91200000000000003\\nWe get 91.2% accuracy on the test set—close enough!\\nThe oob decision function for each training instance is also available through the\\noob_decision_function_  variable. In this case (since the base estimator has a pre\\ndict_proba()  method) the decision function returns the class probabilities for each\\ntraining instance. For example, the oob evaluation estimates that the first training\\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\\nbelonging to the negative class):\\n>>> bag_clf.oob_decision_function_\\narray([[0.31746032, 0.68253968],\\n       [0.34117647, 0.65882353],\\n       [1.        , 0.        ],\\n       ...\\n       [1.        , 0.        ],\\n       [0.03108808, 0.96891192],\\n       [0.57291667, 0.42708333]])\\nRandom Patches and Random Subspaces\\nThe BaggingClassifier  class supports sampling the features as well. This is con‐\\ntrolled by two hyperparameters: max_features  and bootstrap_features . They work\\nthe same way as max_samples  and bootstrap , but for feature sampling instead of\\ninstance sampling. Thus, each predictor will be trained on a random subset of the\\ninput features.\\nThis is particularly useful when you are dealing with high-dimensional inputs (such\\nas images). Sampling both training instances and features is called the Random\\nPatches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\\nples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\\ntures  smaller than 1.0) is called the Random Subspaces  method .8\\n198 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 140, 'title': 'Random Forests', 'content': '9“Random Decision Forests, ” T. Ho (1995).\\n10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\\n11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\\nFalse ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\\nfier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\\na lower variance.\\nRandom Forests\\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\\ntrained via the bagging method (or sometimes pasting), typically with max_samples\\nset to the size of the training set. Instead of building a BaggingClassifier  and pass‐\\ning it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\\na RandomForestRegressor  class for regression tasks). The following code trains a\\nRandom Forest classifier with 500 trees (each limited to maximum 16 nodes), using\\nall available CPU cores:\\nfrom sklearn.ensemble  import RandomForestClassifier\\nrnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\\nrnd_clf.fit(X_train, y_train)\\ny_pred_rf  = rnd_clf.predict(X_test)\\nWith a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\\nDecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\\nmeters of a BaggingClassifier  to control the ensemble itself.11\\nThe Random Forest algorithm introduces extra randomness when growing trees;\\ninstead of searching for the very best feature when splitting a node (see Chapter 6 ), it\\nsearches for the best feature among a random subset of features. This results in a\\ngreater tree diversity, which (once again) trades a higher bias for a lower variance,\\ngenerally yielding an overall better model. The following BaggingClassifier  is\\nroughly equivalent to the previous RandomForestClassifier :\\nbag_clf = BaggingClassifier (\\n    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\\n    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\\nRandom Forests | 199', 'children': [{'id': 141, 'title': 'Extra-Trees', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 142, 'title': 'Feature Importance', 'content': '12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\\nExtra-Trees\\nWhen you are growing a tree in a Random Forest, at each node only a random subset\\nof the features is considered for splitting (as discussed earlier). It is possible to make\\ntrees even more random by also using random thresholds for each feature rather than\\nsearching for the best possible thresholds (like regular Decision Trees do).\\nA forest of such extremely random trees is simply called an Extremely Randomized\\nTrees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\\nlower variance. It also makes Extra-Trees much faster to train than regular Random\\nForests since finding the best possible threshold for each feature at every node is one\\nof the most time-consuming tasks of growing a tree.\\nY ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\\nclass. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\\nTreesRegressor  class has the same API as the RandomForestRegressor  class.\\nIt is hard to tell in advance whether a RandomForestClassifier\\nwill perform better or worse than an ExtraTreesClassifier . Gen‐\\nerally, the only way to know is to try both and compare them using\\ncross-validation (and tuning the hyperparameters using grid\\nsearch).\\nFeature Importance\\nY et another great quality of Random Forests is that they make it easy to measure the \\nrelative importance of each feature. Scikit-Learn measures a feature’s importance by\\nlooking at how much the tree nodes that use that feature reduce impurity on average\\n(across all trees in the forest). More precisely, it is a weighted average, where each\\nnode’s weight is equal to the number of training samples that are associated with it\\n(see Chapter 6 ).\\nScikit-Learn computes this score automatically for each feature after training, then it\\nscales the results so that the sum of all importances is equal to 1. Y ou can access the\\nresult using the feature_importances_  variable. For example, the following code\\ntrains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\\noutputs each feature’s importance. It seems that the most important features are the\\npetal length (44%) and width (42%), while sepal length and width are rather unim‐\\nportant in comparison (11% and 2%, respectively).\\n200 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}]}, {'id': 143, 'title': 'Boosting', 'content': '>>> from sklearn.datasets  import load_iris\\n>>> iris = load_iris ()\\n>>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\\n>>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\\n...     print(name, score)\\n...\\nsepal length (cm) 0.112492250999\\nsepal width (cm) 0.0231192882825\\npetal length (cm) 0.441030464364\\npetal width (cm) 0.423357996355\\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\\nin Chapter 3 ) and plot each pixel’s importance, you get the image represented in\\nFigure 7-6 .\\nFigure 7-6. MNIST pixel importance (according to a Random Forest classifier)\\nRandom Forests are very handy to get a quick understanding of what features\\nactually matter, in particular if you need to perform feature selection.\\nBoosting\\nBoosting  (originally called hypothesis boosting ) refers to any Ensemble method that\\ncan combine several weak learners into a strong learner. The general idea of most\\nboosting methods is to train predictors sequentially, each trying to correct its prede‐\\ncessor. There are many boosting methods available, but by far the most popular are\\nBoosting | 201', 'children': [{'id': 144, 'title': 'AdaBoost', 'content': '13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\\nRobert E. Schapire (1997).\\n14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\\nare slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\\nBoost.\\nAdaBoost\\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\\nto the training instances that the predecessor underfitted. This results in new predic‐\\ntors focusing more and more on the hard cases. This is the technique used by Ada‐\\nBoost.\\nFor example, to build an AdaBoost classifier, a first base classifier (such as a Decision\\nTree) is trained and used to make predictions on the training set. The relative weight\\nof misclassified training instances is then increased. A second classifier is trained\\nusing the updated weights and again it makes predictions on the training set, weights\\nare updated, and so on (see Figure 7-7 ).\\nFigure 7-7. AdaBoost sequential training with instance weight updates\\nFigure 7-8  shows the decision boundaries of five consecutive predictors on the\\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights\\n202 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 145, 'title': 'Gradient Boosting', 'content': '16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\\n17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\\ntion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\\nScikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\\nstands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\\nWhen there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\\npredictors can estimate class probabilities (i.e., if they have a predict_proba()\\nmethod), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\\nfor “Real”), which relies on class probabilities rather than predictions and generally\\nperforms better.\\nThe following code trains an AdaBoost classifier based on 200 Decision Stumps  using\\nScikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\\nBoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\\nother words, a tree composed of a single decision node plus two leaf nodes. This is\\nthe default base estimator for the AdaBoostClassifier  class:\\nfrom sklearn.ensemble  import AdaBoostClassifier\\nada_clf = AdaBoostClassifier (\\n    DecisionTreeClassifier (max_depth =1), n_estimators =200,\\n    algorithm =\"SAMME.R\" , learning_rate =0.5)\\nada_clf.fit(X_train, y_train)\\nIf your AdaBoost ensemble is overfitting the training set, you can\\ntry reducing the number of estimators or more strongly regulariz‐\\ning the base estimator.\\nGradient Boosting\\nAnother very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\\nerrors  made by the previous predictor.\\nLet’s go through a simple regression example using Decision Trees as the base predic‐\\ntors (of course Gradient Boosting also works great with regression tasks). This is\\ncalled  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\\nfit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\\ning set):\\nBoosting | 205', 'children': []}]}, {'id': 146, 'title': 'Stacking', 'content': '18“Stacked Generalization, ” D. Wolpert (1992).\\nIt is possible to use Gradient Boosting with other cost functions.\\nThis is controlled by the loss  hyperparameter (see Scikit-Learn’s\\ndocumentation for more details).\\nIt is worth noting that an optimized implementation of Gradient Boosting is available\\nin the popular python library XGBoost , which stands for Extreme Gradient Boosting.\\nThis package was initially developed by Tianqi Chen as part of the Distributed (Deep)\\nMachine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\\nand portable. In fact, XGBoost is often an important component of the winning\\nentries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\\nimport xgboost\\nxgb_reg = xgboost.XGBRegressor ()\\nxgb_reg.fit(X_train, y_train)\\ny_pred = xgb_reg.predict(X_val)\\nXGBoost also offers several nice features, such as automatically taking care of early\\nstopping:\\nxgb_reg.fit(X_train, y_train,\\n            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\\ny_pred = xgb_reg.predict(X_val)\\nY ou should definitely check it out!\\nStacking\\nThe last Ensemble method we will discuss in this chapter is called stacking  (short for\\nstacked generalization ).18 It is based on a simple idea: instead of using trivial functions\\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\\nwhy don’t we train a model to perform this aggregation? Figure 7-12  shows such an\\nensemble performing a regression task on a new instance. Each of the bottom three\\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \\n(called a blender , or a meta learner ) takes these predictions as inputs and makes the\\nfinal prediction (3.0).\\n210 | Chapter 7: Ensemble Learning and Random Forests', 'children': []}, {'id': 147, 'title': 'Exercises', 'content': 'Figure 7-15. Predictions in a multilayer stacking ensemble\\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\\nto roll out your own implementation (see the following exercises). Alternatively, you\\ncan use an open source implementation such as brew  (available at https://github.com/\\nviisar/brew ).\\nExercises\\n1.If you have trained five different models on the exact same training data, and\\nthey all achieve 95% precision, is there any chance that you can combine these\\nmodels to get better results? If so, how? If not, why?\\n2.What is the difference between hard and soft voting classifiers?\\n3.Is it possible to speed up training of a bagging ensemble by distributing it across\\nmultiple servers? What about pasting ensembles, boosting ensembles, random\\nforests, or stacking ensembles?\\n4.What is the benefit of out-of-bag evaluation?\\n5.What makes Extra-Trees more random than regular Random Forests? How can\\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\\ndom Forests?\\n6.If your AdaBoost ensemble underfits the training data, what hyperparameters\\nshould you tweak and how?\\nExercises | 213', 'children': []}]}, {'id': 148, 'title': 'Chapter 8. Dimensionality Reduction', 'content': 'CHAPTER 8\\nDimensionality Reduction\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 8 in the final\\nrelease of the book.\\nMany Machine Learning problems involve thousands or even millions of features for\\neach training instance. Not only does this make training extremely slow, it can also\\nmake it much harder to find a good solution, as we will see. This problem is often\\nreferred to as the curse of dimensionality .\\nFortunately, in real-world problems, it is often possible to reduce the number of fea‐\\ntures considerably, turning an intractable problem into a tractable one. For example,\\nconsider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\\nders are almost always white, so you could completely drop these pixels from the\\ntraining set without losing much information. Figure 7-6  confirms that these pixels\\nare utterly unimportant for the classification task. Moreover, two neighboring pixels\\nare often highly correlated: if you merge them into a single pixel (e.g., by taking the\\nmean of the two pixel intensities), you will not lose much information.\\n215', 'children': [{'id': 149, 'title': 'The Curse of Dimensionality', 'content': '1Well, four dimensions if you count time, and a few more if you are a string theorist.\\nReducing dimensionality does lose some information (just like\\ncompressing an image to JPEG can degrade its quality), so even\\nthough it will speed up training, it may also make your system per‐\\nform slightly worse. It also makes your pipelines a bit more com‐\\nplex and thus harder to maintain. So you should first try to train\\nyour system with the original data before considering using dimen‐\\nsionality reduction if training is too slow. In some cases, however,\\nreducing the dimensionality of the training data may filter out\\nsome noise and unnecessary details and thus result in higher per‐\\nformance (but in general it won’t; it will just speed up training).\\nApart from speeding up training, dimensionality reduction is also extremely useful\\nfor data visualization (or DataViz ). Reducing the number of dimensions down to two\\n(or three) makes it possible to plot a condensed view of a high-dimensional training\\nset on a graph and often gain some important insights by visually detecting patterns,\\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\\npeople who are not data scientists, in particular decision makers who will use your\\nresults.\\nIn this chapter we will discuss the curse of dimensionality and get a sense of what\\ngoes on in high-dimensional space. Then, we will present the two main approaches to\\ndimensionality reduction (projection and Manifold Learning), and we will go\\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\\nPCA, and LLE.\\nThe Curse of Dimensionality\\nWe are so used to living in three dimensions1 that our intuition fails us when we try\\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\\npicture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\\n1,000-dimensional space.\\n216 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 150, 'title': 'Main Approaches for Dimensionality Reduction', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 151, 'title': 'Projection', 'content': 'in the MNIST problem), you would need more training instances than atoms in the\\nobservable universe in order for training instances to be within 0.1 of each other on\\naverage, assuming they were spread out uniformly across all dimensions.\\nMain Approaches for Dimensionality Reduction\\nBefore we dive into specific dimensionality reduction algorithms, let’s take a look at\\nthe two main approaches to reducing dimensionality: projection and Manifold\\nLearning.\\nProjection\\nIn most real-world problems, training instances are not spread out uniformly across\\nall dimensions. Many features are almost constant, while others are highly correlated\\n(as discussed earlier for MNIST). As a result, all training instances actually lie within\\n(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\\nsounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\\nset represented by the circles.\\nFigure 8-2. A 3D dataset lying close to a 2D subspace\\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\\nsubspace of the high-dimensional (3D) space. Now if we project every training\\ninstance perpendicularly onto this subspace (as represented by the short lines con‐\\nnecting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\\nTa-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\\nthe axes correspond to new features z1 and z2 (the coordinates of the projections on\\nthe plane).\\n218 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 152, 'title': 'Manifold Learning', 'content': 'Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\\nthe Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\\nwant is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\\nFigure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\\n(right)\\nManifold Learning\\nThe Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\\nlocally resembles a 2D plane, but it is rolled in the third dimension.\\nMany dimensionality reduction algorithms work by modeling the manifold  on which\\nthe training instances lie; this is called Manifold Learning . It relies on the manifold\\nassumption , also called the manifold hypothesis , which holds that most real-world\\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\\nassumption is very often empirically observed.\\nOnce again, think about the MNIST dataset: all handwritten digit images have some\\nsimilarities. They are made of connected lines, the borders are white, they are more\\nor less centered, and so on. If you randomly generated images, only a ridiculously\\ntiny fraction of them would look like handwritten digits. In other words, the degrees\\nof freedom available to you if you try to create a digit image are dramatically lower\\nthan the degrees of freedom you would have if you were allowed to generate any\\nimage you wanted. These constraints tend to squeeze the dataset into a lower-\\ndimensional manifold.\\nThe manifold assumption is often accompanied by another implicit assumption: that\\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\\n220 | Chapter 8: Dimensionality Reduction', 'children': []}]}, {'id': 153, 'title': 'PCA', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 154, 'title': 'Preserving the Variance', 'content': '4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\\nPrincipal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\\ntion algorithm. First it identifies the hyperplane that lies closest to the data, and then\\nit projects the data onto it, just like in Figure 8-2 .\\nPreserving the Variance\\nBefore you can project the training set onto a lower-dimensional hyperplane, you\\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\\nsented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\\nhyperplanes). On the right is the result of the projection of the dataset onto each of\\nthese axes. As you can see, the projection onto the solid line preserves the maximum\\nvariance, while the projection onto the dotted line preserves very little variance, and\\nthe projection onto the dashed line preserves an intermediate amount of variance.\\nFigure 8-7. Selecting the subspace onto which to project\\nIt seems reasonable to select the axis that preserves the maximum amount of var‐\\niance, as it will most likely lose less information than the other projections. Another\\nway to justify this choice is that it is the axis that minimizes the mean squared dis‐\\ntance between the original dataset and its projection onto that axis. This is the rather\\nsimple idea behind PCA .4\\n222 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 155, 'title': 'Principal Components', 'content': 'Principal Components\\nPCA identifies the axis that accounts for the largest amount of variance in the train‐\\ning set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data‐\\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\\na fifth, and so on—as many axes as the number of dimensions in the dataset.\\nThe unit vector that defines the ith axis is called the ith principal component  (PC). In\\nFigure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\\nrepresented by the orthogonal arrows in the plane, and the third PC would be\\northogonal to the plane (pointing up or down).\\nThe direction of the principal components is not stable: if you per‐\\nturb the training set slightly and run PCA again, some of the new\\nPCs may point in the opposite direction of the original PCs. How‐\\never, they will generally still lie on the same axes. In some cases, a\\npair of PCs may even rotate or swap, but the plane they define will\\ngenerally remain the same.\\nSo how can you find the principal components of a training set? Luckily, there is a\\nstandard matrix factorization technique called Singular Value Decomposition  (SVD)\\nthat can decompose the training set matrix X into the matrix multiplication of three\\nmatrices U Σ VT, where V contains all the principal components that we are looking\\nfor, as shown in Equation 8-1 .\\nEquation 8-1. Principal components matrix\\nV=∣ ∣ ∣\\nc1c2⋯cn\\n∣ ∣ ∣\\nThe following Python code uses NumPy’s svd()  function to obtain all the principal\\ncomponents of the training set, then extracts the first two PCs:\\nX_centered  = X - X.mean(axis=0)\\nU, s, Vt = np.linalg.svd(X_centered )\\nc1 = Vt.T[:, 0]\\nc2 = Vt.T[:, 1]\\nPCA | 223', 'children': []}, {'id': 156, 'title': 'Projecting Down to d Dimensions', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 157, 'title': 'Using Scikit-Learn', 'content': 'PCA assumes that the dataset is centered around the origin. As we\\nwill see, Scikit-Learn’s PCA classes take care of centering the data\\nfor you. However, if you implement PCA yourself (as in the pre‐\\nceding example), or if you use other libraries, don’t forget to center\\nthe data first.\\nProjecting Down to d Dimensions\\nOnce you have identified all the principal components, you can reduce the dimen‐\\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\\ndefined by the first d principal components. Selecting this hyperplane ensures that the\\nprojection will preserve as much variance as possible. For example, in Figure 8-2  the\\n3D dataset is projected down to the 2D plane defined by the first two principal com‐\\nponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\\ntion looks very much like the original 3D dataset.\\nTo project the training set onto the hyperplane, you can simply compute the matrix\\nmultiplication of the training set matrix X by the matrix Wd, defined as the matrix\\ncontaining the first d principal components (i.e., the matrix composed of the first d\\ncolumns of V), as shown in Equation 8-2 .\\nEquation 8-2. Projecting the training set down to d dimensions\\nXd‐proj=XWd\\nThe following Python code projects the training set onto the plane defined by the first\\ntwo principal components:\\nW2 = Vt.T[:, :2]\\nX2D = X_centered .dot(W2)\\nThere you have it! Y ou now know how to reduce the dimensionality of any dataset\\ndown to any number of dimensions, while preserving as much variance as possible.\\nUsing Scikit-Learn\\nScikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\\nbefore. The following code applies PCA to reduce the dimensionality of the dataset\\ndown to two dimensions (note that it automatically takes care of centering the data):\\nfrom sklearn.decomposition  import PCA\\npca = PCA(n_components  = 2)\\nX2D = pca.fit_transform (X)\\nAfter fitting the PCA transformer to the dataset, you can access the principal compo‐\\nnents using the components_  variable (note that it contains the PCs as horizontal vec‐\\n224 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 158, 'title': 'Explained Variance Ratio', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 159, 'title': 'Choosing the Right Number of Dimensions', 'content': 'tors, so, for example, the first principal component is equal to pca.components_.T[:,\\n0]).\\nExplained Variance Ratio\\nAnother very useful piece of information is the explained variance ratio  of each prin‐\\ncipal component, available via the explained_variance_ratio_  variable. It indicates\\nthe proportion of the dataset’s variance that lies along the axis of each principal com‐\\nponent. For example, let’s look at the explained variance ratios of the first two compo‐\\nnents of the 3D dataset represented in Figure 8-2 :\\n>>> pca.explained_variance_ratio_\\narray([0.84248607, 0.14631839])\\nThis tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\\nlies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\\nable to assume that it probably carries little information.\\nChoosing the Right Number of Dimensions\\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\\ngenerally preferable to choose the number of dimensions that add up to a sufficiently\\nlarge portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\\nsionality for data visualization—in that case you will generally want to reduce the\\ndimensionality down to 2 or 3.\\nThe following code computes PCA without reducing dimensionality, then computes\\nthe minimum number of dimensions required to preserve 95% of the training set’s\\nvariance:\\npca = PCA()\\npca.fit(X_train)\\ncumsum = np.cumsum(pca.explained_variance_ratio_ )\\nd = np.argmax(cumsum >= 0.95) + 1\\nY ou could then set n_components=d  and run PCA again. However, there is a much\\nbetter option: instead of specifying the number of principal components you want to\\npreserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\\nratio of variance you wish to preserve:\\npca = PCA(n_components =0.95)\\nX_reduced  = pca.fit_transform (X_train)\\nY et another option is to plot the explained variance as a function of the number of\\ndimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\\ncurve, where the explained variance stops growing fast. Y ou can think of this as the\\nintrinsic dimensionality of the dataset. In this case, you can see that reducing the\\nPCA | 225', 'children': []}, {'id': 160, 'title': 'PCA for Compression', 'content': 'dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\\niance.\\nFigure 8-8. Explained variance as a function of the number of dimensions\\nPCA for Compression\\nObviously after dimensionality reduction, the training set takes up much less space.\\nFor example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\\niance. Y ou should find that each instance will have just over 150 features, instead of\\nthe original 784 features. So while most of the variance is preserved, the dataset is\\nnow less than 20% of its original size! This is a reasonable compression ratio, and you\\ncan see how this can speed up a classification algorithm (such as an SVM classifier)\\ntremendously.\\nIt is also possible to decompress the reduced dataset back to 784 dimensions by\\napplying the inverse transformation of the PCA projection. Of course this won’t give\\nyou back the original data, since the projection lost a bit of information (within the\\n5% variance that was dropped), but it will likely be quite close to the original data.\\nThe mean squared distance between the original data and the reconstructed data\\n(compressed and then decompressed) is called the reconstruction error . For example,\\nthe following code compresses the MNIST dataset down to 154 dimensions, then uses\\nthe inverse_transform()  method to decompress it back to 784 dimensions.\\nFigure 8-9  shows a few digits from the original training set (on the left), and the cor‐\\nresponding digits after compression and decompression. Y ou can see that there is a\\nslight image quality loss, but the digits are still mostly intact.\\npca = PCA(n_components  = 154)\\nX_reduced  = pca.fit_transform (X_train)\\nX_recovered  = pca.inverse_transform (X_reduced )\\n226 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 161, 'title': 'Randomized PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}, {'id': 162, 'title': 'Incremental PCA', 'content': 'Figure 8-9. MNIST compression preserving 95% of the variance\\nThe equation of the inverse transformation is shown in Equation 8-3 .\\nEquation 8-3. PCA inverse transformation, back to the original number of\\ndimensions\\nXrecovered=Xd‐projWdT\\nRandomized PCA\\nIf you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\\nchastic algorithm called Randomized PCA  that quickly finds an approximation of the\\nfirst d principal components. Its computational complexity is O(m × d2) + O(d3),\\ninstead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\\nthan full SVD when d is much smaller than n:\\nrnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\\nX_reduced  = rnd_pca.fit_transform (X_train)\\nBy default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\\nSVD, you can set the svd_solver  hyperparameter to \"full\" .\\nIncremental PCA\\nOne problem with the preceding implementations of PCA is that they require the\\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\\nIncremental PCA  (IPCA) algorithms have been developed: you can split the training\\nset into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\\nPCA | 227', 'children': []}]}, {'id': 163, 'title': 'Kernel PCA', 'content': '5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\\n(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\\ninstances arrive).\\nThe following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\\narray_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\\nbefore). Note that you must call the partial_fit()  method with each mini-batch\\nrather than the fit()  method with the whole training set:\\nfrom sklearn.decomposition  import IncrementalPCA\\nn_batches  = 100\\ninc_pca = IncrementalPCA (n_components =154)\\nfor X_batch in np.array_split (X_train, n_batches ):\\n    inc_pca.partial_fit (X_batch)\\nX_reduced  = inc_pca.transform (X_train)\\nAlternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\\nclass uses only a small part of the array at any given time, the memory usage remains\\nunder control. This makes it possible to call the usual fit()  method, as you can see\\nin the following code:\\nX_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\\nbatch_size  = m // n_batches\\ninc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\\ninc_pca.fit(X_mm)\\nKernel PCA\\nIn Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\\nmaps instances into a very high-dimensional space (called the feature space ), enabling\\nnonlinear classification and regression with Support Vector Machines. Recall that a\\nlinear decision boundary in the high-dimensional feature space corresponds to a\\ncomplex nonlinear decision boundary in the original space .\\nIt turns out that the same trick can be applied to PCA, making it possible to perform\\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel\\n228 | Chapter 8: Dimensionality Reduction', 'children': [{'id': 164, 'title': 'Selecting a Kernel and Tuning Hyperparameters', 'content': '6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\\nsometimes even unrolling datasets that lie close to a twisted manifold.\\nFor example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\\nwith an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\\nother kernels):\\nfrom sklearn.decomposition  import KernelPCA\\nrbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\\nX_reduced  = rbf_pca.fit_transform (X)\\nFigure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\\n(Logistic).\\nFigure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\\nSelecting a Kernel and Tuning Hyperparameters\\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\\nmeasure to help you select the best kernel and hyperparameter values. However,\\ndimensionality reduction is often a preparation step for a supervised learning task\\n(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\\nparameters that lead to the best performance on that task. For example, the following\\ncode creates a two-step pipeline, first reducing dimensionality to two dimensions\\nusing kPCA, then applying Logistic Regression for classification. Then it uses Grid\\nSearchCV  to find the best kernel and gamma value for kPCA in order to get the best\\nclassification accuracy at the end of the pipeline:\\nfrom sklearn.model_selection  import GridSearchCV\\nfrom sklearn.linear_model  import LogisticRegression\\nfrom sklearn.pipeline  import Pipeline\\nKernel PCA | 229', 'children': []}]}, {'id': 165, 'title': 'LLE', 'content': '8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\\n>>> from sklearn.metrics  import mean_squared_error\\n>>> mean_squared_error (X, X_preimage )\\n32.786308795766132\\nNow you can use grid search with cross-validation to find the kernel and hyperpara‐\\nmeters that minimize this pre-image reconstruction error.\\nLLE\\nLocally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\\nreduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\\non projections like the previous algorithms. In a nutshell, LLE works by first measur‐\\ning how each training instance linearly relates to its closest neighbors (c.n.), and then\\nlooking for a low-dimensional representation of the training set where these local\\nrelationships are best preserved (more details shortly). This makes it particularly\\ngood at unrolling twisted manifolds, especially when there is not too much noise.\\nFor example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\\nunroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\\nsee, the Swiss roll is completely unrolled and the distances between instances are\\nlocally well preserved. However, distances are not preserved on a larger scale: the left\\npart of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\\nless, LLE did a pretty good job at modeling the manifold.\\nfrom sklearn.manifold  import LocallyLinearEmbedding\\nlle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\\nX_reduced  = lle.fit_transform (X)\\n232 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 166, 'title': 'Other Dimensionality Reduction Techniques', 'content': 'Equation 8-4. LLE step 1: linearly modeling local relationships\\nW= argmin\\nW∑\\ni= 1m\\nxi−∑\\nj= 1m\\nwi,jxj2\\nsubject towi,j= 0 if xjis not one of the kc.n. of xi\\n∑\\nj= 1m\\nwi,j= 1 for i= 1, 2,⋯,m\\nAfter this step, the weight matrix W (containing the weights wi,j) encodes the local\\nlinear relationships between the training instances. Now the second step is to map the\\ntraining instances into a d-dimensional space (where d < n) while preserving these\\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\\nspace, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\\nas possible. This idea leads to the unconstrained optimization problem described in\\nEquation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\\nweights fixed and finding the optimal position of the instances’ images in the low-\\ndimensional space. Note that Z is the matrix containing all z(i).\\nEquation 8-5. LLE step 2: reducing dimensionality while preserving relationships\\nZ= argmin\\nZ∑\\ni= 1m\\nzi−∑\\nj= 1m\\nwi,jzj2\\nScikit-Learn’s LLE implementation has the following computational complexity:\\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\\nOther Dimensionality Reduction Techniques\\nThere are many other dimensionality reduction techniques, several of which are\\navailable in Scikit-Learn. Here are some of the most popular:\\n•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\\nthe distances between the instances (see Figure 8-13 ).\\n234 | Chapter 8: Dimensionality Reduction', 'children': []}, {'id': 167, 'title': 'Exercises', 'content': '9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\\nthese nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\\nreduces dimensionality while trying to preserve the geodesic distances9 between\\nthe instances.\\n•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\\nwhile trying to keep similar instances close and dissimilar instances apart. It is\\nmostly used for visualization, in particular to visualize clusters of instances in\\nhigh-dimensional space (e.g., to visualize the MNIST images in 2D).\\n•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\\ning training it learns the most discriminative axes between the classes, and these\\naxes can then be used to define a hyperplane onto which to project the data. The\\nbenefit is that the projection will keep classes as far apart as possible, so LDA is a\\ngood technique to reduce dimensionality before running another classification\\nalgorithm such as an SVM classifier.\\nFigure 8-13. Reducing the Swiss roll to 2D using various techniques\\nExercises\\n1.What are the main motivations for reducing a dataset’s dimensionality? What are\\nthe main drawbacks?\\n2.What is the curse of dimensionality?\\n3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\\noperation? If so, how? If not, why?\\n4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\\n5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\\nvariance ratio to 95%. How many dimensions will the resulting dataset have?\\nExercises | 235', 'children': []}]}, {'id': 168, 'title': 'Chapter 9. Unsupervised Learning Techniques', 'content': 'CHAPTER 9\\nUnsupervised Learning Techniques\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 9 in the final\\nrelease of the book.\\nAlthough most of the applications of Machine Learning today are based on super‐\\nvised learning (and as a result, this is where most of the investments go to), the vast\\nmajority of the available data is actually unlabeled: we have the input features X, but\\nwe do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\\nunsupervised learning would be the cake, supervised learning would be the icing on\\nthe cake, and reinforcement learning would be the cherry on the cake” . In other\\nwords, there is a huge potential in unsupervised learning that we have only barely\\nstarted to sink our teeth into.\\nFor example, say you want to create a system that will take a few pictures of each item\\non a manufacturing production line and detect which items are defective. Y ou can\\nfairly easily create a system that will take pictures automatically, and this might give\\nyou thousands of pictures every day. Y ou can then build a reasonably large dataset in\\njust a few weeks. But wait, there are no labels! If you want to train a regular binary\\nclassifier that will predict whether an item is defective or not, you will need to label\\nevery single picture as “defective” or “normal” . This will generally require human\\nexperts to sit down and manually go through all the pictures. This is a long, costly\\nand tedious task, so it will usually only be done on a small subset of the available pic‐\\ntures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\\nmance will be disappointing. Moreover, every time the company makes any change to\\nits products, the whole process will need to be started over from scratch. Wouldn’t it\\n237', 'children': [{'id': 169, 'title': 'Clustering', 'content': 'be great if the algorithm could just exploit the unlabeled data without needing\\nhumans to label every picture? Enter unsupervised learning.\\nIn Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\\nality reduction. In this chapter, we will look at a few more unsupervised learning tasks\\nand algorithms:\\n•Clustering : the goal is to group similar instances together into clusters . This is a\\ngreat tool for data analysis, customer segmentation, recommender systems,\\nsearch engines, image segmentation, semi-supervised learning, dimensionality\\nreduction, and more.\\n•Anomaly detection : the objective is to learn what “normal” data looks like, and\\nuse this to detect abnormal instances, such as defective items on a production\\nline or a new trend in a time series.\\n•Density estimation : this is the task of estimating the probability density function\\n(PDF) of the random process that generated the dataset. This is commonly used\\nfor anomaly detection: instances located in very low-density regions are likely to\\nbe anomalies. It is also useful for data analysis and visualization.\\nReady for some cake? We will start with clustering, using K-Means and DBSCAN,\\nand then we will discuss Gaussian mixture models and see how they can be used for\\ndensity estimation, clustering, and anomaly detection.\\nClustering\\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\\nbefore. Y ou look around and you notice a few more. They are not perfectly identical,\\nyet they are sufficiently similar for you to know that they most likely belong to the\\nsame species (or at least the same genus). Y ou may need a botanist to tell you what\\nspecies that is, but you certainly don’t need an expert to identify groups of similar-\\nlooking objects. This is called clustering : it is the task of identifying similar instances\\nand assigning them to clusters , i.e., groups of similar instances.\\nJust like in classification, each instance gets assigned to a group. However, this is an\\nunsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\\nChapter 4 ), where each instance’s species (i.e., its class) is represented with a different\\nmarker. It is a labeled dataset, for which classification algorithms such as Logistic\\nRegression, SVMs or Random Forest classifiers are well suited. On the right is the\\nsame dataset, but without the labels, so you cannot use a classification algorithm any‐\\nmore. This is where clustering algorithms step in: many of them can easily detect the\\ntop left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\\nthat the lower right cluster is actually composed of two distinct sub-clusters. That\\nsaid, the dataset actually has two additional features (sepal length and width), not\\n238 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 170, 'title': 'K-Means', 'content': '1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\\nthat are similar to a reference image. To build such a system, you would first\\napply a clustering algorithm to all the images in your database: similar images\\nwould end up in the same cluster. Then when a user provides a reference image,\\nall you need to do is to find this image’s cluster using the trained clustering\\nmodel, and you can then simply return all the images from this cluster.\\n•To segment an image: by clustering pixels according to their color, then replacing\\neach pixel’s color with the mean color of its cluster, it is possible to reduce the\\nnumber of different colors in the image considerably. This technique is used in\\nmany object detection and tracking systems, as it makes it easier to detect the\\ncontour of each object.\\nThere is no universal definition of what a cluster is: it really depends on the context,\\nand different algorithms will capture different kinds of clusters. For example, some\\nalgorithms look for instances centered around a particular point, called a centroid .\\nOthers look for continuous regions of densely packed instances: these clusters can\\ntake on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\\nAnd the list goes on.\\nIn this section, we will look at two popular clustering algorithms: K-Means and\\nDBSCAN, and we will show some of their applications, such as non-linear dimen‐\\nsionality reduction, semi-supervised learning and anomaly detection.\\nK-Means\\nConsider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\\nof instances. The K-Means algorithm is a simple algorithm capable of clustering this\\nkind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\\nposed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\\ntion, but it was only published outside of the company in 1982, in a paper titled\\n“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\\nlished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\\nForgy.\\n240 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 171, 'title': 'Limits of K-Means', 'content': 'ters. We can see that when k=3 and when k=6, we get bad clusters. But when k=4 or\\nk=5, the clusters look pretty good – most instances extend beyond the dashed line, to\\nthe right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top),\\nis rather big, while when k=5, all clusters have similar sizes, so even though the over‐\\nall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea\\nto use k=5 to get clusters of similar sizes.\\nLimits of K-Means\\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect.\\nAs we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐\\nutions, plus you need to specify the number of clusters, which can be quite a hassle.\\nMoreover, K-Means does not behave very well when the clusters have varying sizes,\\ndifferent densities, or non-spherical shapes. For example, Figure 9-11  shows how K-\\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\\ndensities and orientations:\\nFigure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\\nAs you can see, neither of these solutions are any good. The solution on the left is\\nbetter, but it still chops off 25% of the middle cluster and assigns it to the cluster on\\nthe right. The solution on the right is just terrible, even though its inertia is lower. So\\ndepending on the data, different clustering algorithms may perform better. For exam‐\\nple, on these types of elliptical clusters, Gaussian mixture models work great.\\nIt is important to scale the input features before you run K-Means,\\nor else the clusters may be very stretched, and K-Means will per‐\\nform poorly. Scaling the features does not guarantee that all the\\nclusters will be nice and spherical, but it generally improves things.\\nNow let’s look at a few ways we can benefit from clustering. We will use K-Means, but\\nfeel free to experiment with other clustering algorithms.\\n250 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 172, 'title': 'Using clustering for image segmentation', 'content': 'Using clustering for image segmentation\\nImage segmentation  is the task of partitioning an image into multiple segments. In\\nsemantic segmentation , all pixels that are part of the same object type get assigned to\\nthe same segment. For example, in a self-driving car’s vision system, all pixels that are\\npart of a pedestrian’s image might be assigned to the “pedestrian” segment (there\\nwould just be one segment containing all the pedestrians). In instance segmentation ,\\nall pixels that are part of the same individual object are assigned to the same segment.\\nIn this case there would be a different segment for each pedestrian. The state of the\\nart in semantic or instance segmentation today is achieved using complex architec‐\\ntures based on convolutional neural networks (see Chapter 14 ). Here, we are going to\\ndo something much simpler: color segmentation . We will simply assign pixels to the\\nsame segment if they have a similar color. In some applications, this may be sufficient,\\nfor example if you want to analyze satellite images to measure how much total forest\\narea there is in a region, color segmentation may be just fine.\\nFirst, let’s load the image (see the upper left image in Figure 9-12 ) using Matplotlib’s\\nimread()  function:\\n>>> from matplotlib.image  import imread  # you could also use `imageio.imread()`\\n>>> image = imread(os.path.join(\"images\" ,\"clustering\" ,\"ladybug.png\" ))\\n>>> image.shape\\n(533, 800, 3)\\nThe image is represented as a 3D array: the first dimension’s size is the height, the\\nsecond is the width, and the third is the number of color channels, in this case red,\\ngreen and blue (RGB). In other words, for each pixel there is a 3D vector containing\\nthe intensities of red, green and blue, each between 0.0 and 1.0 (or between 0 and 255\\nif you use imageio.imread() ). Some images may have less channels, such as gray‐\\nscale images (one channel), or more channels, such as images with an additional\\nalpha channel  for transparency, or satellite images which often contain channels for\\nmany light frequencies (e.g., infrared). The following code reshapes the array to get a\\nlong list of RGB colors, then it clusters these colors using K-Means. For example, it\\nmay identify a color cluster for all shades of green. Next, for each color (e.g., dark\\ngreen), it looks for the mean color of the pixel’s color cluster. For example, all shades\\nof green may be replaced with the same light green color (assuming the mean color of\\nthe green cluster is light green). Finally it reshapes this long list of colors to get the\\nsame shape as the original image. And we’re done!\\nX = image.reshape(-1, 3)\\nkmeans = KMeans(n_clusters =8).fit(X)\\nsegmented_img  = kmeans.cluster_centers_ [kmeans.labels_]\\nsegmented_img  = segmented_img .reshape(image.shape)\\nThis outputs the image shown in the upper right of Figure 9-12 . Y ou can experiment\\nwith various numbers of clusters, as shown in the figure. When you use less than 8\\nclusters, notice that the ladybug’s flashy red color fails to get a cluster of its own: it\\nClustering | 251', 'children': []}, {'id': 173, 'title': 'Using Clustering for Preprocessing', 'content': 'gets merged with colors from the environment. This is due to the fact that the lady‐\\nbug is quite small, much smaller than the rest of the image, so even though its color is\\nflashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means prefers\\nclusters of similar sizes.\\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\\nThat was not too hard, was it? Now let’s look at another application of clustering: pre‐\\nprocessing.\\nUsing Clustering for Preprocessing\\nClustering can be an efficient approach to dimensionality reduction, in particular as a\\npreprocessing step before a supervised learning algorithm. For example, let’s tackle\\nthe digits dataset  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8\\nimages representing digits 0 to 9. First, let’s load the dataset:\\nfrom sklearn.datasets  import load_digits\\nX_digits , y_digits  = load_digits (return_X_y =True)\\nNow, let’s split it into a training set and a test set:\\nfrom sklearn.model_selection  import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split (X_digits , y_digits )\\nNext, let’s fit a Logistic Regression model:\\nfrom sklearn.linear_model  import LogisticRegression\\nlog_reg = LogisticRegression (random_state =42)\\nlog_reg.fit(X_train, y_train)\\nLet’s evaluate its accuracy on the test set:\\n>>> log_reg.score(X_test, y_test)\\n0.9666666666666667\\n252 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 174, 'title': 'Using Clustering for Semi-Supervised Learning', 'content': 'Using Clustering for Semi-Supervised Learning\\nAnother use case for clustering is in semi-supervised learning, when we have plenty\\nof unlabeled instances and very few labeled instances. Let’s train a logistic regression\\nmodel on a sample of 50 labeled instances from the digits dataset:\\nn_labeled  = 50\\nlog_reg = LogisticRegression ()\\nlog_reg.fit(X_train[:n_labeled ], y_train[:n_labeled ])\\nWhat is the performance of this model on the test set?\\n>>> log_reg.score(X_test, y_test)\\n0.8266666666666667\\nThe accuracy is just 82.7%: it should come as no surprise that this is much lower than\\nearlier, when we trained the model on the full training set. Let’s see how we can do\\nbetter. First, let’s cluster the training set into 50 clusters, then for each cluster let’s find\\nthe image closest to the centroid. We will call these images the representative images:\\nk = 50\\nkmeans = KMeans(n_clusters =k)\\nX_digits_dist  = kmeans.fit_transform (X_train)\\nrepresentative_digit_idx  = np.argmin(X_digits_dist , axis=0)\\nX_representative_digits  = X_train[representative_digit_idx ]\\nFigure 9-13  shows these 50 representative images:\\nFigure 9-13. Fifty  representative digit images (one per cluster)\\nNow let’s look at each image and manually label it:\\ny_representative_digits  = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\\nNow we have a dataset with just 50 labeled instances, but instead of being completely\\nrandom instances, each of them is a representative image of its cluster. Let’s see if the\\nperformance is any better:\\n>>> log_reg = LogisticRegression ()\\n>>> log_reg.fit(X_representative_digits , y_representative_digits )\\n>>> log_reg.score(X_test, y_test)\\n0.9244444444444444\\nWow! We jumped from 82.7% accuracy to 92.4%, although we are still only training\\nthe model on 50 instances. Since it is often costly and painful to label instances, espe‐\\n254 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 175, 'title': 'DBSCAN', 'content': '>>> np.mean(y_train_partially_propagated  == y_train[partially_propagated ])\\n0.9896907216494846\\nActive Learning\\nTo continue improving your model and your training set, the next step could be to do\\na few rounds of active learning : this is when a human expert interacts with the learn‐\\ning algorithm, providing labels when the algorithm needs them. There are many dif‐\\nferent strategies for active learning, but one of the most common ones is called\\nuncertainty sampling :\\n•The model is trained on the labeled instances gathered so far, and this model is\\nused to make predictions on all the unlabeled instances.\\n•The instances for which the model is most uncertain (i.e., when its estimated\\nprobability is lowest) must be labeled by the expert.\\n•Then you just iterate this process again and again, until the performance\\nimprovement stops being worth the labeling effort.\\nOther strategies include labeling the instances that would result in the largest model\\nchange, or the largest drop in the model’s validation error, or the instances that differ‐\\nent models disagree on (e.g., an SVM, a Random Forest, and so on).\\nBefore we move on to Gaussian mixture models, let’s take a look at DBSCAN,\\nanother popular clustering algorithm that illustrates a very different approach based\\non local density estimation. This approach allows the algorithm to identify clusters of\\narbitrary shapes.\\nDBSCAN\\nThis algorithm defines clusters as continuous regions of high density. It is actually\\nquite simple:\\n•For each instance, the algorithm counts how many instances are located within a\\nsmall distance ε (epsilon) from it. This region is called the instance’s ε-\\nneighborhood .\\n•If an instance has at least min_samples  instances in its ε-neighborhood (includ‐\\ning itself), then it is considered a core instance . In other words, core instances are\\nthose that are located in dense regions.\\n•All instances in the neighborhood of a core instance belong to the same cluster.\\nThis may include other core instances, therefore a long sequence of neighboring\\ncore instances forms a single cluster.\\n256 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 176, 'title': 'Other Clustering Algorithms', 'content': 'Figure 9-15. cluster_classification_diagram\\nIn short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any\\nnumber of clusters, of any shape, it is robust to outliers, and it has just two hyper‐\\nparameters ( eps and min_samples ). However, if the density varies significantly across\\nthe clusters, it can be impossible for it to capture all the clusters properly. Moreover,\\nits computational complexity is roughly O( m log m), making it pretty close to linear\\nwith regards to the number of instances. However, Scikit-Learn’s implementation can\\nrequire up to O( m2) memory if eps is large.\\nOther Clustering Algorithms\\nScikit-Learn implements several more clustering algorithms that you should take a\\nlook at. We cannot cover them all in detail here, but here is a brief overview:\\n•Agglomerative clustering : a hierarchy of clusters is built from the bottom up.\\nThink of many tiny bubbles floating on water and gradually attaching to each\\nother until there’s just one big group of bubbles. Similarly, at each iteration\\nagglomerative clustering connects the nearest pair of clusters (starting with indi‐\\nvidual instances). If you draw a tree with a branch for every pair of clusters that\\nmerged, you get a binary tree of clusters, where the leaves are the individual\\ninstances. This approach scales very well to large numbers of instances or clus‐\\nters, it can capture clusters of various shapes, it produces a flexible and informa‐\\ntive cluster tree instead of forcing you to choose a particular cluster scale, and it\\ncan be used with any pairwise distance. It can scale nicely to large numbers of\\ninstances if you provide a connectivity matrix. This is a sparse m by m matrix\\nthat indicates which pairs of instances are neighbors (e.g., returned by\\nsklearn.neighbors.kneighbors_graph() ). Without a connectivity matrix, the\\nalgorithm does not scale well to large datasets.\\n•Birch : this algorithm was designed specifically for very large datasets, and it can\\nbe faster than batch K-Means, with similar results, as long as the number of fea‐\\ntures is not too large (<20). It builds a tree structure during training containing\\nClustering | 259', 'children': []}]}, {'id': 177, 'title': 'Gaussian Mixtures', 'content': 'just enough information to quickly assign each new instance to a cluster, without\\nhaving to store all the instances in the tree: this allows it to use limited memory,\\nwhile handle huge datasets.\\n•Mean-shift : this algorithm starts by placing a circle centered on each instance,\\nthen for each circle it computes the mean of all the instances located within it,\\nand it shifts the circle so that it is centered on the mean. Next, it iterates this\\nmean-shift step until all the circles stop moving (i.e., until each of them is cen‐\\ntered on the mean of the instances it contains). This algorithm shifts the circles\\nin the direction of higher density, until each of them has found a local density\\nmaximum. Finally, all the instances whose circles have settled in the same place\\n(or close enough) are assigned to the same cluster. This has some of the same fea‐\\ntures as DBSCAN, in particular it can find any number of clusters of any shape, it\\nhas just one hyperparameter (the radius of the circles, called the bandwidth) and\\nit relies on local density estimation. However, it tends to chop clusters into pieces\\nwhen they have internal density variations. Unfortunately, its computational\\ncomplexity is O( m2), so it is not suited for large datasets.\\n•Affinity  propagation : this algorithm uses a voting system, where instances vote for\\nsimilar instances to be their representatives, and once the algorithm converges,\\neach representative and its voters form a cluster. This algorithm can detect any\\nnumber of clusters of different sizes. Unfortunately, this algorithm has a compu‐\\ntational complexity of O( m2), so it is not suited for large datasets.\\n•Spectral clustering : this algorithm takes a similarity matrix between the instances\\nand creates a low-dimensional embedding from it (i.e., it reduces its dimension‐\\nality), then it uses another clustering algorithm in this low-dimensional space\\n(Scikit-Learn’s implementation uses K-Means). Spectral clustering can capture\\ncomplex cluster structures, and it can also be used to cut graphs (e.g., to identify\\nclusters of friends on a social network), however it does not scale well to large\\nnumber of instances, and it does not behave well when the clusters have very dif‐\\nferent sizes.\\nNow let’s dive into Gaussian mixture models, which can be used for density estima‐\\ntion, clustering and anomaly detection.\\nGaussian Mixtures\\nA Gaussian mixture model  (GMM) is a probabilistic model that assumes that the\\ninstances were generated from a mixture of several Gaussian distributions whose\\nparameters are unknown. All the instances generated from a single Gaussian distri‐\\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐\\nferent ellipsoidal shape, size, density and orientation, just like in Figure 9-11 . When\\nyou observe an instance, you know it was generated from one of the Gaussian distri‐\\n260 | Chapter 9: Unsupervised Learning Techniques', 'children': [{'id': 178, 'title': 'Anomaly Detection using Gaussian Mixtures', 'content': 'Figure 9-18. covariance_type_diagram\\nThe computational complexity of training a GaussianMixture\\nmodel depends on the number of instances m, the number of\\ndimensions n, the number of clusters k, and the constraints on the\\ncovariance matrices. If covariance_type  is \"spherical  or \"diag\" ,\\nit is O( kmn ), assuming the data has a clustering structure. If cova\\nriance_type  is \"tied\"  or \"full\" , it is O( kmn2 + kn3), so it will not\\nscale to large numbers of features.\\nGaussian mixture models can also be used for anomaly detection. Let’s see how.\\nAnomaly Detection using Gaussian Mixtures\\nAnomaly detection  (also called outlier detection ) is the task of detecting instances that\\ndeviate strongly from the norm. These instances are of course called anomalies  or\\noutliers , while the normal instances are called inliers . Anomaly detection is very use‐\\nful in a wide variety of applications, for example in fraud detection, or for detecting\\ndefective products in manufacturing, or to remove outliers from a dataset before\\ntraining another model, which can significantly improve the performance of the\\nresulting model.\\nUsing a Gaussian mixture model for anomaly detection is quite simple: any instance\\nlocated in a low-density region can be considered an anomaly. Y ou must define what\\ndensity threshold you want to use. For example, in a manufacturing company that\\ntries to detect defective products, the ratio of defective products is usually well-\\nknown. Say it is equal to 4%, then you can set the density threshold to be the value\\nthat results in having 4% of the instances located in areas below that threshold den‐\\nsity. If you notice that you get too many false positives (i.e., perfectly good products\\nthat are flagged as defective), you can lower the threshold. Conversely, if you have too\\nmany false negatives (i.e., defective products that the system does not flag as defec‐\\ntive), you can increase the threshold. This is the usual precision/recall tradeoff (see\\nChapter 3 ). Here is how you would identify the outliers using the 4th percentile low‐\\n266 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 179, 'title': 'Selecting the Number of Clusters', 'content': 'est density as the threshold (i.e., approximately 4% of the instances will be flagged as\\nanomalies):\\ndensities  = gm.score_samples (X)\\ndensity_threshold  = np.percentile (densities , 4)\\nanomalies  = X[densities  < density_threshold ]\\nThese anomalies are represented as stars on Figure 9-19 :\\nFigure 9-19. Anomaly detection using a Gaussian mixture model\\nA closely related task is novelty detection : it differs from anomaly detection in that the\\nalgorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,\\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\\nis often precisely used to clean up a dataset.\\nGaussian mixture models try to fit all the data, including the outli‐\\ners, so if you have too many of them, this will bias the model’s view\\nof “normality”: some outliers may wrongly be considered as nor‐\\nmal. If this happens, you can try to fit the model once, use it to\\ndetect and remove the most extreme outliers, then fit the model\\nagain on the cleaned up dataset. Another approach is to use robust\\ncovariance estimation methods (see the EllipticEnvelope  class).\\nJust like K-Means, the GaussianMixture  algorithm requires you to specify the num‐\\nber of clusters. So how can you find it?\\nSelecting the Number of Clusters\\nWith K-Means, you could use the inertia or the silhouette score to select the appro‐\\npriate number of clusters, but with Gaussian mixtures, it is not possible to use these\\nmetrics because they are not reliable when the clusters are not spherical or have dif‐\\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐\\nGaussian Mixtures | 267', 'children': []}, {'id': 180, 'title': 'Bayesian Gaussian Mixture Models', 'content': '>>> gm.bic(X)\\n8189.74345832983\\n>>> gm.aic(X)\\n8102.518178214792\\nFigure 9-21  shows the BIC for different numbers of clusters k. As you can see, both\\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\\nthat we could also search for the best value for the covariance_type  hyperparameter.\\nFor example, if it is \"spherical\"  rather than \"full\" , then the model has much fewer\\nparameters to learn, but it does not fit the data as well.\\nFigure 9-21. AIC and BIC for different  numbers of clusters k\\nBayesian Gaussian Mixture Models\\nRather than manually searching for the optimal number of clusters, it is possible to\\nuse instead the BayesianGaussianMixture  class which is capable of giving weights\\nequal (or close) to zero to unnecessary clusters. Just set the number of clusters n_com\\nponents  to a value that you have good reason to believe is greater than the optimal\\nnumber of clusters (this assumes some minimal knowledge about the problem at\\nhand), and the algorithm will eliminate the unnecessary clusters automatically. For\\nexample, let’s set the number of clusters to 10 and see what happens:\\n>>> from sklearn.mixture  import BayesianGaussianMixture\\n>>> bgm = BayesianGaussianMixture (n_components =10, n_init=10, random_state =42)\\n>>> bgm.fit(X)\\n>>> np.round(bgm.weights_ , 2)\\narray([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])\\nPerfect: the algorithm automatically detected that only 3 clusters are needed, and the\\nresulting clusters are almost identical to the ones in Figure 9-17 .\\nIn this model, the cluster parameters (including the weights, means and covariance\\nmatrices) are not treated as fixed model parameters anymore, but as latent random\\nvariables, like the cluster assignments (see Figure 9-22 ). So z now includes both the\\ncluster parameters and the cluster assignments.\\n270 | Chapter 9: Unsupervised Learning Techniques', 'children': []}, {'id': 181, 'title': 'Other Anomaly Detection and Novelty Detection Algorithms', 'content': 'Other Anomaly Detection and Novelty Detection Algorithms\\nScikit-Learn also implements a few algorithms dedicated to anomaly detection or\\nnovelty detection:\\n•Fast-MCD  (minimum covariance determinant), implemented by the EllipticEn\\nvelope  class: this algorithm is useful for outlier detection, in particular to\\ncleanup a dataset. It assumes that the normal instances (inliers) are generated\\nfrom a single Gaussian distribution (not a mixture), but it also assumes that the\\ndataset is contaminated with outliers that were not generated from this Gaussian\\ndistribution. When it estimates the parameters of the Gaussian distribution (i.e.,\\nthe shape of the elliptic envelope around the inliers), it is careful to ignore the\\ninstances that are most likely outliers. This gives a better estimation of the elliptic\\nenvelope, and thus makes it better at identifying the outliers.\\n•Isolation forest : this is an efficient algorithm for outlier detection, especially in\\nhigh-dimensional datasets. The algorithm builds a Random Forest in which each\\nDecision Tree is grown randomly: at each node, it picks a feature randomly, then\\nit picks a random threshold value (between the min and max value) to split the\\ndataset in two. The dataset gradually gets chopped into pieces this way, until all\\ninstances end up isolated from the other instances. An anomaly is usually far\\nfrom other instances, so on average (across all the Decision Trees) it tends to get\\nisolated in less steps than normal instances.\\n•Local outlier factor  (LOF): this algorithm is also good for outlier detection. It\\ncompares the density of instances around a given instance to the density around\\nits neighbors. An anomaly is often more isolated than its k nearest neighbors.\\n•One-class SVM : this algorithm is better suited for novelty detection. Recall that a\\nkernelized SVM classifier separates two classes by first (implicitly) mapping all\\nthe instances to a high-dimensional space, then separating the two classes using a\\nlinear SVM classifier within this high-dimensional space (see Chapter 5 ). Since\\nwe just have one class of instances, the one-class SVM algorithm instead tries to\\nseparate the instances in high-dimensional space from the origin. In the original\\nspace, this will correspond to finding a small region that encompasses all the\\ninstances. If a new instance does not fall within this region, it is an anomaly.\\nThere are a few hyperparameters to tweak: the usual ones for a kernelized SVM,\\nplus a margin hyperparameter that corresponds to the probability of a new\\ninstance being mistakenly considered as novel, when it is in fact normal. It works\\ngreat, especially with high-dimensional datasets, but just like all SVMs, it does\\nnot scale to large datasets.\\n274 | Chapter 9: Unsupervised Learning Techniques', 'children': []}]}]}]}, {'id': 182, 'title': 'Part II. Neural Networks and Deep Learning', 'content': 'PART II\\nNeural Networks and Deep Learning', 'children': [{'id': 183, 'title': 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 'content': '1Y ou can get the best of both worlds by being open to biological inspirations without being afraid to create\\nbiologically unrealistic models, as long as they work well.\\nCHAPTER 10\\nIntroduction to Artificial  Neural Networks\\nwith Keras\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 10 in the final\\nrelease of the book.\\nBirds inspired us to fly, burdock plants inspired velcro, and countless more inven‐\\ntions were inspired by nature. It seems only logical, then, to look at the brain’s archi‐\\ntecture for inspiration on how to build an intelligent machine. This is the key idea\\nthat sparked artificial  neural networks  (ANNs). However, although planes were\\ninspired by birds, they don’t have to flap their wings. Similarly, ANNs have gradually\\nbecome quite different from their biological cousins. Some researchers even argue\\nthat we should drop the biological analogy altogether (e.g., by saying “units” rather\\nthan “neurons”), lest we restrict our creativity to biologically plausible systems.1\\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐\\nble, making them ideal to tackle large and highly complex Machine Learning tasks,\\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni‐\\ntion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds\\nof millions of users every day (e.g., Y ouTube), or learning to beat the world champion\\nat the game of Go by playing millions of games against itself (DeepMind’s Alpha‐\\nZero).\\n277', 'children': [{'id': 184, 'title': 'From Biological to Artificial Neurons', 'content': '2“ A Logical Calculus of Ideas Immanent in Nervous Activity, ” W . McCulloch and W . Pitts (1943).In the first part of this chapter, we will introduce artificial neural networks, starting\\nwith a quick tour of the very first ANN architectures, leading up to Multi-Layer Per‐\\nceptrons  (MLPs) which are heavily used today (other architectures will be explored in\\nthe next chapters). In the second part, we will look at how to implement neural net‐\\nworks using the popular Keras API. This is a beautifully designed and simple high-\\nlevel API for building, training, evaluating and running neural networks. But don’t be\\nfooled by its simplicity: it is expressive and flexible enough to let you build a wide\\nvariety of neural network architectures. In fact, it will probably be sufficient for most\\nof your use cases. Moreover, should you ever need extra flexibility, you can always\\nwrite custom Keras components using its lower-level API, as we will see in Chap‐\\nter 12 .\\nBut first, let’s go back in time to see how artificial neural networks came to be!\\nFrom Biological to Artificial  Neurons\\nSurprisingly, ANNs have been around for quite a while: they were first introduced\\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\\nWalter Pitts. In their landmark paper ,2 “ A Logical Calculus of Ideas Immanent in\\nNervous Activity, ” McCulloch and Pitts presented a simplified computational model\\nof how biological neurons might work together in animal brains to perform complex\\ncomputations using propositional logic . This was the first artificial neural network\\narchitecture. Since then many other architectures have been invented, as we will see.\\nThe early successes of ANNs until the 1960s led to the widespread belief that we\\nwould soon be conversing with truly intelligent machines. When it became clear that\\nthis promise would go unfulfilled (at least for quite a while), funding flew elsewhere\\nand ANNs entered a long winter. In the early 1980s there was a revival of interest in \\nconnectionism  (the study of neural networks), as new architectures were invented and\\nbetter training techniques were developed. But progress was slow, and by the 1990s\\nother powerful Machine Learning techniques were invented, such as Support Vector\\nMachines (see Chapter 5 ). These techniques seemed to offer better results and stron‐\\nger theoretical foundations than ANNs, so once again the study of neural networks\\nentered a long winter.\\nFinally, we are now witnessing yet another wave of interest in ANNs. Will this wave\\ndie out like the previous ones did? Well, there are a few good reasons to believe that\\nthis wave is different and that it will have a much more profound impact on our lives:\\n278 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 185, 'title': 'Biological Neurons', 'content': '•There is now a huge quantity of data available to train neural networks, and\\nANNs frequently outperform other ML techniques on very large and complex\\nproblems.\\n•The tremendous increase in computing power since the 1990s now makes it pos‐\\nsible to train large neural networks in a reasonable amount of time. This is in\\npart due to Moore’s Law, but also thanks to the gaming industry, which has pro‐\\nduced powerful GPU cards by the millions.\\n•The training algorithms have been improved. To be fair they are only slightly dif‐\\nferent from the ones used in the 1990s, but these relatively small tweaks have a\\nhuge positive impact.\\n•Some theoretical limitations of ANNs have turned out to be benign in practice.\\nFor example, many people thought that ANN training algorithms were doomed\\nbecause they were likely to get stuck in local optima, but it turns out that this is\\nrather rare in practice (or when it is the case, they are usually fairly close to the\\nglobal optimum).\\n•ANNs seem to have entered a virtuous circle of funding and progress. Amazing\\nproducts based on ANNs regularly make the headline news, which pulls more\\nand more attention and funding toward them, resulting in more and more pro‐\\ngress, and even more amazing products.\\nBiological Neurons\\nBefore we discuss artificial neurons, let’s take a quick look at a biological neuron (rep‐\\nresented in Figure 10-1 ). It is an unusual-looking cell mostly found in animal cerebral\\ncortexes (e.g., your brain), composed of a cell body  containing the nucleus and most\\nof the cell’s complex components, and many branching extensions called dendrites ,\\nplus one very long extension called the axon . The axon’s length may be just a few\\ntimes longer than the cell body, or up to tens of thousands of times longer. Near its\\nextremity the axon splits off into many branches called telodendria , and at the tip of\\nthese branches are minuscule structures called synaptic terminals  (or simply synap‐\\nses), which are connected to the dendrites (or directly to the cell body) of other neu‐\\nrons. Biological neurons receive short electrical impulses called signals  from other\\nneurons via these synapses. When a neuron receives a sufficient number of signals\\nfrom other neurons within a few milliseconds, it fires its own signals.\\nFrom Biological to Artificial  Neurons | 279', 'children': []}, {'id': 186, 'title': 'Logical Computations with Neurons', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 187, 'title': 'The Perceptron', 'content': 'Logical Computations with Neurons\\nWarren McCulloch and Walter Pitts proposed a very simple model of the biological\\nneuron, which later became known as an artificial  neuron : it has one or more binary\\n(on/off) inputs and one binary output. The artificial neuron simply activates its out‐\\nput when more than a certain number of its inputs are active. McCulloch and Pitts\\nshowed that even with such a simplified model it is possible to build a network of\\nartificial neurons that computes any logical proposition you want. For example, let’s\\nbuild a few ANNs that perform various logical computations (see Figure 10-3 ),\\nassuming that a neuron is activated when at least two of its inputs are active.\\nFigure 10-3. ANNs performing simple logical computations\\n•The first network on the left is simply the identity function: if neuron A is activa‐\\nted, then neuron C gets activated as well (since it receives two input signals from\\nneuron A), but if neuron A is off, then neuron C is off as well.\\n•The second network performs a logical AND: neuron C is activated only when\\nboth neurons A and B are activated (a single input signal is not enough to acti‐\\nvate neuron C).\\n•The third network performs a logical OR: neuron C gets activated if either neu‐\\nron A or neuron B is activated (or both).\\n•Finally, if we suppose that an input connection can inhibit the neuron’s activity\\n(which is the case with biological neurons), then the fourth network computes a\\nslightly more complex logical proposition: neuron C is activated only if neuron A\\nis active and if neuron B is off. If neuron A is active all the time, then you get a\\nlogical NOT: neuron C is active when neuron B is off, and vice versa.\\nY ou can easily imagine how these networks can be combined to compute complex\\nlogical expressions (see the exercises at the end of the chapter).\\nThe Perceptron\\nThe Perceptron  is one of the simplest ANN architectures, invented in 1957 by Frank\\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4 ) called \\nFrom Biological to Artificial  Neurons | 281', 'children': []}, {'id': 188, 'title': 'Multi-Layer Perceptron and Backpropagation', 'content': '8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\\nANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\\n9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\\nMulti-Layer Perceptron and Backpropagation\\nAn MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\\ncalled hidden layers , and one final layer of TLUs called the output layer  (see\\nFigure 10-7 ). The layers close to the input layer are usually called the lower layers,\\nand the ones close to the outputs are usually called the upper layers. Every layer\\nexcept the output layer includes a bias neuron and is fully connected to the next layer.\\nFigure 10-7. Multi-Layer Perceptron\\nThe signal flows only in one direction (from the inputs to the out‐\\nputs), so this architecture is an example of a feedforward neural net‐\\nwork  (FNN).\\nWhen an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\\nwork  (DNN). The field of Deep Learning studies DNNs, and more generally models\\ncontaining deep stacks of computations. However, many people talk about Deep\\nLearning whenever neural networks are involved (even shallow ones).\\nFor many years researchers struggled to find a way to train MLPs, without success.\\nBut in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\\ngroundbreaking paper9 introducing the backpropagation  training algorithm, which is\\nstill used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\\n286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 189, 'title': 'Regression MLPs', 'content': '11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\\none of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\\noutput value also helps reduce some issues during Gradient Descent (we will\\ncome back to this in Chapter 11 ).\\nThese popular activation functions and their derivatives are represented in\\nFigure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\\nyou chain several linear transformations, all you get is a linear transformation. For\\nexample, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\\ngives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\\nhave some non-linearity between layers, then even a deep stack of layers is equivalent\\nto a single layer: you cannot solve very complex problems with that.\\nFigure 10-8. Activation functions and their derivatives\\nOkay! So now you know where neural nets came from, what their architecture is and\\nhow to compute their outputs, and you also learned about the backpropagation algo‐\\nrithm. But what exactly can you do with them?\\nRegression MLPs\\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\\nthe price of a house given many of its features), then you just need a single output\\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict\\nmultiple values at once), you need one output neuron per output dimension. For\\nexample, to locate the center of an object on an image, you need to predict 2D coordi‐\\nnates, so you need two output neurons. If you also want to place a bounding box\\naround the object, then you need two more numbers: the width and the height of the\\nobject. So you end up with 4 output neurons.\\nFrom Biological to Artificial  Neurons | 289', 'children': []}, {'id': 190, 'title': 'Classification MLPs', 'content': 'In general, when building an MLP for regression, you do not want to use any activa‐\\ntion function for the output neurons, so they are free to output any range of values.\\nHowever, if you want to guarantee that the output will always be positive, then you\\ncan use the ReLU activation function, or the softplus  activation function in the output\\nlayer. Finally, if you want to guarantee that the predictions will fall within a given\\nrange of values, then you can use the logistic function or the hyperbolic tangent, and\\nscale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\\nthe hyperbolic tangent.\\nThe loss function to use during training is typically the mean squared error, but if you\\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\\nboth.\\nThe Huber loss is quadratic when the error is smaller than a thres‐\\nhold δ (typically 1), but linear when the error is larger than δ. This\\nmakes it less sensitive to outliers than the mean squared error, and\\nit is often more precise and converges faster than the mean abso‐\\nlute error.\\nTable 10-1  summarizes the typical architecture of a regression MLP .\\nTable 10-1. Typical Regression MLP Architecture\\nHyperparameter Typical Value\\n# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\\n# hidden layers Depends on the problem. Typically 1 to 5.\\n# neurons per hidden layer Depends on the problem. Typically 10 to 100.\\n# output neurons 1 per prediction dimension\\nHidden activation ReLU (or SELU, see Chapter 11 )\\nOutput activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\\nLoss function MSE or MAE/Huber (if outliers)\\nClassification  MLPs\\nMLPs can also be used for classification tasks. For a binary classification problem,\\nyou just need a single output neuron using the logistic activation function: the output\\nwill be a number between 0 and 1, which you can interpret as the estimated probabil‐\\nity of the positive class. Obviously, the estimated probability of the negative class is\\nequal to one minus that number.\\nMLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\\nexample, you could have an email classification system that predicts whether each\\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\\n290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 191, 'title': 'Implementing MLPs with Keras', 'content': '12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\\nHyperparameter Binary classification Multilabel binary classification Multiclass classification\\nLoss function Cross-Entropy Cross-Entropy Cross-Entropy\\nBefore we go on, I recommend you go through exercise 1, at the\\nend of this chapter. Y ou will play with various neural network\\narchitectures and visualize their outputs using the TensorFlow Play‐\\nground . This will be very useful to better understand MLPs, for\\nexample the effects of all the hyperparameters (number of layers\\nand neurons, activation functions, and more).\\nNow you have all the concepts you need to start implementing MLPs with Keras!\\nImplementing MLPs with Keras\\nKeras is a high-level Deep Learning API that allows you to easily build, train, evaluate\\nand execute all sorts of neural networks. Its documentation (or specification) is avail‐\\nable at https://keras.io . The reference implementation is simply called Keras as well, so\\nto avoid any confusion we will call it keras-team (since it is available at https://\\ngithub.com/keras-team/keras ). It was developed by François Chollet as part of a\\nresearch project12 and released as an open source project in March 2015. It quickly\\ngained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\\nform the heavy computations required by neural networks, keras-team relies on a\\ncomputation backend. At the present, you can choose from three popular open\\nsource deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\\nTheano.\\nMoreover, since late 2016, other implementations have been released. Y ou can now\\nrun Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\\ncode in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\\njust Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\\nimplementation called tf.keras. It only supports TensorFlow as the backend, but it has\\nthe advantage of offering some very useful extra features (see Figure 10-10 ): for\\nexample, it supports TensorFlow’s Data API which makes it quite easy to load and\\npreprocess data efficiently. For this reason, we will use tf.keras in this book. However,\\nin this chapter we will not use any of the TensorFlow-specific features, so the code\\nshould run fine on other Keras implementations as well (at least in Python), with only\\nminor modifications, such as changing the imports.\\n292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': [{'id': 192, 'title': 'Installing TensorFlow 2', 'content': \"Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\\nAs tf.keras is bundled with TensorFlow, let’s install TensorFlow!\\nInstalling TensorFlow 2\\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\\ntions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\\nisolated environment using virtualenv, you first need to activate it:\\n$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\\n$ source env/bin/activate  # on Linux or MacOSX\\n$ .\\\\env\\\\Scripts\\\\activate   # on Windows\\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\\ntrator rights, or to add the --user  option):\\n$ python3 -m pip install --upgrade tensorflow\\nFor GPU support, you need to install tensorflow-gpu  instead of\\ntensorflow , and there are other libraries to install. See https://\\ntensorflow.org/install/gpu  for more details.\\nTo test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\\nsorFlow and tf.keras, and print their versions:\\n>>> import tensorflow  as tf\\n>>> from tensorflow  import keras\\n>>> tf.__version__\\n'2.0.0'\\n>>> keras.__version__\\n'2.2.4-tf'\\nImplementing MLPs with Keras | 293\", 'children': []}, {'id': 193, 'title': 'Building an Image Classifier Using the Sequential API', 'content': \"The second version is the version of the Keras API implemented by tf.keras. Note that\\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\\nsome extra TensorFlow-specific features.\\nNow let’s use tf.keras! Let’s start by building a simple image classifier.\\nBuilding an Image Classifier  Using the Sequential API\\nFirst, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\\nreplacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\\nMNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\\nimages represent fashion items rather than handwritten digits, so each class is more\\ndiverse and the problem turns out to be significantly more challenging than MNIST.\\nFor example, a simple linear model reaches about 92% accuracy on MNIST, but only\\nabout 83% on Fashion MNIST.\\nUsing Keras to Load the Dataset\\nKeras provides some utility functions to fetch and load common datasets, including\\nMNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\\nFashion MNIST:\\nfashion_mnist  = keras.datasets .fashion_mnist\\n(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\\nimportant difference is that every image is represented as a 28×28 array rather than a\\n1D array of size 784. Moreover, the pixel intensities are represented as integers (from\\n0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\\ntraining set:\\n>>> X_train_full .shape\\n(60000, 28, 28)\\n>>> X_train_full .dtype\\ndtype('uint8')\\nNote that the dataset is already split into a training set and a test set, but there is no\\nvalidation set, so let’s create one. Moreover, since we are going to train the neural net‐\\nwork using Gradient Descent, we must scale the input features. For simplicity, we just\\nscale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\\nconverts them to floats):\\nX_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\\ny_valid, y_train = y_train_full [:5000], y_train_full [5000:]\\nWith MNIST, when the label is equal to 5, it means that the image represents the\\nhandwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\\nnames to know what we are dealing with:\\n294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\", 'children': []}, {'id': 194, 'title': 'Building a Regression MLP Using the Sequential API', 'content': '>>> y_new = y_test[:3]\\n>>> y_new\\narray([9, 2, 1])\\nNow you know how to build, train, evaluate and use a classification MLP using the\\nSequential API. But what about regression?\\nBuilding a Regression MLP Using the Sequential API\\nLet’s switch to the California housing problem and tackle it using a regression neural\\nnetwork. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\\nfunction to load the data: this dataset is simpler than the one we used in Chapter 2 ,\\nsince it contains only numerical features (there is no ocean_proximity  feature), and\\nthere is no missing value. After loading the data, we split it into a training set, a vali‐\\ndation set and a test set, and we scale all the features:\\nfrom sklearn.datasets  import fetch_california_housing\\nfrom sklearn.model_selection  import train_test_split\\nfrom sklearn.preprocessing  import StandardScaler\\nhousing = fetch_california_housing ()\\nX_train_full , X_test, y_train_full , y_test = train_test_split (\\n    housing.data, housing.target)\\nX_train, X_valid, y_train, y_valid = train_test_split (\\n    X_train_full , y_train_full )\\nscaler = StandardScaler ()\\nX_train_scaled  = scaler.fit_transform (X_train)\\nX_valid_scaled  = scaler.transform (X_valid)\\nX_test_scaled  = scaler.transform (X_test)\\nBuilding, training, evaluating and using a regression MLP using the Sequential API to\\nmake predictions is quite similar to what we did for classification. The main differ‐\\nences are the fact that the output layer has a single neuron (since we only want to\\npredict a single value) and uses no activation function, and the loss function is the\\nmean squared error. Since the dataset is quite noisy, we just use a single hidden layer\\nwith fewer neurons than before, to avoid overfitting:\\nmodel = keras.models.Sequential ([\\n    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\\n    keras.layers.Dense(1)\\n])\\nmodel.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\\nhistory = model.fit(X_train, y_train, epochs=20,\\n                    validation_data =(X_valid, y_valid))\\nmse_test  = model.evaluate (X_test, y_test)\\nX_new = X_test[:3] # pretend these are new instances\\ny_pred = model.predict(X_new)\\nImplementing MLPs with Keras | 303', 'children': []}, {'id': 195, 'title': 'Building Complex Models Using the Functional API', 'content': '14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\\nmodels are extremely common, it is sometimes useful to build neural networks with\\nmore complex topologies, or with multiple inputs or outputs. For this purpose, Keras\\noffers the Functional API.\\nBuilding Complex Models Using the Functional API\\nOne example of a non-sequential neural network is a Wide & Deep  neural network.\\nThis neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\\net al.14. It connects all or part of the inputs directly to the output layer, as shown in\\nFigure 10-13 . This architecture makes it possible for the neural network to learn both\\ndeep patterns (using the deep path) and simple rules (through the short path). In\\ncontrast, a regular MLP forces all the data to flow through the full stack of layers, thus\\nsimple patterns in the data may end up being distorted by this sequence of transfor‐\\nmations.\\n304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 196, 'title': 'Building Dynamic Models Using the Subclassing API', 'content': '[...] # Same as above, up to the main output layer\\noutput = keras.layers.Dense(1)(concat)\\naux_output  = keras.layers.Dense(1)(hidden2)\\nmodel = keras.models.Model(inputs=[input_A, input_B],\\n                           outputs=[output, aux_output ])\\nEach output will need its own loss function, so when we compile the model we\\nshould pass a list of losses (if we pass a single loss, Keras will assume that the same\\nloss must be used for all outputs). By default, Keras will compute all these losses and\\nsimply add them up to get the final loss used for training. However, we care much\\nmore about the main output than about the auxiliary output (as it is just used for reg‐\\nularization), so we want to give the main output’s loss a much greater weight. Fortu‐\\nnately, it is possible to set all the loss weights when compiling the model:\\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\\nNow when we train the model, we need to provide some labels for each output. In\\nthis example, the main output and the auxiliary output should try to predict the same\\nthing, so they should use the same labels. So instead of passing y_train , we just need\\nto pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\\nhistory = model.fit(\\n    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\\n    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ‐\\nual losses:\\ntotal_loss , main_loss , aux_loss  = model.evaluate (\\n    [X_test_A , X_test_B ], [y_test, y_test])\\nSimilarly, the predict()  method will return predictions for each output:\\ny_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\\nAs you can see, you can build any sort of architecture you want quite easily with the\\nFunctional API. Let’s look at one last way you can build Keras models.\\nBuilding Dynamic Models Using the Subclassing API\\nBoth the Sequential API and the Functional API are declarative: you start by declar‐\\ning which layers you want to use and how they should be connected, and only then\\ncan you start feeding the model some data for training or inference. This has many\\nadvantages: the model can easily be saved, cloned, shared, its structure can be dis‐\\nplayed and analyzed, the framework can infer shapes and check types, so errors can\\nbe caught early (i.e., before any data ever goes through the model). It’s also fairly easy\\nto debug, since the whole model is just a static graph of layers. But the flip side is just\\nthat: it’s static. Some models involve loops, varying shapes, conditional branching,\\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\\ntive programming style, the Subclassing API is for you.\\nImplementing MLPs with Keras | 309', 'children': []}, {'id': 197, 'title': 'Saving and Restoring a Model', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 198, 'title': 'Using Callbacks', 'content': 'Keras models can be used just like regular layers, so you can easily\\ncompose them to build complex architectures.\\nNow that you know how to build and train neural nets using Keras, you will want to\\nsave them!\\nSaving and Restoring a Model\\nSaving a trained Keras model is as simple as it gets:\\nmodel.save(\"my_keras_model.h5\" )\\nKeras will save both the model’s architecture (including every layer’s hyperparame‐\\nters) and the value of all the model parameters for every layer (e.g., connection\\nweights and biases), using the HDF5 format. It also saves the optimizer (including its\\nhyperparameters and any state it may have).\\nY ou will typically have a script that trains a model and saves it, and one or more\\nscripts (or web services) that load the model and use it to make predictions. Loading\\nthe model is just as easy:\\nmodel = keras.models.load_model (\"my_keras_model.h5\" )\\nThis will work when using the Sequential API or the Functional\\nAPI, but unfortunately not when using Model subclassing. How‐\\never, you can use save_weights()  and load_weights()  to at least\\nsave and restore the model parameters (but you will need to save\\nand restore everything else yourself).\\nBut what if training lasts several hours? This is quite common, especially when train‐\\ning on large datasets. In this case, you should not only save your model at the end of\\ntraining, but also save checkpoints at regular intervals during training. But how can\\nyou tell the fit()  method to save checkpoints? The answer is: using callbacks.\\nUsing Callbacks\\nThe fit()  method accepts a callbacks  argument that lets you specify a list of objects\\nthat Keras will call during training at the start and end of training, at the start and end\\nof each epoch and even before and after processing each batch. For example, the Mod\\nelCheckpoint  callback saves checkpoints of your model at regular intervals during\\ntraining, by default at the end of each epoch:\\nImplementing MLPs with Keras | 311', 'children': []}, {'id': 199, 'title': 'Visualization Using TensorBoard', 'content': 'class PrintValTrainRatioCallback (keras.callbacks .Callback ):\\n    def on_epoch_end (self, epoch, logs):\\n        print(\"\\\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\\nAs you might expect, you can implement on_train_begin() , on_train_end() ,\\non_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\\nMoreover, callbacks can also be used during evaluation and predictions, should you\\never need them (e.g., for debugging). In this case, you should implement\\non_test_begin() , on_test_end() , on_test_batch_begin() , or\\non_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\\ndict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\\npredict() ).\\nNow let’s take a look at one more tool you should definitely have in your toolbox\\nwhen using tf.keras: TensorBoard.\\nVisualization Using TensorBoard\\nTensorBoard is a great interactive visualization tool that you can use to view the\\nlearning curves during training, compare learning curves between multiple runs, vis‐\\nualize the computation graph, analyze training statistics, view images generated by\\nyour model, visualize complex multidimensional data projected down to 3D and\\nautomatically clustered for you, and more! This tool is installed automatically when\\nyou install TensorFlow, so you already have it!\\nTo use it, you must modify your program so that it outputs the data you want to visu‐\\nalize to special binary log files called event files. Each binary data record is called a\\nsummary . The TensorBoard server will monitor the log directory, and it will automat‐\\nically pick up the changes and update the visualizations: this allows you to visualize\\nlive data (with a short delay), such as the learning curves during training. In general,\\nyou want to point the TensorBoard server to a root log directory, and configure your\\nprogram so that it writes to a different subdirectory every time it runs. This way, the\\nsame TensorBoard server instance will allow you to visualize and compare data from\\nmultiple runs of your program, without getting everything mixed up.\\nSo let’s start by defining the root log directory we will use for our TensorBoard logs,\\nplus a small function that will generate a subdirectory path based on the current date\\nand time, so that it is different at every run. Y ou may want to include extra informa‐\\ntion in the log directory name, such as hyperparameter values that you are testing, to\\nmake it easier to know what you are looking at in TensorBoard:\\nroot_logdir  = os.path.join(os.curdir, \"my_logs\" )\\ndef get_run_logdir ():\\n    import time\\n    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\\n    return os.path.join(root_logdir , run_id)\\nImplementing MLPs with Keras | 313', 'children': []}]}, {'id': 200, 'title': 'Fine-Tuning Neural Network Hyperparameters', 'content': 'Figure 10-16. Visualizing Learning Curves with TensorBoard\\nUnfortunately, at the time of writing, no other data is exported by the TensorBoard\\ncallback, but this issue will probably be fixed by the time you read these lines. In Ten‐\\nsorFlow 1, this callback exported the computation graph and many useful statistics:\\ntype help(keras.callbacks.TensorBoard)  to see all the options.\\nLet’s summarize what you learned so far in this chapter: we saw where neural nets\\ncame from, what an MLP is and how you can use it for classification and regression,\\nhow to build MLPs using tf.keras’s Sequential API, or more complex architectures\\nusing the Functional API or Model  Subclassing, you learned how to save and restore a\\nmodel, use callbacks for checkpointing, early stopping, and more, and finally how to\\nuse TensorBoard for visualization. Y ou can already go ahead and use neural networks\\nto tackle many problems! However, you may wonder how to choose the number of\\nhidden layers, the number of neurons in the network, and all the other hyperparame‐\\nters. Let’s look at this now.\\nFine-Tuning Neural Network Hyperparameters\\nThe flexibility of neural networks is also one of their main drawbacks: there are many\\nhyperparameters to tweak. Not only can you use any imaginable network architec‐\\nture, but even in a simple MLP you can change the number of layers, the number of\\nneurons per layer, the type of activation function to use in each layer, the weight initi‐\\nFine-Tuning Neural Network Hyperparameters | 315', 'children': [{'id': 201, 'title': 'Number of Hidden Layers', 'content': 'Number of Hidden Layers\\nFor many problems, you can just begin with a single hidden layer and you will get\\nreasonable results. It has actually been shown that an MLP with just one hidden layer\\ncan model even the most complex functions provided it has enough neurons. For a\\nlong time, these facts convinced researchers that there was no need to investigate any\\ndeeper neural networks. But they overlooked the fact that deep networks have a much\\nhigher parameter efficiency  than shallow ones: they can model complex functions\\nusing exponentially fewer neurons than shallow nets, allowing them to reach much\\nbetter performance with the same amount of training data.\\nTo understand why, suppose you are asked to draw a forest using some drawing soft‐\\nware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\\nindividually, branch per branch, leaf per leaf. If you could instead draw one leaf,\\ncopy/paste it to draw a branch, then copy/paste that branch to create a tree, and\\nfinally copy/paste this tree to make a forest, you would be finished in no time. Real-\\nworld data is often structured in such a hierarchical way and Deep Neural Networks\\nautomatically take advantage of this fact: lower hidden layers model low-level struc‐\\ntures (e.g., line segments of various shapes and orientations), intermediate hidden\\nlayers combine these low-level structures to model intermediate-level structures (e.g.,\\nsquares, circles), and the highest hidden layers and the output layer combine these\\nintermediate structures to model high-level structures (e.g., faces).\\nNot only does this hierarchical architecture help DNNs converge faster to a good sol‐\\nution, it also improves their ability to generalize to new datasets. For example, if you\\nhave already trained a model to recognize faces in pictures, and you now want to\\ntrain a new neural network to recognize hairstyles, then you can kickstart training by\\nreusing the lower layers of the first network. Instead of randomly initializing the\\nweights and biases of the first few layers of the new neural network, you can initialize\\nthem to the value of the weights and biases of the lower layers of the first network.\\nThis way the network will not have to learn from scratch all the low-level structures\\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\\nhairstyles). This is called transfer learning .\\nIn summary, for many problems you can start with just one or two hidden layers and\\nit will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\\ndataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\\nracy using two hidden layers with the same total amount of neurons, in roughly the\\nsame amount of training time). For more complex problems, you can gradually ramp\\nup the number of hidden layers, until you start overfitting the training set. Very com‐\\nplex tasks, such as large image classification or speech recognition, typically require\\nnetworks with dozens of layers (or even hundreds, but not fully connected ones, as\\nwe will see in Chapter 14 ), and they need a huge amount of training data. However,\\nyou will rarely have to train such networks from scratch: it is much more common to\\nFine-Tuning Neural Network Hyperparameters | 319', 'children': []}, {'id': 202, 'title': 'Number of Neurons per Hidden Layer', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}, {'id': 203, 'title': 'Learning Rate, Batch Size and Other Hyperparameters', 'content': '17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\\nTraining will be a lot faster and require much less data (we will discuss this in Chap‐\\nter 11 ).\\nNumber of Neurons per Hidden Layer\\nObviously the number of neurons in the input and output layers is determined by the\\ntype of input and output your task requires. For example, the MNIST task requires 28\\nx 28 = 784 input neurons and 10 output neurons.\\nAs for the hidden layers, it used to be a common practice to size them to form a pyra‐\\nmid, with fewer and fewer neurons at each layer—the rationale being that many low-\\nlevel features can coalesce into far fewer high-level features. For example, a typical\\nneural network for MNIST may have three hidden layers, the first with 300 neurons,\\nthe second with 200, and the third with 100. However, this practice has been largely\\nabandoned now, as it seems that simply using the same number of neurons in all hid‐\\nden layers performs just as well in most cases, or even better, and there is just one\\nhyperparameter to tune instead of one per layer—for example, all hidden layers could\\nsimply have 150 neurons. However, depending on the dataset, it can sometimes help\\nto make the first hidden layer bigger than the others.\\nJust like for the number of layers, you can try increasing the number of neurons grad‐\\nually until the network starts overfitting. In general you will get more bang for the\\nbuck by increasing the number of layers than the number of neurons per layer.\\nUnfortunately, as you can see, finding the perfect amount of neurons is still somewhat\\nof a dark art.\\nA simpler approach is to pick a model with more layers and neurons than you\\nactually need, then use early stopping to prevent it from overfitting (and other regu‐\\nlarization techniques, such as dropout , as we will see in Chapter 11 ). This has been\\ndubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\\nperfectly match your size, just use large stretch pants that will shrink down to the\\nright size.\\nLearning Rate, Batch Size and Other Hyperparameters\\nThe number of hidden layers and neurons are not the only hyperparameters you can\\ntweak in an MLP . Here are some of the most important ones, and some tips on how\\nto set them:\\n•The learning rate is arguably the most important hyperparameter. In general, the\\noptimal learning rate is about half of the maximum learning rate (i.e., the learn‐\\n320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 204, 'title': 'Exercises', 'content': '19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\\ndata.19\\nExercises\\n1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\\n•Layers and patterns: try training the default neural network by clicking the run\\nbutton (top left). Notice how it quickly finds a good solution for the classifica‐\\ntion task. Notice that the neurons in the first hidden layer have learned simple\\npatterns, while the neurons in the second hidden layer have learned to com‐\\nbine the simple patterns of the first hidden layer into more complex patterns.\\nIn general, the more layers, the more complex the patterns can be.\\n•Activation function: try replacing the Tanh activation function with the ReLU\\nactivation function, and train the network again. Notice that it finds a solution\\neven faster, but this time the boundaries are linear. This is due to the shape of\\nthe ReLU function.\\n•Local minima: modify the network architecture to have just one hidden layer\\nwith three neurons. Train it multiple times (to reset the network weights, click\\nthe reset button next to the play button). Notice that the training time varies a\\nlot, and sometimes it even gets stuck in a local minimum.\\n•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\\nwork is now incapable of finding a good solution, even if you try multiple\\ntimes. The model has too few parameters and it systematically underfits the\\ntraining set.\\n•Large enough: next, set the number of neurons to 8 and train the network sev‐\\neral times. Notice that it is now consistently fast and never gets stuck. This\\nhighlights an important finding in neural network theory: large neural net‐\\nworks almost never get stuck in local minima, and even when they do these\\nlocal optima are almost as good as the global optimum. However, they can still\\nget stuck on long plateaus for a long time.\\n•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\\ntom right dataset under “DATA ”). Change the network architecture to have 4\\nhidden layers with 8 neurons each. Notice that training takes much longer, and\\noften gets stuck on plateaus for long periods of time. Also notice that the neu‐\\nrons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\\nrons in the lowest layers (i.e. on the left). This problem, called the “vanishing\\ngradients” problem, can be alleviated using better weight initialization and\\n322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras', 'children': []}]}, {'id': 205, 'title': 'Chapter 11. Training Deep Neural Networks', 'content': 'CHAPTER 11\\nTraining Deep Neural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 11 in the final\\nrelease of the book.\\nIn Chapter 10  we introduced artificial neural networks and trained our first deep\\nneural networks. But they were very shallow nets, with just a few hidden layers. What\\nif you need to tackle a very complex problem, such as detecting hundreds of types of\\nobjects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\\nhaps with 10 layers or much more, each containing hundreds of neurons, connected\\nby hundreds of thousands of connections. This would not be a walk in the park:\\n•First, you would be faced with the tricky vanishing gradients  problem (or the\\nrelated exploding gradients  problem) that affects deep neural networks and makes\\nlower layers very hard to train.\\n•Second, you might not have enough training data for such a large network, or it\\nmight be too costly to label.\\n•Third, training may be extremely slow.\\n•Fourth, a model with millions of parameters would severely risk overfitting the\\ntraining set, especially if there are not enough training instances, or they are too\\nnoisy.\\nIn this chapter, we will go through each of these problems in turn and present techni‐\\nques to solve them. We will start by explaining the vanishing gradients problem and\\nexploring some of the most popular solutions to this problem. Next, we will look at\\ntransfer learning and unsupervised pretraining, which can help you tackle complex\\n325', 'children': [{'id': 206, 'title': 'Vanishing/Exploding Gradients Problems', 'content': '1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\\nthat can speed up training large models tremendously compared to plain Gradient\\nDescent. Finally, we will go through a few popular regularization techniques for large\\nneural networks.\\nWith these tools, you will be able to train very deep nets: welcome to Deep Learning!\\nVanishing/Exploding Gradients Problems\\nAs we discussed in Chapter 10 , the backpropagation algorithm works by going from\\nthe output layer to the input layer, propagating the error gradient on the way. Once\\nthe algorithm has computed the gradient of the cost function with regards to each\\nparameter in the network, it uses these gradients to update each parameter with a\\nGradient Descent step.\\nUnfortunately, gradients often get smaller and smaller as the algorithm progresses\\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\\nlayer connection weights virtually unchanged, and training never converges to a good\\nsolution. This is called the vanishing gradients  problem. In some cases, the opposite\\ncan happen: the gradients can grow bigger and bigger, so many layers get insanely\\nlarge weight updates and the algorithm diverges. This is the exploding gradients  prob‐\\nlem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\\nally, deep neural networks suffer from unstable gradients; different layers may learn at\\nwidely different speeds.\\nAlthough this unfortunate behavior has been empirically observed for quite a while\\n(it was one of the reasons why deep neural networks were mostly abandoned for a\\nlong time), it is only around 2010 that significant progress was made in understand‐\\ning it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\\nNeural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\\ning the combination of the popular logistic sigmoid activation function and the\\nweight initialization technique that was most popular at the time, namely random ini‐\\ntialization using a normal distribution with a mean of 0 and a standard deviation of 1.\\nIn short, they showed that with this activation function and this initialization scheme,\\nthe variance of the outputs of each layer is much greater than the variance of its\\ninputs. Going forward in the network, the variance keeps increasing after each layer\\nuntil the activation function saturates at the top layers. This is actually made worse by\\nthe fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\\nfunction has a mean of 0 and behaves slightly better than the logistic function in deep\\nnetworks).\\n326 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 207, 'title': 'Glorot and He Initialization', 'content': '2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\\nif you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\\nout loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\\nas it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\\nderivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\\nno gradient to propagate back through the network, and what little gradient exists\\nkeeps getting diluted as backpropagation progresses down through the top layers, so\\nthere is really nothing left for the lower layers.\\nFigure 11-1. Logistic activation function saturation\\nGlorot and He Initialization\\nIn their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\\nlem. We need the signal to flow properly in both directions: in the forward direction\\nwhen making predictions, and in the reverse direction when backpropagating gradi‐\\nents. We don’t want the signal to die out, nor do we want it to explode and saturate.\\nFor the signal to flow properly, the authors argue that we need the variance of the\\noutputs of each layer to be equal to the variance of its inputs,2 and we also need the\\ngradients to have equal variance before and after flowing through a layer in the\\nreverse direction (please check out the paper if you are interested in the mathematical\\ndetails). It is actually not possible to guarantee both unless the layer has an equal\\nnumber of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\\nlayer), but they proposed a good compromise that has proven to work very well in\\npractice: the connection weights of each layer must be initialized randomly as\\nVanishing/Exploding Gradients Problems | 327', 'children': []}, {'id': 208, 'title': 'Nonsaturating Activation Functions', 'content': '4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\\ninputs is positive again.\\n5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\\nIf you want He initialization with a uniform distribution, but based on fanavg rather\\nthan fanin, you can use the VarianceScaling  initializer like this:\\nhe_avg_init  = keras.initializers .VarianceScaling (scale=2., mode=\\'fan_avg\\' ,\\n                                                 distribution =\\'uniform\\' )\\nkeras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\\nNonsaturating Activation Functions\\nOne of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\\nexploding gradients problems were in part due to a poor choice of activation func‐\\ntion. Until then most people had assumed that if Mother Nature had chosen to use\\nroughly sigmoid activation functions in biological neurons, they must be an excellent\\nchoice. But it turns out that other activation functions behave much better in deep\\nneural networks, in particular the ReLU activation function, mostly because it does\\nnot saturate for positive values (and also because it is quite fast to compute).\\nUnfortunately, the ReLU activation function is not perfect. It suffers from a problem\\nknown as the dying ReLUs : during training, some neurons effectively die, meaning\\nthey stop outputting anything other than 0. In some cases, you may find that half of\\nyour network’s neurons are dead, especially if you used a large learning rate. A neu‐\\nron dies when its weights get tweaked in such a way that the weighted sum of its\\ninputs are negative for all instances in the training set. When this happens, it just\\nkeeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\\nent of the ReLU function is 0 when its input is negative.4\\nTo solve this problem, you may want to use a variant of the ReLU function, such as\\nthe leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\\nFigure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\\nslope of the function for z < 0, and is typically set to 0.01. This small slope ensures\\nthat leaky ReLUs never die; they can go into a long coma, but they have a chance to\\neventually wake up. A 2015 paper5 compared several variants of the ReLU activation\\nfunction and one of its conclusions was that the leaky variants always outperformed\\nthe strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\\nresult in better performance than α = 0.01 (small leak). They also evaluated the\\nrandomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\\ntraining, and it is fixed to an average value during testing. It also performed fairly well\\nand seemed to act as a regularizer (reducing the risk of overfitting the training set).\\nVanishing/Exploding Gradients Problems | 329', 'children': []}, {'id': 209, 'title': 'Batch Normalization', 'content': '8“Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, ” S. Ioffe\\nand C. Szegedy (2015).Batch Normalization\\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi‐\\ncantly reduce the vanishing/exploding gradients problems at the beginning of train‐\\ning, it doesn’t guarantee that they won’t come back during training.\\nIn a 2015 paper ,8 Sergey Ioffe and Christian Szegedy proposed a technique called\\nBatch Normalization  (BN) to address the vanishing/exploding gradients problems.\\nThe technique consists of adding an operation in the model just before or after the\\nactivation function of each hidden layer, simply zero-centering and normalizing each\\ninput, then scaling and shifting the result using two new parameter vectors per layer:\\none for scaling, the other for shifting. In other words, this operation lets the model\\nlearn the optimal scale and mean of each of the layer’s inputs. In many cases, if you\\nadd a BN layer as the very first layer of your neural network, you do not need to\\nstandardize your training set (e.g., using a StandardScaler ): the BN layer will do it\\nfor you (well, approximately, since it only looks at one batch at a time, and it can also\\nrescale and shift each input feature).\\nIn order to zero-center and normalize the inputs, the algorithm needs to estimate\\neach input’s mean and standard deviation. It does so by evaluating the mean and stan‐\\ndard deviation of each input over the current mini-batch (hence the name “Batch\\nNormalization”). The whole operation is summarized in Equation 11-3 .\\nEquation 11-3. Batch Normalization algorithm\\n1 . μB=1\\nmB∑\\ni= 1mB\\nxi\\n2 . σB2=1\\nmB∑\\ni= 1mB\\nxi−μB2\\n3 . xi=xi−μB\\nσB2+?\\n4 . zi=γ⊗xi+β\\n•μB is the vector of input means, evaluated over the whole mini-batch B (it con‐\\ntains one mean per input).\\nVanishing/Exploding Gradients Problems | 333', 'children': []}, {'id': 210, 'title': 'Gradient Clipping', 'content': '10“Fixup Initialization: Residual Learning Without Normalization, ” Hongyi Zhang, Y ann N. Dauphin, Tengyu\\nMa (2019).\\n11“On the difficulty of training recurrent neural networks, ” R. Pascanu et al. (2013).layer, then the input batches will be 3D, with shape [batch size, height, width], there‐\\nfore the BN layer will compute 28 means and 28 standard deviations (one per column\\nof pixels, computed across all instances in the batch, and all rows in the column), and\\nit will normalize all pixels in a given column using the same mean and standard devi‐\\nation. There will also be just 28 scale parameters and 28 shift parameters. If instead\\nyou still want to treat each of the 784 pixels independently, then you should set\\naxis=[1, 2] .\\nNotice that the BN layer does not perform the same computation during training and\\nafter training: it uses batch statistics during training, and the “final” statistics after\\ntraining (i.e., the final value of the moving averages). Let’s take a peek at the source\\ncode of this class to see how this is handled:\\nclass BatchNormalization (Layer):\\n    [...]\\n    def call(self, inputs, training =None):\\n        if training  is None:\\n            training  = keras.backend.learning_phase ()\\n        [...]\\nThe call()  method is the one that actually performs the computations, and as you\\ncan see it has an extra training  argument: if it is None  it falls back to keras.back\\nend.learning_phase() , which returns 1 during training (the fit()  method ensures\\nthat). Otherwise, it returns 0. If you ever need to write a custom layer, and it needs to\\nbehave differently during training and testing, simply use the same pattern (we will\\ndiscuss custom layers in Chapter 12 ).\\nBatch Normalization has become one of the most used layers in deep neural net‐\\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\\nadded after every layer. However, a very recent paper10 by Hongyi Zhang et al. may\\nwell change this: the authors show that by using a novel fixed-update (fixup) weight\\ninitialization technique, they manage to train a very deep neural network (10,000 lay‐\\ners!) without BN, achieving state-of-the-art performance on complex image classifi‐\\ncation tasks.\\nGradient Clipping\\nAnother popular technique to lessen the exploding gradients problem is to simply\\nclip the gradients during backpropagation so that they never exceed some threshold.\\nThis is called Gradient Clipping .11 This technique is most often used in recurrent neu‐\\n338 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 211, 'title': 'Reusing Pretrained Layers', 'content': 'ral networks, as Batch Normalization is tricky to use in RNNs, as we will see in ???.\\nFor other types of networks, BN is usually sufficient.\\nIn Keras, implementing Gradient Clipping is just a matter of setting the clipvalue  or\\nclipnorm  argument when creating an optimizer. For example:\\noptimizer  = keras.optimizers .SGD(clipvalue =1.0)\\nmodel.compile(loss=\"mse\", optimizer =optimizer )\\nThis will clip every component of the gradient vector to a value between –1.0 and 1.0.\\nThis means that all the partial derivatives of the loss (with regards to each and every\\ntrainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyper‐\\nparameter you can tune. Note that it may change the orientation of the gradient vec‐\\ntor: for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\\ndirection of the second axis, but once you clip it by value, you get [0.9, 1.0], which\\npoints roughly in the diagonal between the two axes. In practice however, this\\napproach works well. If you want to ensure that Gradient Clipping does not change\\nthe direction of the gradient vector, you should clip by norm by setting clipnorm\\ninstead of clipvalue . This will clip the whole gradient if its ℓ2 norm is greater than\\nthe threshold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9,\\n100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\\nalmost eliminating the first component. If you observe that the gradients explode\\nduring training (you can track the size of the gradients using TensorBoard), you may\\nwant to try both clipping by value and clipping by norm, with different threshold,\\nand see which option performs best on the validation set.\\nReusing Pretrained Layers\\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\\nshould always try to find an existing neural network that accomplishes a similar task\\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14 ),\\nthen just reuse the lower layers of this network: this is called transfer learning . It will\\nnot only speed up training considerably, but will also require much less training data.\\nFor example, suppose that you have access to a DNN that was trained to classify pic‐\\ntures into 100 different categories, including animals, plants, vehicles, and everyday\\nobjects. Y ou now want to train a DNN to classify specific types of vehicles. These\\ntasks are very similar, even partly overlapping, so you should try to reuse parts of the\\nfirst network (see Figure 11-4 ).\\nReusing Pretrained Layers | 339', 'children': [{'id': 212, 'title': 'Transfer Learning With Keras', 'content': 'layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\\nreused layers: this will avoid wrecking their fine-tuned weights.\\nIf you still cannot get good performance, and you have little training data, try drop‐\\nping the top hidden layer(s) and freeze all remaining hidden layers again. Y ou can\\niterate until you find the right number of layers to reuse. If you have plenty of train‐\\ning data, you may try replacing the top hidden layers instead of dropping them, and\\neven add more hidden layers.\\nTransfer Learning With Keras\\nLet’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\\nfor example all classes except for sandals and shirts. Someone built and trained a\\nKeras model on that set and got reasonably good performance (>90% accuracy). Let’s\\ncall this model A. Y ou now want to tackle a different task: you have images of sandals\\nand shirts, and you want to train a binary classifier (positive=shirts, negative=san‐\\ndals). However, your dataset is quite small, you only have 200 labeled images. When\\nyou train a new model for this task (let’s call it model B), with the same architecture\\nas model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\\ntask (there are just 2 classes), you were hoping for more. While drinking your morn‐\\ning coffee, you realize that your task is quite similar to task A, so perhaps transfer\\nlearning can help? Let’s find out!\\nFirst, you need to load model A, and create a new model based on the model A ’s lay‐\\ners. Let’s reuse all layers except for the output layer:\\nmodel_A = keras.models.load_model (\"my_model_A.h5\" )\\nmodel_B_on_A  = keras.models.Sequential (model_A.layers[:-1])\\nmodel_B_on_A .add(keras.layers.Dense(1, activation =\"sigmoid\" ))\\nNote that model_A  and model_B_on_A  now share some layers. When you train\\nmodel_B_on_A , it will also affect model_A . If you want to avoid that, you need to clone\\nmodel_A  before you reuse its layers. To do this, you must clone model A ’s architecture,\\nthen copy its weights (since clone_model()  does not clone the weights):\\nmodel_A_clone  = keras.models.clone_model (model_A)\\nmodel_A_clone .set_weights (model_A.get_weights ())\\nNow we could just train model_B_on_A  for task B, but since the new output layer was\\ninitialized randomly, it will make large errors, at least during the first few epochs, so\\nthere will be large error gradients that may wreck the reused weights. To avoid this,\\none approach is to freeze the reused layers during the first few epochs, giving the new\\nlayer some time to learn reasonable weights. To do this, simply set every layer’s train\\nable  attribute to False  and compile the model:\\nfor layer in model_B_on_A .layers[:-1]:\\n    layer.trainable  = False\\nReusing Pretrained Layers | 341', 'children': []}, {'id': 213, 'title': 'Unsupervised Pretraining', 'content': 'Unsupervised Pretraining\\nSuppose you want to tackle a complex task for which you don’t have much labeled\\ntraining data, but unfortunately you cannot find a model trained on a similar task.\\nDon’t lose all hope! First, you should of course try to gather more labeled training\\ndata, but if this is too hard or too expensive, you may still be able to perform unsuper‐\\nvised pretraining  (see Figure 11-5 ). It is often rather cheap to gather unlabeled train‐\\ning examples, but quite expensive to label them. If you can gather plenty of unlabeled\\ntraining data, you can try to train the layers one by one, starting with the lowest layer\\nand then going up, using an unsupervised feature detector algorithm such as Restric‐\\nted Boltzmann Machines  (RBMs; see ???) or autoencoders (see ???). Each layer is\\ntrained on the output of the previously trained layers (all layers except the one being\\ntrained are frozen). Once all layers have been trained this way, you can add the output\\nlayer for your task, and fine-tune the final network using supervised learning (i.e.,\\nwith the labeled training examples). At this point, you can unfreeze all the pretrained\\nlayers, or just some of the upper ones.\\nFigure 11-5. Unsupervised pretraining\\nThis is a rather long and tedious process, but it often works well; in fact, it is this\\ntechnique that Geoffrey Hinton and his team used in 2006 and which led to the\\nrevival of neural networks and the success of Deep Learning. Until 2010, unsuper‐\\nvised pretraining (typically using RBMs) was the norm for deep nets, and it was only\\nafter the vanishing gradients problem was alleviated that it became much more com‐\\nReusing Pretrained Layers | 343', 'children': []}, {'id': 214, 'title': 'Pretraining on an Auxiliary Task', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 215, 'title': 'Faster Optimizers', 'content': 'mon to train DNNs purely using supervised learning. However, unsupervised pre‐\\ntraining (today typically using autoencoders rather than RBMs) is still a good option\\nwhen you have a complex task to solve, no similar model you can reuse, and little\\nlabeled training data but plenty of unlabeled training data.\\nPretraining on an Auxiliary Task\\nIf you do not have much labeled training data, one last option is to train a first neural\\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\\ntraining data, then reuse the lower layers of that network for your actual task. The\\nfirst neural network’s lower layers will learn feature detectors that will likely be reusa‐\\nble by the second neural network.\\nFor example, if you want to build a system to recognize faces, you may only have a\\nfew pictures of each individual—clearly not enough to train a good classifier. Gather‐\\ning hundreds of pictures of each person would not be practical. However, you could\\ngather a lot of pictures of random people on the web and train a first neural network\\nto detect whether or not two different pictures feature the same person. Such a net‐\\nwork would learn good feature detectors for faces, so reusing its lower layers would\\nallow you to train a good face classifier using little training data.\\nFor natural language processing  (NLP) applications, you can easily download millions\\nof text documents and automatically generate labeled data from it. For example, you\\ncould randomly mask out some words and train a model to predict what the missing\\nwords are (e.g., it should predict that the missing word in the sentence “What ___\\nyou saying?” is probably “are” or “were”). If you can train a model to reach good per‐\\nformance on this task, then it will already know quite a lot about language, and you\\ncan certainly reuse it for your actual task, and fine-tune it on your labeled data (we\\nwill discuss more pretraining tasks in ???).\\nSelf-supervised learning  is when you automatically generate the\\nlabels from the data itself, then you train a model on the resulting\\n“labeled” dataset using supervised learning techniques. Since this\\napproach requires no human labeling whatsoever, it is best classi‐\\nfied as a form of unsupervised learning.\\nFaster Optimizers\\nTraining a very large deep neural network can be painfully slow. So far we have seen\\nfour ways to speed up training (and reach a better solution): applying a good initiali‐\\nzation strategy for the connection weights, using a good activation function, using\\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\\n344 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 216, 'title': 'Momentum Optimization', 'content': '12“Some methods of speeding up the convergence of iteration methods, ” B. Polyak (1964).we will present the most popular ones: Momentum optimization, Nesterov Acceler‐\\nated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\\nMomentum Optimization\\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\\nMomentum optimization , proposed by Boris Polyak in 1964 .12 In contrast, regular\\nGradient Descent will simply take small regular steps down the slope, so it will take\\nmuch more time to reach the bottom.\\nRecall that Gradient Descent simply updates the weights θ by directly subtracting the\\ngradient of the cost function J(θ) with regards to the weights ( ∇θJ(θ)) multiplied by\\nthe learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\\nearlier gradients were. If the local gradient is tiny, it goes very slowly.\\nMomentum optimization cares a great deal about what previous gradients were: at\\neach iteration, it subtracts the local gradient from the momentum vector  m (multi‐\\nplied by the learning rate η), and it updates the weights by simply adding this\\nmomentum vector (see Equation 11-4 ). In other words, the gradient is used for accel‐\\neration, not for speed. To simulate some sort of friction mechanism and prevent the\\nmomentum from growing too large, the algorithm introduces a new hyperparameter\\nβ, simply called the momentum , which must be set between 0 (high friction) and 1\\n(no friction). A typical momentum value is 0.9.\\nEquation 11-4. Momentum algorithm\\n1 . m βm−η∇θJθ\\n2 . θ θ+m\\nY ou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\\nlearning rate η multiplied by 1\\n1 −β (ignoring the sign). For example, if β = 0.9, then the\\nterminal velocity is equal to 10 times the gradient times the learning rate, so Momen‐\\ntum optimization ends up going 10 times faster than Gradient Descent! This allows\\nMomentum optimization to escape from plateaus much faster than Gradient Descent.\\nIn particular, we saw in Chapter 4  that when the inputs have very different scales the \\ncost function will look like an elongated bowl (see Figure 4-7 ). Gradient Descent goes\\ndown the steep slope quite fast, but then it takes a very long time to go down the val‐\\nFaster Optimizers | 345', 'children': []}, {'id': 217, 'title': 'Nesterov Accelerated Gradient', 'content': '13“ A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence O(1/k2), ” Yurii\\nNesterov (1983).\\nley. In contrast, Momentum optimization will roll down the valley faster and faster\\nuntil it reaches the bottom (the optimum). In deep neural networks that don’t use\\nBatch Normalization, the upper layers will often end up having inputs with very dif‐\\nferent scales, so using Momentum optimization helps a lot. It can also help roll past\\nlocal optima.\\nDue to the momentum, the optimizer may overshoot a bit, then\\ncome back, overshoot again, and oscillate like this many times\\nbefore stabilizing at the minimum. This is one of the reasons why it\\nis good to have a bit of friction in the system: it gets rid of these\\noscillations and thus speeds up convergence.\\nImplementing Momentum optimization in Keras is a no-brainer: just use the SGD\\noptimizer and set its momentum  hyperparameter, then lie back and profit!\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9)\\nThe one drawback of Momentum optimization is that it adds yet another hyperpara‐\\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\\nand almost always goes faster than regular Gradient Descent.\\nNesterov Accelerated Gradient\\nOne small variant to Momentum optimization, proposed by Yurii Nesterov in 1983 ,13\\nis almost always faster than vanilla Momentum optimization. The idea of Nesterov\\nMomentum optimization , or Nesterov Accelerated Gradient  (NAG), is to measure the\\ngradient of the cost function not at the local position but slightly ahead in the direc‐\\ntion of the momentum (see Equation 11-5 ). The only difference from vanilla\\nMomentum optimization is that the gradient is measured at θ + βm rather than at θ.\\nEquation 11-5. Nesterov Accelerated Gradient algorithm\\n1 . m βm−η∇θJθ+βm\\n2 . θ θ+m\\nThis small tweak works because in general the momentum vector will be pointing in\\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\\nuse the gradient measured a bit farther in that direction rather than using the gradi‐\\nent at the original position, as you can see in Figure 11-6  (where ∇1 represents the\\ngradient of the cost function measured at the starting point θ, and ∇2 represents the\\n346 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 218, 'title': 'AdaGrad', 'content': '14“ Adaptive Subgradient Methods for Online Learning and Stochastic Optimization, ” J. Duchi et al. (2011).gradient at the point located at θ + βm). As you can see, the Nesterov update ends up\\nslightly closer to the optimum. After a while, these small improvements add up and\\nNAG ends up being significantly faster than regular Momentum optimization. More‐\\nover, note that when the momentum pushes the weights across a valley, ∇1 continues\\nto push further across the valley, while ∇2 pushes back toward the bottom of the val‐\\nley. This helps reduce oscillations and thus converges faster.\\nNAG will almost always speed up training compared to regular Momentum optimi‐\\nzation. To use it, simply set nesterov=True  when creating the SGD optimizer:\\noptimizer  = keras.optimizers .SGD(lr=0.001, momentum =0.9, nesterov =True)\\nFigure 11-6. Regular versus Nesterov Momentum optimization\\nAdaGrad\\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\\ndown the steepest slope, then slowly goes down the bottom of the valley. It would be\\nnice if the algorithm could detect this early on and correct its direction to point a bit\\nmore toward the global optimum.\\nThe AdaGrad  algorithm14 achieves this by scaling down the gradient vector along the\\nsteepest dimensions (see Equation 11-6 ):\\nEquation 11-6. AdaGrad algorithm\\n1 . s s+∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nFaster Optimizers | 347', 'children': []}, {'id': 219, 'title': 'RMSProp', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 220, 'title': 'Adam and Nadam Optimization', 'content': '15This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012, and presented by Geoffrey\\nHinton in his Coursera class on neural networks (slides: https://homl.info/57 ; video: https://homl.info/58 ).\\nAmusingly, since the authors did not write a paper to describe it, researchers often cite “slide 29 in lecture 6”\\nin their papers.\\n16“ Adam: A Method for Stochastic Optimization, ” D. Kingma, J. Ba (2015).\\n17These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\\nfirst moment , while the variance is often called the second moment , hence the name of the algorithm.RMSProp\\nAlthough AdaGrad slows down a bit too fast and ends up never converging to the\\nglobal optimum, the RMSProp  algorithm15 fixes this by accumulating only the gradi‐\\nents from the most recent iterations (as opposed to all the gradients since the begin‐\\nning of training). It does so by using exponential decay in the first step (see Equation\\n11-7 ).\\nEquation 11-7. RMSProp algorithm\\n1 . s βs+1 −β∇θJθ⊗∇θJθ\\n2 . θ θ−η∇θJθ⊘s+?\\nThe decay rate β is typically set to 0.9. Y es, it is once again a new hyperparameter, but\\nthis default value often works well, so you may not need to tune it at all.\\nAs you might expect, Keras has an RMSProp  optimizer:\\noptimizer  = keras.optimizers .RMSprop(lr=0.001, rho=0.9)\\nExcept on very simple problems, this optimizer almost always performs much better\\nthan AdaGrad. In fact, it was the preferred optimization algorithm of many research‐\\ners until Adam optimization came around.\\nAdam and Nadam Optimization\\nAdam ,16 which stands for adaptive moment estimation , combines the ideas of Momen‐\\ntum optimization and RMSProp: just like Momentum optimization it keeps track of\\nan exponentially decaying average of past gradients, and just like RMSProp it keeps\\ntrack of an exponentially decaying average of past squared gradients (see Equation\\n11-8 ).17\\nFaster Optimizers | 349', 'children': []}, {'id': 221, 'title': 'Learning Rate Scheduling', 'content': '20“Primal-Dual Subgradient Methods for Convex Problems, ” Yurii Nesterov (2005).\\n21“ Ad Click Prediction: a View from the Trenches, ” H. McMahan et al. (2013).often don’t even fit in memory, and even when they do, computing the Hessians is \\njust too slow.\\nTraining Sparse Models\\nAll the optimization algorithms just presented produce dense models, meaning that\\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\\nyou need it to take up less memory, you may prefer to end up with a sparse model\\ninstead.\\nOne trivial way to achieve this is to train the model as usual, then get rid of the tiny\\nweights (set them to 0). However, this will typically not lead to a very sparse model,\\nand it may degrade the model’s performance.\\nA better option is to apply strong ℓ1 regularization during training, as it pushes the\\noptimizer to zero out as many weights as it can (as discussed in Chapter 4  about Lasso\\nRegression).\\nHowever, in some cases these techniques may remain insufficient. One last option is\\nto apply Dual Averaging , often called Follow The Regularized Leader  (FTRL), a techni‐\\nque proposed by Yurii Nesterov .20 When used with ℓ1 regularization, this technique\\noften leads to very sparse models. Keras implements a variant of FTRL called FTRL-\\nProximal21 in the FTRL  optimizer.\\nLearning Rate Scheduling\\nFinding a good learning rate can be tricky. If you set it way too high, training may\\nactually diverge (as we discussed in Chapter 4 ). If you set it too low, training will\\neventually converge to the optimum, but it will take a very long time. If you set it\\nslightly too high, it will make progress very quickly at first, but it will end up dancing\\naround the optimum, never really settling down. If you have a limited computing\\nbudget, you may have to interrupt training before it has converged properly, yielding\\na suboptimal solution (see Figure 11-8 ).\\n352 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 222, 'title': 'Avoiding Overfitting Through Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': [{'id': 223, 'title': 'l1 and l2 Regularization', 'content': 'ers.schedules , then pass this learning rate to any optimizer. This approach updates\\nthe learning rate at each step rather than at each epoch. For example, here is how to\\nimplement the same exponential schedule as earlier:\\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\\nlearning_rate  = keras.optimizers .schedules .ExponentialDecay (0.01, s, 0.1)\\noptimizer  = keras.optimizers .SGD(learning_rate )\\nThis is nice and simple, plus when you save the model, the learning rate and its\\nschedule (including its state) get saved as well. However, this approach is not part of\\nthe Keras API, it is specific to tf.keras.\\nTo sum up, exponential decay or performance scheduling can considerably speed up\\nconvergence, so give them a try!\\nAvoiding Overfitting  Through Regularization\\nWith four parameters I can fit an elephant and with five I can make him wiggle his\\ntrunk.\\n—John von Neumann, cited by Enrico Fermi in Nature 427\\nWith thousands of parameters you can fit the whole zoo. Deep neural networks typi‐\\ncally have tens of thousands of parameters, sometimes even millions. With so many\\nparameters, the network has an incredible amount of freedom and can fit a huge vari‐\\nety of complex datasets. But this great flexibility also means that it is prone to overfit‐\\nting the training set. We need regularization.\\nWe already implemented one of the best regularization techniques in Chapter 10 :\\nearly stopping. Moreover, even though Batch Normalization was designed to solve\\nthe vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\\nIn this section we will present other popular regularization techniques for neural net‐\\nworks: ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\\nℓ1 and ℓ2 Regularization\\nJust like you did in Chapter 4  for simple linear models, you can use ℓ1 and ℓ2 regulari‐\\nzation to constrain a neural network’s connection weights (but typically not its bia‐\\nses). Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\\nusing a regularization factor of 0.01:\\nlayer = keras.layers.Dense(100, activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nThe l2()  function returns a regularizer that will be called to compute the regulariza‐\\ntion loss, at each step during training. This regularization loss is then added to the\\nfinal loss. As you might expect, you can just use keras.regularizers.l1()  if you\\n356 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 224, 'title': 'Dropout', 'content': '23“Improving neural networks by preventing co-adaptation of feature detectors, ” G. Hinton et al. (2012).\\n24“Dropout: A Simple Way to Prevent Neural Networks from Overfitting, ” N. Srivastava et al. (2014).want ℓ1 regularization, and if you want both ℓ1 and ℓ2 regularization, use keras.regu\\nlarizers.l1_l2()  (specifying both regularization factors).\\nSince you will typically want to apply the same regularizer to all layers in your net‐\\nwork, as well as the same activation function and the same initialization strategy in all\\nhidden layers, you may find yourself repeating the same arguments over and over.\\nThis makes it ugly and error-prone. To avoid this, you can try refactoring your code\\nto use loops. Another option is to use Python’s functools.partial()  function: it lets\\nyou create a thin wrapper for any callable, with some default argument values. For\\nexample:\\nfrom functools  import partial\\nRegularizedDense  = partial(keras.layers.Dense,\\n                           activation =\"elu\",\\n                           kernel_initializer =\"he_normal\" ,\\n                           kernel_regularizer =keras.regularizers .l2(0.01))\\nmodel = keras.models.Sequential ([\\n    keras.layers.Flatten(input_shape =[28, 28]),\\n    RegularizedDense (300),\\n    RegularizedDense (100),\\n    RegularizedDense (10, activation =\"softmax\" ,\\n                     kernel_initializer =\"glorot_uniform\" )\\n])\\nDropout\\nDropout  is one of the most popular regularization techniques for deep neural net‐\\nworks. It was proposed23 by Geoffrey Hinton in 2012 and further detailed in a paper24\\nby Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-\\nthe-art neural networks got a 1–2% accuracy boost simply by adding dropout. This\\nmay not sound like a lot, but when a model already has 95% accuracy, getting a 2%\\naccuracy boost means dropping the error rate by almost 40% (going from 5% error to\\nroughly 3%).\\nIt is a fairly simple algorithm: at every training step, every neuron (including the\\ninput neurons, but always excluding the output neurons) has a probability p of being\\ntemporarily “dropped out, ” meaning it will be entirely ignored during this training\\nstep, but it may be active during the next step (see Figure 11-9 ). The hyperparameter\\np is called the dropout rate , and it is typically set to 50%. After training, neurons don’t\\nget dropped anymore. And that’s all (except for a technical detail we will discuss\\nmomentarily).\\nAvoiding Overfitting  Through Regularization | 357', 'children': []}, {'id': 225, 'title': 'Monte-Carlo (MC) Dropout', 'content': '26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\\nGhahramani (2016).\\n27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\\ninference in a specific type of probabilistic model called a deep Gaussian Process .\\nIf you want to regularize a self-normalizing network based on the\\nSELU activation function (as discussed earlier), you should use\\nAlphaDropout : this is a variant of dropout that preserves the mean\\nand standard deviation of its inputs (it was introduced in the same\\npaper as SELU, as regular dropout would break self-normalization).\\nMonte-Carlo (MC) Dropout\\nIn 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\\nuse dropout:\\n•First, the paper establishes a profound connection between dropout networks\\n(i.e., neural networks containing a dropout layer before every weight layer) and\\napproximate Bayesian inference27, giving dropout a solid mathematical justifica‐\\ntion.\\n•Second, they introduce a powerful technique called MC Dropout , which can\\nboost the performance of any trained dropout model, without having to retrain it\\nor even modify it at all!\\n•Moreover, MC Dropout also provides a much better measure of the model’s\\nuncertainty.\\n•Finally, it is also amazingly simple to implement. If this all sounds like a “one\\nweird trick” advertisement, then take a look at the following code. It is the full\\nimplementation of MC Dropout , boosting the dropout model we trained earlier,\\nwithout retraining it:\\nwith keras.backend.learning_phase_scope (1): # force training mode = dropout on\\n    y_probas  = np.stack([model.predict(X_test_scaled )\\n                         for sample in range(100)])\\ny_proba = y_probas .mean(axis=0)\\nWe first force training mode on, using a learning_phase_scope(1)  context. This\\nturns dropout on within the with  block. Then we make 100 predictions over the test\\nset, and we stack them. Since dropout is on, all predictions will be different. Recall\\nthat predict()  returns a matrix with one row per instance, and one column per class.\\nSince there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\\n[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\\n10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\\nshape [10000, 10], like we would get with a single prediction. That’s all! Averaging\\n360 | Chapter 11: Training Deep Neural Networks', 'children': []}, {'id': 226, 'title': 'Max-Norm Regularization', 'content': 'The number of Monte Carlo samples you use (100 in this example)\\nis a hyperparameter you can tweak. The higher it is, the more accu‐\\nrate the predictions and their uncertainty estimates will be. How‐\\never, it you double it, inference time will also be doubled.\\nMoreover, above a certain number of samples, you will notice little\\nimprovement. So your job is to find the right tradeoff between\\nlatency and accuracy, depending on your application.\\nIf your model contains other layers that behave in a special way during training (such\\nas Batch Normalization layers), then you should not force training mode like we just\\ndid. Instead, you should replace the Dropout  layers with the following MCDropout\\nclass:\\nclass MCDropout (keras.layers.Dropout):\\n    def call(self, inputs):\\n        return super().call(inputs, training =True)\\nWe just sublass the Dropout  layer and override the call()  method to force its train\\ning argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\\nout class by subclassing AlphaDropout  instead. If you are creating a model from\\nscratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\\nmodel that was already trained using Dropout , you need to create a new model, iden‐\\ntical to the existing model except replacing the Dropout  layers with MCDropout , then\\ncopy the existing model’s weights to your new model.\\nIn short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\\nvides better uncertainty estimates. And of course, since it is just regular dropout dur‐\\ning training, it also acts like a regularizer.\\nMax-Norm Regularization\\nAnother regularization technique that is quite popular for neural networks is called\\nmax-norm regularization : for each neuron, it constrains the weights w of the incom‐\\ning connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\\nand ∥ · ∥2 is the ℓ2 norm.\\nMax-norm regularization does not add a regularization loss term to the overall loss\\nfunction. Instead, it is typically implemented by computing ∥w∥2 after each training\\nstep and clipping w if needed ( w wr\\n∥w∥2).\\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\\nnorm regularization can also help alleviate the vanishing/exploding gradients prob‐\\nlems (if you are not using Batch Normalization).\\n362 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 227, 'title': 'Summary and Practical Guidelines', 'content': 'To implement max-norm regularization in Keras, just set every hidden layer’s ker\\nnel_constraint  argument to a max_norm()  constraint, with the appropriate max\\nvalue, for example:\\nkeras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\\n                   kernel_constraint =keras.constraints .max_norm (1.))\\nAfter each training iteration, the model’s fit()  method will call the object returned\\nby max_norm() , passing it the layer’s weights and getting clipped weights in return,\\nwhich then replace the layer’s weights. As we will see in Chapter 12 , you can define\\nyour own custom constraint function if you ever need to, and use it as the ker\\nnel_constraint . Y ou can also constrain the bias terms by setting the bias_con\\nstraint  argument.\\nThe max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\\nally has weights of shape [number of inputs, number of neurons], so using axis=0\\nmeans that the max norm constraint will apply independently to each neuron’s weight\\nvector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\\nmake sure to set the max_norm()  constraint’s axis  argument appropriately (usually\\naxis=[0, 1, 2] ).\\nSummary and Practical Guidelines\\nIn this chapter, we have covered a wide range of techniques and you may be wonder‐\\ning which ones you should use. The configuration in Table 11-2  will work fine in\\nmost cases, without requiring much hyperparameter tuning.\\nTable 11-2. Default DNN configuration\\nHyperparameter Default value\\nKernel initializer: LeCun initialization\\nActivation function: SELU\\nNormalization: None (self-normalization)\\nRegularization: Early stopping\\nOptimizer: Nadam\\nLearning rate schedule: Performance scheduling\\nDon’t forget to standardize the input features! Of course, you should also try to reuse\\nparts of a pretrained neural network if you can find one that solves a similar problem,\\nor use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\\nan auxiliary task if you have a lot of labeled data for a similar task.\\nThe default configuration in Table 11-2  may need to be tweaked:\\nSummary and Practical Guidelines | 363', 'children': []}, {'id': 228, 'title': 'Exercises', 'content': '•If your model self-normalizes:\\n—If it overfits the training set, then you should add alpha dropout (and always\\nuse early stopping as well). Do not use other regularization methods, or else\\nthey would break self-normalization.\\n•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\\nconnections):\\n—Y ou can try using ELU (or another activation function) instead of SELU, it\\nmay perform better. Make sure to change the initialization method accord‐\\ningly (e.g., He init for ELU or ReLU).\\n—If it is a deep network, you should use Batch Normalization after every hidden\\nlayer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\\nularization.\\n•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\\nthe tiny weights after training). If you need an even sparser model, you can try\\nusing FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\\ncase, this will break self-normalization, so you will need to switch to BN if your\\nmodel is deep.\\n•If you need a low-latency model (one that performs lightning-fast predictions),\\nyou may need to use less layers, avoid Batch Normalization, and possibly replace\\nthe SELU activation function with the leaky ReLU. Having a sparse model will\\nalso help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\\n(or even 8-bits) (see ???).\\n•If you are building a risk-sensitive application, or inference latency is not very\\nimportant in your application, you can use MC Dropout to boost performance\\nand get more reliable probability estimates, along with uncertainty estimates.\\nWith these guidelines, you are now ready to train very deep nets! I hope you are now\\nconvinced that you can go a very long way using just Keras. However, there may\\ncome a time when you need to have even more control, for example to write a custom\\nloss function or to tweak the training algorithm. For such cases, you will need to use\\nTensorFlow’s lower-level API, as we will see in the next chapter.\\nExercises\\n1.Is it okay to initialize all the weights to the same value as long as that value is\\nselected randomly using He initialization?\\n2.Is it okay to initialize the bias terms to 0?\\n3.Name three advantages of the SELU activation function over ReLU.\\n364 | Chapter 11: Training Deep Neural Networks', 'children': []}]}, {'id': 229, 'title': 'Chapter 12. Custom Models and Training with TensorFlow', 'content': 'CHAPTER 12\\nCustom Models and Training with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 12 in the final\\nrelease of the book.\\nSo far we have used only TensorFlow’s high level API, tf.keras, but it already got us\\npretty far: we built various neural network architectures, including regression and\\nclassification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\\nniques, such as Batch Normalization, dropout, learning rate schedules, and more. In\\nfact, 95% of the use cases you will encounter will not require anything else than\\ntf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\\nand take a look at its lower-level Python API . This will be useful when you need extra\\ncontrol, to write custom loss functions, custom metrics, layers, models, initializers,\\nregularizers, weight constraints and more. Y ou may even need to fully control the\\ntraining loop itself, for example to apply special transformations or constraints to the\\ngradients (beyond just clipping them), or to use multiple optimizers for different\\nparts of the network. We will cover all these cases in this chapter, then we will also\\nlook at how you can boost your custom models and training algorithms using Ten‐\\nsorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\\nsorFlow.\\n367', 'children': [{'id': 230, 'title': 'A Quick Tour of TensorFlow', 'content': '1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\\nto use tf.keras instead.\\nTensorFlow 2.0 was released in March 2019, making TensorFlow\\nmuch easier to use. The first edition of this book used TF 1, while\\nthis edition uses TF 2.\\nA Quick Tour of TensorFlow\\nAs you know, TensorFlow  is a powerful library for numerical computation, particu‐\\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\\nit for anything else that requires heavy computations). It was developed by the Google\\nBrain team and it powers many of Google’s large-scale services, such as Google Cloud\\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\\nand it is now the most popular deep learning library (in terms of citations in papers,\\nadoption in companies, stars on github, etc.): countless projects use TensorFlow for\\nall sorts of Machine Learning tasks, such as image classification, natural language\\nprocessing (NLP), recommender systems, time series forecasting, and much more.\\nSo what does TensorFlow actually offer? Here’s a summary:\\n•Its core is very similar to NumPy, but with GPU support.\\n•It also supports distributed computing (across multiple devices and servers).\\n•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\\ntations for speed and memory usage: it works by extracting the computation\\ngraph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\\nand finally running it efficiently (e.g., by automatically running independent\\noperations in parallel).\\n•Computation graphs can be exported to a portable format, so you can train a\\nTensorFlow model in one environment (e.g., using Python on Linux), and run it\\nin another (e.g., using Java on an Android device).\\n•It implements autodiff (see Chapter 10  and ???), and provides some excellent\\noptimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\\neasily minimize all sorts of loss functions.\\n•TensorFlow offers many more features, built on top of these core features: the\\nmost important is of course tf.keras1, but it also has data loading & preprocessing\\nops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\\n(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\\nAPI).\\n368 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 231, 'title': 'Using TensorFlow like NumPy', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': [{'id': 232, 'title': 'Tensors and Operations', 'content': 'Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\\nopers, and a large community contributing to improving it. To ask technical ques‐\\ntions, you should use http://stackoverflow.com/  and tag your question with tensorflow\\nand python . Y ou can file bugs and feature requests through GitHub. For general dis‐\\ncussions, join the Google group .\\nOkay, it’s time to start coding!\\nUsing TensorFlow like NumPy\\nTensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\\nusually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\\na scalar (a simple value, such as 42). These tensors will be important when we create\\ncustom cost functions, custom metrics, custom layers and more, so let’s see how to\\ncreate and manipulate them.\\nTensors and Operations\\nY ou can easily create a tensor, using tf.constant() . For example, here is a tensor\\nrepresenting a matrix with two rows and three columns of floats:\\n>>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\n>>> tf.constant (42) # scalar\\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\\nJust like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\\n>>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\\n>>> t.shape\\nTensorShape([2, 3])\\n>>> t.dtype\\ntf.float32\\nIndexing works much like in NumPy:\\n>>> t[:, 1:]\\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\\narray([[2., 3.],\\n       [5., 6.]], dtype=float32)>\\n>>> t[..., 1, tf.newaxis]\\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\\narray([[2.],\\n       [5.]], dtype=float32)>\\nMost importantly, all sorts of tensor operations are available:\\n>>> t + 10\\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\\nUsing TensorFlow like NumPy | 371', 'children': []}, {'id': 233, 'title': 'Tensors and NumPy', 'content': '4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\\nconfused with logging).\\nMany functions and classes have aliases. For example, tf.add()\\nand tf.math.add()  are the same function. This allows TensorFlow\\nto have concise names for the most common operations4, while\\npreserving well organized packages.\\nKeras’ Low-Level API\\nThe Keras API actually has its own low-level API, located in keras.backend . It\\nincludes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\\ntions generally just call the corresponding TensorFlow operations. If you want to\\nwrite code that will be portable to other Keras implementations, you should use these\\nKeras functions. However, they only cover a subset of all functions available in Ten‐\\nsorFlow, so in this book we will use the TensorFlow operations directly. Here is as\\nsimple example using keras.backend , which is commonly named K for short:\\n>>> from tensorflow  import keras\\n>>> K = keras.backend\\n>>> K.square(K.transpose (t)) + 10\\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\\narray([[11., 26.],\\n       [14., 35.],\\n       [19., 46.]], dtype=float32)>\\nTensors and NumPy\\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\\nversa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\\noperations to tensors:\\n>>> a = np.array([2., 4., 5.])\\n>>> tf.constant (a)\\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\\n>>> t.numpy() # or np.array(t)\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)\\n>>> tf.square(a)\\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\\n>>> np.square(t)\\narray([[ 1.,  4.,  9.],\\n       [16., 25., 36.]], dtype=float32)\\nUsing TensorFlow like NumPy | 373', 'children': []}, {'id': 234, 'title': 'Type Conversions', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 235, 'title': 'Variables', 'content': \"Notice that NumPy uses 64-bit precision by default, while Tensor‐\\nFlow uses 32-bit. This is because 32-bit precision is generally more\\nthan enough for neural networks, plus it runs faster and uses less\\nRAM. So when you create a tensor from a NumPy array, make sure\\nto set dtype=tf.float32 .\\nType Conversions\\nType conversions can significantly hurt performance, and they can easily go unno‐\\nticed when they are done automatically. To avoid this, TensorFlow does not perform\\nany type conversions automatically: it just raises an exception if you try to execute an\\noperation on tensors with incompatible types. For example, you cannot add a float\\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\\n>>> tf.constant (2.) + tf.constant (40)\\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\\n>>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\\nThis may be a bit annoying at first, but remember that it’s for a good cause! And of\\ncourse you can use tf.cast()  when you really need to convert types:\\n>>> t2 = tf.constant (40., dtype=tf.float64)\\n>>> tf.constant (2.0) + tf.cast(t2, tf.float32)\\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\\nVariables\\nSo far, we have used constant tensors: as their name suggests, you cannot modify\\nthem. However, the weights in a neural network need to be tweaked by backpropaga‐\\ntion, and other parameters may also need to change over time (e.g., a momentum\\noptimizer keeps track of past gradients). What we need is a tf.Variable :\\n>>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\\n>>> v\\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\\narray([[1., 2., 3.],\\n       [4., 5., 6.]], dtype=float32)>\\nA tf.Variable  acts much like a constant tensor: you can perform the same opera‐\\ntions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\\nit can also be modified in place using the assign()  method (or assign_add()  or\\nassign_sub()  which increment or decrement the variable by the given value). Y ou\\ncan also modify individual cells (or slices), using the cell’s (or slice’s) assign()\\nmethod (direct item assignment will not work), or using the scatter_update()  or\\nscatter_nd_update()  methods:\\nv.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\\nv[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\\n374 | Chapter 12: Custom Models and Training with TensorFlow\", 'children': []}, {'id': 236, 'title': 'Other Data Structures', 'content': 'v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\\nv.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\\n                          # => [[100., 42., 0.], [8., 10., 200.]]\\nIn practice you will rarely have to create variables manually, since\\nKeras provides an add_weight()  method that will take care of it for\\nyou, as we will see. Moreover, model parameters will generally be\\nupdated directly by the optimizers, so you will rarely need to\\nupdate variables manually.\\nOther Data Structures\\nTensorFlow supports several other data structures, including the following (please see\\nthe notebook or ??? for more details):\\n•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\\n0s. The tf.sparse  package contains operations for sparse tensors.\\n•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\\ndefault, but can optionally be made dynamic. All tensors they contain must have\\nthe same shape and data type.\\n•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\\nevery tensor has the same shape and data type. The tf.ragged  package contains\\noperations for ragged tensors.\\n•String tensors  are regular tensors of type tf.string . These actually represent byte\\nstrings, not Unicode strings, so if you create a string tensor using a Unicode\\nstring (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\\nUTF-8 automatically (e.g., b\"caf\\\\xc3\\\\xa9\" ). Alternatively, you can represent\\nUnicode strings using tensors of type tf.int32 , where each item represents a\\nUnicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\\nan s) contains ops for byte strings and Unicode strings (and to convert one into\\nthe other).\\n•Sets are just represented as regular tensors (or sparse tensors) containing one or\\nmore sets, and you can manipulate them using operations from the tf.sets\\npackage.\\n•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\\nprioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\\nShuffleQueue ), and queues that can batch items of different shapes by padding\\n(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\\nWith tensors, operations, variables and various data structures at your disposal, you\\nare now ready to customize your models and training algorithms!\\nUsing TensorFlow like NumPy | 375', 'children': []}]}, {'id': 237, 'title': 'Customizing Models and Training Algorithms', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 238, 'title': 'Custom Loss Functions', 'content': 'Customizing Models and Training Algorithms\\nLet’s start by creating a custom loss function, which is a simple and common use case.\\nCustom Loss Functions\\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\\nbut it turns out to be insufficient, the dataset is still noisy. Which loss function should\\nyou use? The mean squared error might penalize large errors too much, so your\\nmodel will end up being imprecise. The mean absolute error would not penalize out‐\\nliers as much, but training might take a while to converge and the trained model\\nmight not be very precise. This is probably a good time to use the Huber loss (intro‐\\nduced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\\nkeras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\\npie! Just create a function that takes the labels and predictions as arguments, and use\\nTensorFlow operations to compute every instance’s loss:\\ndef huber_fn (y_true, y_pred):\\n    error = y_true - y_pred\\n    is_small_error  = tf.abs(error) < 1\\n    squared_loss  = tf.square(error) / 2\\n    linear_loss   = tf.abs(error) - 0.5\\n    return tf.where(is_small_error , squared_loss , linear_loss )\\nFor better performance, you should use a vectorized implementa‐\\ntion, as in this example. Moreover, if you want to benefit from Ten‐\\nsorFlow’s graph features, you should use only TensorFlow\\noperations.\\nIt is also preferable to return a tensor containing one loss per instance, rather than\\nreturning the mean loss. This way, Keras can apply class weights or sample weights\\nwhen requested (see Chapter 10 ).\\nNext, you can just use this loss when you compile the Keras model, then train your\\nmodel:\\nmodel.compile(loss=huber_fn , optimizer =\"nadam\")\\nmodel.fit(X_train, y_train, [...])\\nAnd that’s it! For each batch during training, Keras will call the huber_fn()  function\\nto compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\\nkeep track of the total loss since the beginning of the epoch, and it will display the\\nmean loss.\\n376 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 239, 'title': 'Saving and Loading Models That Contain Custom Components', 'content': 'But what happens to this custom loss when we save the model?\\nSaving and Loading Models That Contain Custom Components\\nSaving a model containing a custom loss function actually works fine, as Keras just\\nsaves the name of the function. However, whenever you load it, you need to provide a\\ndictionary that maps the function name to the actual function. More generally, when\\nyou load a model containing custom objects, you need to map the names to the\\nobjects:\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\\n                                custom_objects ={\"huber_fn\" : huber_fn })\\nWith the current implementation, any error between -1 and 1 is considered “small” .\\nBut what if we want a different threshold? One solution is to create a function that\\ncreates a configured loss function:\\ndef create_huber (threshold =1.0):\\n    def huber_fn (y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    return huber_fn\\nmodel.compile(loss=create_huber (2.0), optimizer =\"nadam\")\\nUnfortunately, when you save the model, the threshold  will not be saved. This means\\nthat you will have to specify the threshold  value when loading the model (note that\\nthe name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\\nthe name of the function that created it):\\nmodel = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\\n                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\\nY ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\\nment its get_config()  method:\\nclass HuberLoss (keras.losses.Loss):\\n    def __init__ (self, threshold =1.0, **kwargs):\\n        self.threshold  = threshold\\n        super().__init__ (**kwargs)\\n    def call(self, y_true, y_pred):\\n        error = y_true - y_pred\\n        is_small_error  = tf.abs(error) < self.threshold\\n        squared_loss  = tf.square(error) / 2\\n        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\\n        return tf.where(is_small_error , squared_loss , linear_loss )\\n    def get_config (self):\\n        base_config  = super().get_config ()\\n        return {**base_config , \"threshold\" : self.threshold }\\nCustomizing Models and Training Algorithms | 377', 'children': []}, {'id': 240, 'title': 'Custom Activation Functions, Initializers, Regularizers, and Constraints', 'content': 'Custom Activation Functions, Initializers, Regularizers, and\\nConstraints\\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\\nrics, activation functions, layers and even full models can be customized in very much\\nthe same way. Most of the time, you will just need to write a simple function, with the\\nappropriate inputs and outputs. For example, here are examples of a custom activa‐\\ntion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\\ntom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\\nstraint that ensures weights are all positive (equivalent to\\nkeras.constraints.nonneg()  or tf.nn.relu ):\\ndef my_softplus (z): # return value is just tf.nn.softplus(z)\\n    return tf.math.log(tf.exp(z) + 1.0)\\ndef my_glorot_initializer (shape, dtype=tf.float32):\\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\\ndef my_l1_regularizer (weights):\\n    return tf.reduce_sum (tf.abs(0.01 * weights))\\ndef my_positive_weights (weights): # return value is just tf.nn.relu(weights)\\n    return tf.where(weights < 0., tf.zeros_like (weights), weights)\\nAs you can see, the arguments depend on the type of custom function. These custom\\nfunctions can then be used normally, for example:\\nlayer = keras.layers.Dense(30, activation =my_softplus ,\\n                           kernel_initializer =my_glorot_initializer ,\\n                           kernel_regularizer =my_l1_regularizer ,\\n                           kernel_constraint =my_positive_weights )\\nThe activation function will be applied to the output of this Dense  layer, and its result\\nwill be passed on to the next layer. The layer’s weights will be initialized using the\\nvalue returned by the initializer. At each training step the weights will be passed to the\\nregularization function to compute the regularization loss, which will be added to the\\nmain loss to get the final loss used for training. Finally, the constraint function will be\\ncalled after each training step, and the layer’s weights will be replaced by the con‐\\nstrained weights.\\nIf a function has some hyperparameters that need to be saved along with the model,\\nthen you will want to subclass the appropriate class, such as keras.regulariz\\ners.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\\nizer  or keras.layers.Layer  (for any layer, including activation functions). For\\nexample, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\\nCustomizing Models and Training Algorithms | 379', 'children': []}, {'id': 241, 'title': 'Custom Metrics', 'content': '6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\\nconstructor or the get_config()  method, as they are not defined by the parent class):\\nclass MyL1Regularizer (keras.regularizers .Regularizer ):\\n    def __init__ (self, factor):\\n        self.factor = factor\\n    def __call__ (self, weights):\\n        return tf.reduce_sum (tf.abs(self.factor * weights))\\n    def get_config (self):\\n        return {\"factor\" : self.factor}\\nNote that you must implement the call()  method for losses, layers (including activa‐\\ntion functions) and models, or the __call__()  method for regularizers, initializers\\nand constraints. For metrics, things are a bit different, as we will see now.\\nCustom Metrics\\nLosses and metrics are conceptually not the same thing: losses are used by Gradient\\nDescent to train  a model, so they must be differentiable (at least where they are evalu‐\\nated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\\neasily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\\nevaluate  a model, they must be more easily interpretable, and they can be non-\\ndifferentiable or have 0 gradients everywhere (e.g., accuracy).\\nThat said, in most cases, defining a custom metric function is exactly the same as\\ndefining a custom loss function. In fact, we could even use the Huber loss function we\\ncreated earlier as a metric6, it would work just fine (and persistence would also work\\nthe same way, in this case only saving the name of the function, \"huber_fn\" ):\\nmodel.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\\nFor each batch during training, Keras will compute this metric and keep track of its\\nmean since the beginning of the epoch. Most of the time, this is exactly what you\\nwant. But not always! Consider a binary classifier’s precision, for example. As we saw\\nin Chapter 3 , precision is the number of true positives divided by the number of posi‐\\ntive predictions (including both true positives and false positives). Suppose the model\\nmade 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\\ncision. Then suppose the model made 3 positive predictions in the second batch, but\\nthey were all incorrect: that’s 0% precision for the second batch. If you just compute\\nthe mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\\nel’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\\n0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\\nwe need is an object that can keep track of the number of true positives and the num‐\\n380 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 242, 'title': 'Custom Layers', 'content': 'Custom Layers\\nY ou may occasionally want to build an architecture that contains an exotic layer for\\nwhich TensorFlow does not provide a default implementation. In this case, you will\\nneed to create a custom layer. Or sometimes you may simply want to build a very\\nrepetitive architecture, containing identical blocks of layers repeated many times, and\\nit would be convenient to treat each block of layers as a single layer. For example, if\\nthe model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\\ndefine a custom layer D containing layers A, B, C, and your model would then simply\\nbe D, D, D. Let’s see how to build custom layers.\\nFirst, some layers have no weights, such as keras.layers.Flatten  or keras.lay\\ners.ReLU . If you want to create a custom layer without any weights, the simplest\\noption is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\\nple, the following layer will apply the exponential function to its inputs:\\nexponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\\nThis custom layer can then be used like any other layer, using the sequential API, the\\nfunctional API, or the subclassing API. Y ou can also use it as an activation function\\n(or you could just use activation=tf.exp , or activation=keras.activations.expo\\nnential , or simply activation=\"exponential\" ). The exponential layer is sometimes\\nused in the output layer of a regression model when the values to predict have very\\ndifferent scales (e.g., 0.001, 10., 1000.).\\nAs you probably guessed by now, to build a custom stateful layer (i.e., a layer with\\nweights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\\nple, the following class implements a simplified version of the Dense  layer:\\nclass MyDense(keras.layers.Layer):\\n    def __init__ (self, units, activation =None, **kwargs):\\n        super().__init__ (**kwargs)\\n        self.units = units\\n        self.activation  = keras.activations .get(activation )\\n    def build(self, batch_input_shape ):\\n        self.kernel = self.add_weight (\\n            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\\n            initializer =\"glorot_normal\" )\\n        self.bias = self.add_weight (\\n            name=\"bias\", shape=[self.units], initializer =\"zeros\")\\n        super().build(batch_input_shape ) # must be at the end\\n    def call(self, X):\\n        return self.activation (X @ self.kernel + self.bias)\\n    def compute_output_shape (self, batch_input_shape ):\\n        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\\nCustomizing Models and Training Algorithms | 383', 'children': []}, {'id': 243, 'title': 'Custom Models', 'content': '10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\\nmany other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\\nmodels.\\nCustom Models\\nWe already looked at custom model classes in Chapter 10  when we discussed the sub‐\\nclassing API.10 It is actually quite straightforward, just subclass the keras.mod\\nels.Model  class, create layers and variables in the constructor, and implement the\\ncall()  method to do whatever you want the model to do. For example, suppose you\\nwant to build the model represented in Figure 12-3 :\\nFigure 12-3. Custom Model Example\\nThe inputs go through a first dense layer, then through a residual block  composed of\\ntwo dense layers and an addition operation (as we will see in Chapter 14 , a residual\\nblock adds its inputs to its outputs), then through this same residual block 3 more\\ntimes, then through a second residual block, and the final result goes through a dense\\noutput layer. Note that this model does not make much sense, it’s just an example to\\nillustrate the fact that you can easily build any kind of model you want, even contain‐\\n386 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 244, 'title': 'Losses and Metrics Based on Model Internals', 'content': 'iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\\nmethod (which can return any of the model’s layers by name or by index), and the\\nsave()  method (and support for keras.models.load_model()  and keras.mod\\nels.clone_model() ). So if models provide more functionalities than layers, why not\\njust define every layer as a model? Well, technically you could, but it is probably\\ncleaner to distinguish the internal components of your model (layers or reusable\\nblocks of layers) from the model itself. The former should subclass the Layer  class,\\nwhile the latter should subclass the Model  class.\\nWith that, you can quite naturally and concisely build almost any model that you find\\nin a paper, either using the sequential API, the functional API, the subclassing API, or\\neven a mix of these. “ Almost” any model? Y es, there are still a couple things that we\\nneed to look at: first, how to define losses or metrics based on model internals, and\\nsecond how to build a custom training loop.\\nLosses and Metrics Based on Model Internals\\nThe custom losses and metrics we defined earlier were all based on the labels and the\\npredictions (and optionally sample weights). However, you will occasionally want to\\ndefine losses based on other parts of your model, such as the weights or activations of\\nits hidden layers. This may be useful for regularization purposes, or to monitor some\\ninternal aspect of your model.\\nTo define a custom loss based on model internals, just compute it based on any part\\nof the model you want, then pass the result to the add_loss()  method. For example,\\nthe following custom model represents a standard MLP regressor with 5 hidden lay‐\\ners, except it also implements a reconstruction loss  (see ???): we add an extra Dense\\nlayer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\\nthe model. Since the reconstruction must have the same shape as the model’s inputs,\\nwe need to create this Dense  layer in the build()  method to have access to the shape\\nof the inputs. In the call()  method, we compute both the regular output of the MLP ,\\nplus the output of the reconstruction layer. We then compute the mean squared dif‐\\nference between the reconstructions and the inputs, and we add this value (times\\n0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\\nadd this loss to the main loss (which is why we scaled down the reconstruction loss,\\nto ensure the main loss dominates). As a result, the model will be forced to preserve\\nas much information as possible through the hidden layers, even information that is\\nnot directly useful for the regression task itself. In practice, this loss sometimes\\nimproves generalization; it is a regularization loss:\\nclass ReconstructingRegressor (keras.models.Model):\\n    def __init__ (self, output_dim , **kwargs):\\n        super().__init__ (**kwargs)\\n        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\\n                                          kernel_initializer =\"lecun_normal\" )\\n388 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 245, 'title': 'Computing Gradients Using Autodiff', 'content': '                       for _ in range(5)]\\n        self.out = keras.layers.Dense(output_dim )\\n    def build(self, batch_input_shape ):\\n        n_inputs  = batch_input_shape [-1]\\n        self.reconstruct  = keras.layers.Dense(n_inputs )\\n        super().build(batch_input_shape )\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.hidden:\\n            Z = layer(Z)\\n        reconstruction  = self.reconstruct (Z)\\n        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\\n        self.add_loss (0.05 * recon_loss )\\n        return self.out(Z)\\nSimilarly, you can add a custom metric based on model internals by computing it in\\nany way you want, as long at the result is the output of a metric object. For example,\\nyou can create a keras.metrics.Mean()  object in the constructor, then call it in the\\ncall()  method, passing it the recon_loss , and finally add it to the model by calling\\nthe model’s add_metric()  method. This way, when you train the model, Keras will\\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\\n0.05 times the reconstruction loss) and the mean reconstruction error over each\\nepoch. Both will go down during training:\\nEpoch 1/5\\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\\nEpoch 2/5\\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\\n[...]\\nIn over 99% of the cases, everything we have discussed so far will be sufficient to\\nimplement whatever model you want to build, even with complex architectures, los‐\\nses, metrics, and so on. However, in some rare cases you may need to customize the\\ntraining loop itself. However, before we get there, we need to look at how to compute\\ngradients automatically in TensorFlow.\\nComputing Gradients Using Autodiff\\nTo understand how to use autodiff (see Chapter 10  and ???) to compute gradients\\nautomatically, let’s consider a simple toy function:\\ndef f(w1, w2):\\n    return 3 * w1 ** 2 + 2 * w1 * w2\\nIf you know calculus, you can analytically find that the partial derivative of this func‐\\ntion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\\nwith regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\\nCustomizing Models and Training Algorithms | 389', 'children': []}, {'id': 246, 'title': 'Custom Training Loops', 'content': 'Finally, you may occasionally run into some numerical issues when computing gradi‐\\nents. For example, if you compute the gradients of the my_softplus()  function for\\nlarge inputs, the result will be NaN:\\n>>> x = tf.Variable ([100.])\\n>>> with tf.GradientTape () as tape:\\n...     z = my_softplus (x)\\n...\\n>>> tape.gradient (z, [x])\\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\\nThis is because computing the gradients of this function using autodiff leads to some\\nnumerical difficulties: due to floating point precision errors, autodiff ends up com‐\\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\\ncomputing the gradients of the my_softplus()  function, by decorating it with\\n@tf.custom_gradient , and making it return both its normal output and the function\\nthat computes the derivatives (note that it will receive as input the gradients that were\\nbackpropagated so far, down to the softplus function, and according to the chain rule\\nwe should multiply them with this function’s gradients):\\n@tf.custom_gradient\\ndef my_better_softplus (z):\\n    exp = tf.exp(z)\\n    def my_softplus_gradients (grad):\\n        return grad / (1 + 1 / exp)\\n    return tf.math.log(exp + 1), my_softplus_gradients\\nNow when we compute the gradients of the my_better_softplus()  function, we get\\nthe proper result, even for large input values (however, the main output still explodes\\nbecause of the exponential: one workaround is to use tf.where()  to just return the\\ninputs when they are large).\\nCongratulations! Y ou can now compute the gradients of any function (provided it is\\ndifferentiable at the point where you compute it), you can even compute Hessians,\\nblock backpropagation when needed and even write your own gradient functions!\\nThis is probably more flexibility than you will ever need, even if you build your own\\ncustom training loops, as we will see now.\\nCustom Training Loops\\nIn some rare cases, the fit()  method may not be flexible enough for what you need\\nto do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\\nuses two different optimizers: one for the wide path and the other for the deep path.\\nSince the fit()  method only uses one optimizer (the one that we specify when\\nCustomizing Models and Training Algorithms | 393', 'children': []}]}, {'id': 247, 'title': 'TensorFlow Functions and Graphs', 'content': '11The truth is we did not process every single instance in the training set because we sampled instances ran‐\\ndomly, so some were processed more than once while others were not processed at all. In practice that’s fine.\\nMoreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\\n12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\\n13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\\ntrainable variable ( not all variables!), and we apply them to the optimizer to per‐\\nform a Gradient Descent step.\\n•Next we update the mean loss and the metrics (over the current epoch), and we\\ndisplay the status bar.\\n•At the end of each epoch, we display the status bar again to make it look com‐\\nplete11 and to print a line feed, and we reset the states of the mean loss and the\\nmetrics.\\nIf you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\\nthis for you. If you want to apply any other transformation to the gradients, simply do\\nso before calling the apply_gradients()  method.\\nIf you add weight constraints to your model (e.g., by setting kernel_constraint  or\\nbias_constraint  when creating a layer), you should update the training loop to\\napply these constraints just after apply_gradients() :\\nfor variable  in model.variables :\\n    if variable .constraint  is not None:\\n        variable .assign(variable .constraint (variable ))\\nMost importantly, this training loop does not handle layers that behave differently\\nduring training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\\nyou need to call the model with training=True  and make sure it propagates this to\\nevery layer that needs it.12\\nAs you can see, there are quite a lot of things you need to get right, it is easy to make a\\nmistake. But on the bright side, you get full control, so it’s your call.\\nNow that you know how to customize any part of your models13 and training algo‐\\nrithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\\ncan speed up your custom code considerably, and it will also make it portable to any\\nplatform supported by TensorFlow (see ???).\\nTensorFlow Functions and Graphs\\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\\nthem): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\\n396 | Chapter 12: Custom Models and Training with TensorFlow', 'children': [{'id': 248, 'title': 'Autograph and Tracing', 'content': '14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\\ntf_cube()  actually runs much slower than cube() .\\ncomputations.14 Most of the time you will not really need to know more than that:\\nwhen you want to boost a Python function, just transform it into a TF Function.\\nThat’s all!\\nMoreover, when you write a custom loss function, a custom metric, a custom layer or\\nany other custom function, and you use it in a Keras model (as we did throughout\\nthis chapter), Keras automatically converts your function into a TF Function, no need\\nto use tf.function() . So most of the time, all this magic is 100% transparent.\\nY ou can tell Keras not to convert your Python functions to TF\\nFunctions by setting dynamic=True  when creating a custom layer\\nor a custom model. Alternatively, you can set run_eagerly=True\\nwhen calling the model’s compile()  method.\\nTF Function generates a new graph for every unique set of input shapes and data\\ntypes, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\\nstant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\\ntf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\\ntf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\\nof shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\\ntypes and shapes). However, this is only true for tensor arguments: if you pass numer‐\\nical Python values to a TF Function, a new graph will be generated for every distinct\\nvalue: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\\nIf you call a TF Function many times with different numerical\\nPython values, then many graphs will be generated, slowing down\\nyour program and using up a lot of RAM. Python values should be\\nreserved for arguments that will have few unique values, such as\\nhyperparameters like the number of neurons per layer. This allows\\nTensorFlow to better optimize each variant of your model.\\nAutograph and Tracing\\nSo how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\\nfunction’s source code to capture all the control flow statements, such as for loops\\nand while  loops, if statements, as well as break , continue  and return  statements.\\nThis first step is called autograph . The reason TensorFlow has to analyze the source\\ncode is that Python does not provide any other way to capture control flow state‐\\nments: it offers magic methods like __add__()  or __mul__()  to capture operators like\\n398 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}, {'id': 249, 'title': 'TF Function Rules', 'content': 'To view the generated function’s source code, you can call tf.auto\\ngraph.to_code(sum_squares.python_function) . The code is not\\nmeant to be pretty, but it can sometimes help for debugging.\\nTF Function Rules\\nMost of the time, converting a Python function that performs TensorFlow operations\\ninto a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\\nof it for you. However, there are a few rules to respect:\\n•If you call any external library, including NumPy or even the standard library,\\nthis call will run only during tracing, it will not be part of the graph. Indeed, a\\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\\nvariables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\\nnp.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\\n(unless you really want the code to run only during tracing).\\n—For example, if you define a TF function f(x)  that just returns np.ran\\ndom.rand() , a random number will only be generated when the function is\\ntraced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\\nsame random number, but f(tf.constant([2., 3.]))  will return a different\\none. If you replace np.random.rand()  with tf.random.uniform([]) , then a\\nnew random number will be generated upon every call, since the operation\\nwill be part of the graph.\\n—If your non-TensorFlow code has side-effects (such as logging something or\\nupdating a Python counter), then you should not expect that side-effect to\\noccur every time you call the TF Function, as it will only occur when the func‐\\ntion is traced.\\n—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\\nthis will hinder performance, as TensorFlow will not be able to do any graph\\noptimization on this code, and it will also reduce portability, as the graph will\\nonly run on platforms where Python is available (and the right libraries\\ninstalled).\\n•Y ou can call other Python functions or TF Functions, but they should follow the\\nsame rules, as TensorFlow will also capture their operations in the computation\\ngraph. Note that these other functions do not need to be decorated with\\n@tf.function .\\n•If the function creates a TensorFlow variable (or any other stateful TensorFlow\\nobject, such as a dataset or a queue), it must do so upon the very first call, and\\nonly then, or else you will get an exception. It is usually preferable to create vari‐\\nables outside of the TF Function (e.g., in the build()  method of a custom layer).\\n400 | Chapter 12: Custom Models and Training with TensorFlow', 'children': []}]}]}, {'id': 250, 'title': 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 'content': 'CHAPTER 13\\nLoading and Preprocessing Data with\\nTensorFlow\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 13 in the final\\nrelease of the book.\\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\\nlibraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\\nset object, tell it where to get the data, then transform it in any way you want, and\\nTensorFlow takes care of all the implementation details, such as multithreading,\\nqueuing, batching, prefetching, and so on.\\nOff the shelf, the Data API can read from text files (such as CSV files), binary files\\nwith fixed-size records, and binary files that use TensorFlow’s TFRecord format,\\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\\nformat based on Protocol Buffers (an open source binary format). The Data API also\\nhas support for reading from SQL databases. Moreover, many Open Source exten‐\\nsions are available to read from all sorts of data sources, such as Google’s BigQuery\\nservice.\\nHowever, reading huge datasets efficiently is not the only difficulty: the data also\\nneeds to be preprocessed. Indeed, it is not always composed strictly of convenient\\nnumerical fields: sometimes there will be text features, categorical features, and so on.\\nTo handle this, TensorFlow provides the Features API : it lets you easily convert these\\nfeatures to numerical features that can be consumed by your neural network. For\\n403', 'children': [{'id': 251, 'title': 'The Data API', 'content': 'example, categorical features with a large number of categories (such as cities, or\\nwords) can be encoded using embeddings  (as we will see, an embedding is a trainable\\ndense vector that represents a category).\\nBoth the Data API and the Features API work seamlessly with\\ntf.keras.\\nIn this chapter, we will cover the Data API, the TFRecord format and the Features\\nAPI in detail. We will also take a quick look at a few related projects from Tensor‐\\nFlow’s ecosystem:\\n•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\\nfunction that can be run both in batch mode on your full training set, before\\ntraining (to speed it up), and then exported to a TF Function and incorporated\\ninto your trained model, so that once it is deployed in production, it can take\\ncare of preprocessing new instances on the fly.\\n•TF Datasets (TFDS) provides a convenient function to download many common\\ndatasets of all kinds, including large ones like ImageNet, and it provides conve‐\\nnient dataset objects to manipulate them using the Data API.\\nSo let’s get started!\\nThe Data API\\nThe whole Data API revolves around the concept of a dataset : as you might suspect,\\nthis represents a sequence of data items. Usually you will use datasets that gradually\\nread data from disk, but for simplicity let’s just create a dataset entirely in RAM using\\ntf.data.Dataset.from_tensor_slices() :\\n>>> X = tf.range(10)  # any data tensor\\n>>> dataset = tf.data.Dataset.from_tensor_slices (X)\\n>>> dataset\\n<TensorSliceDataset shapes: (), types: tf.int32>\\nThe from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\\nwhose elements are all the slices of X (along the first dimension), so this dataset con‐\\ntains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\\ndataset if we had used tf.data.Dataset.range(10) .\\nY ou can simply iterate over a dataset’s items like this:\\n>>> for item in dataset:\\n...     print(item)\\n404 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 252, 'title': 'Chaining Transformations', 'content': '...\\ntf.Tensor(0, shape=(), dtype=int32)\\ntf.Tensor(1, shape=(), dtype=int32)\\ntf.Tensor(2, shape=(), dtype=int32)\\n[...]\\ntf.Tensor(9, shape=(), dtype=int32)\\nChaining Transformations\\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\\ntransformation methods. Each method returns a new dataset, so you can chain trans‐\\nformations like this (this chain is illustrated in Figure 13-1 ):\\n>>> dataset = dataset.repeat(3).batch(7)\\n>>> for item in dataset:\\n...     print(item)\\n...\\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\\ntf.Tensor([8 9], shape=(2,), dtype=int32)\\nFigure 13-1. Chaining Dataset Transformations\\nIn this example, we first call the repeat()  method on the original dataset, and it\\nreturns a new dataset that will repeat the items of the original dataset 3 times. Of\\ncourse, this will not copy the whole data in memory 3 times! In fact, if you call this\\nmethod with no arguments, the new dataset will repeat the source dataset forever.\\nThen we call the batch()  method on this new dataset, and again this creates a new\\ndataset. This one will group the items of the previous dataset in batches of 7 items.\\nFinally, we iterate over the items of this final dataset. As you can see, the batch()\\nmethod had to output a final batch of size 2 instead of 7, but you can call it with\\ndrop_remainder=True  if you want it to drop this final batch so that all batches have\\nthe exact same size.\\nThe Data API | 405', 'children': []}, {'id': 253, 'title': 'Shuffling the Data', 'content': 'The dataset methods do not modify datasets, they create new ones,\\nso make sure to keep a reference to these new datasets (e.g., data\\nset = ... ), or else nothing will happen.\\nY ou can also apply any transformation you want to the items by calling the map()\\nmethod. For example, this creates a new dataset with all items doubled:\\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\\nThis function is the one you will call to apply any preprocessing you want to your\\ndata. Sometimes, this will include computations that can be quite intensive, such as\\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\\nspeed things up: it’s as simple as setting the num_parallel_calls  argument.\\nWhile the map()  applies a transformation to each item, the apply()  method applies a\\ntransformation to the dataset as a whole. For example, the following code “unbatches”\\nthe dataset, by applying the unbatch()  function to the dataset (this function is cur‐\\nrently experimental, but it will most likely move to the core API in a future release).\\nEach item in the new dataset will be a single integer tensor instead of a batch of 7\\nintegers:\\n>>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\\nIt is also possible to simply filter the dataset using the filter()  method:\\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\\nY ou will often want to look at just a few items from a dataset. Y ou can use the take()\\nmethod for that:\\n>>> for item in dataset.take(3):\\n...     print(item)\\n...\\ntf.Tensor(0, shape=(), dtype=int64)\\ntf.Tensor(2, shape=(), dtype=int64)\\ntf.Tensor(4, shape=(), dtype=int64)\\nShuffling  the Data\\nAs you know, Gradient Descent works best when the instances in the training set are\\nindependent and identically distributed (see Chapter 4 ). A simple way to ensure this\\nis to shuffle the instances. For this, you can just use the shuffle()  method. It will\\ncreate a new dataset that will start by filling up a buffer with the first items of the\\nsource dataset, then whenever it is asked for an item, it will pull one out randomly\\nfrom the buffer, and replace it with a fresh one from the source dataset, until it has\\niterated entirely through the source dataset. At this point it continues to pull out\\nitems randomly from the buffer until it is empty. Y ou must specify the buffer size, and\\n406 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 254, 'title': 'Preprocessing the Data', 'content': \"By default, interleave()  does not use parallelism, it just reads one line at a time\\nfrom each file, sequentially. However, if you want it to actually read files in parallel,\\nyou can set the num_parallel_calls  argument to the number of threads you want.\\nY ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\\nthe right number of threads dynamically based on the available CPU (however, this is\\nan experimental feature for now). Let’s look at what the dataset contains now:\\n>>> for line in dataset.take(5):\\n...     print(line.numpy())\\n...\\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\\nThese are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\\nLooks good! But as you can see, these are just byte strings, we need to parse them,\\nand also scale the data.\\nPreprocessing the Data\\nLet’s implement a small function that will perform this preprocessing:\\nX_mean, X_std = [...] # mean and scale of each feature in the training set\\nn_inputs  = 8\\ndef preprocess (line):\\n  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\\n  fields = tf.io.decode_csv (line, record_defaults =defs)\\n  x = tf.stack(fields[:-1])\\n  y = tf.stack(fields[-1:])\\n  return (x - X_mean) / X_std, y\\nLet’s walk through this code:\\n•First, we assume that you have precomputed the mean and standard deviation of\\neach feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\\narrays) containing 8 floats, one per input feature.\\n•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\\nit uses the tf.io.decode_csv()  function, which takes two arguments: the first is\\nthe line to parse, and the second is an array containing the default value for each\\ncolumn in the CSV file. This tells TensorFlow not only the default value for each\\ncolumn, but also the number of columns and the type of each column. In this\\nexample, we tell it that all feature columns are floats and missing values should\\ndefault to 0, but we provide an empty array of type tf.float32  as the default\\nvalue for the last column (the target): this tells TensorFlow that this column con‐\\nThe Data API | 409\", 'children': []}, {'id': 255, 'title': 'Putting Everything Together', 'content': \"tains floats, but that there is no default value, so it will raise an exception if it\\nencounters a missing value.\\n•The decode_csv()  function returns a list of scalar tensors (one per column) but\\nwe need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\\nfor the last one (the target): this will stack these tensors into a 1D array. We then\\ndo the same for the target value (this makes it a 1D tensor array with a single\\nvalue, rather than a scalar tensor).\\n•Finally, we scale the input features by subtracting the feature means and then\\ndividing by the feature standard deviations, and we return a tuple containing the\\nscaled features and the target.\\nLet’s test this preprocessing function:\\n>>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\\n array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\\n <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\\nWe can now apply this preprocessing function to the dataset.\\nPutting Everything Together\\nTo make the code reusable, let’s put together everything we have discussed so far into\\na small helper function: it will create and return a dataset that will efficiently load Cal‐\\nifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\\n(see Figure 13-2 ):\\ndef csv_reader_dataset (filepaths , repeat=None, n_readers =5,\\n                       n_read_threads =None, shuffle_buffer_size =10000,\\n                       n_parse_threads =5, batch_size =32):\\n    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\\n    dataset = dataset.interleave (\\n        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\\n        cycle_length =n_readers , num_parallel_calls =n_read_threads )\\n    dataset = dataset.shuffle(shuffle_buffer_size )\\n    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\\n    dataset = dataset.batch(batch_size )\\n    return dataset.prefetch (1)\\n410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\", 'children': []}, {'id': 256, 'title': 'Prefetching', 'content': '2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\\nexperimental feature for now).\\nFigure 13-2. Loading and Preprocessing Data From Multiple CSV Files\\nEverything should make sense in this code, except the very last line ( prefetch(1) ),\\nwhich is actually quite important for performance.\\nPrefetching\\nBy calling prefetch(1)  at the end, we are creating a dataset that will do its best to\\nalways be one batch ahead2. In other words, while our training algorithm is working\\non one batch, the dataset will already be working in parallel on getting the next batch\\nready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\\nwe also ensure that loading and preprocessing are multithreaded (by setting num_par\\nallel_calls  when calling interleave()  and map() ), we can exploit multiple cores\\non the CPU and hopefully make preparing one batch of data shorter than running a\\ntraining step on the GPU: this way the GPU will be almost 100% utilized (except for\\nthe data transfer time from the CPU to the GPU), and training will run much faster.\\nThe Data API | 411', 'children': []}, {'id': 257, 'title': 'Using the Dataset With tf.keras', 'content': '3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\\n4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\\nspecify it, the progress bar will not be displayed during the first epoch.\\n5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\\nthese lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\\nNow we can use the csv_reader_dataset()  function to create a dataset for the train‐\\ning set (ensuring it repeats the data forever), the validation set and the test set:\\ntrain_set  = csv_reader_dataset (train_filepaths , repeat=None)\\nvalid_set  = csv_reader_dataset (valid_filepaths )\\ntest_set  = csv_reader_dataset (test_filepaths )\\nAnd now we can simply build and train a Keras model using these datasets.3 All we\\nneed to do is to call the fit()  method with the datasets instead of X_train  and\\ny_train , and specify the number of steps per epoch for each set:4\\nmodel = keras.models.Sequential ([...])\\nmodel.compile([...])\\nmodel.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\\n          validation_data =valid_set ,\\n          validation_steps =len(X_valid) // batch_size )\\nSimilarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\\nspecify the number of steps per epoch):\\nmodel.evaluate (test_set , steps=len(X_test) // batch_size )\\nmodel.predict(new_set, steps=len(X_new) // batch_size )\\nUnlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\\njust ignore them). Note that in all these cases, you can still use NumPy arrays instead\\nof datasets if you want (but of course they need to have been loaded and preprocessed\\nfirst).\\nIf you want to build your own custom training loop (as in Chapter 12 ), you can just\\niterate over the training set, very naturally:\\nfor X_batch, y_batch in train_set :\\n    [...] # perform one gradient descent step\\nIn fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\\nwhole training loop!5\\n@tf.function\\ndef train(model, optimizer , loss_fn, n_epochs , [...]):\\n    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\\n    for X_batch, y_batch in train_set :\\n        with tf.GradientTape () as tape:\\nThe Data API | 413', 'children': []}]}, {'id': 258, 'title': 'The TFRecord Format', 'content': '            y_pred = model(X_batch)\\n            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\\n            loss = tf.add_n([main_loss ] + model.losses)\\n        grads = tape.gradient (loss, model.trainable_variables )\\n        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\\nCongratulations, you now know how to build powerful input pipelines using the Data\\nAPI! However, so far we have used CSV files, which are common, simple and conve‐\\nnient, but they are not really efficient, and they do not support large or complex data\\nstructures very well, such as images or audio. So let’s use TFRecords instead.\\nIf you are happy with CSV files (or whatever other format you are\\nusing), you do not have  to use TFRecords. As the saying goes, if it\\nain’t broke, don’t fix it! TFRecords are useful when the bottleneck\\nduring training is loading and parsing the data.\\nThe TFRecord Format\\nThe TFRecord format is TensorFlow’s preferred format for storing large amounts of\\ndata and reading it efficiently. It is a very simple binary format that just contains a\\nsequence of binary records of varying sizes (each record just has a length, a CRC\\nchecksum to check that the length was not corrupted, then the actual data, and finally\\na CRC checksum for the data). Y ou can easily create a TFRecord file using the\\ntf.io.TFRecordWriter  class:\\nwith tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\\n    f.write(b\"This is the first record\" )\\n    f.write(b\"And this is the second record\" )\\nAnd you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\\nfiles:\\nfilepaths  = [\"my_data.tfrecord\" ]\\ndataset = tf.data.TFRecordDataset (filepaths )\\nfor item in dataset:\\n    print(item)\\nThis will output:\\ntf.Tensor(b\\'This is the first record\\', shape=(), dtype=string)\\ntf.Tensor(b\\'And this is the second record\\', shape=(), dtype=string)\\nBy default, a TFRecordDataset  will read files one by one, but you\\ncan make it read multiple files in parallel and interleave their\\nrecords by setting num_parallel_reads . Alternatively, you could\\nobtain the same result by using list_files()  and interleave()\\nas we did earlier to read multiple CSV files.\\n414 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 259, 'title': 'Compressed TFRecord Files', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 260, 'title': 'A Brief Introduction to Protocol Buffers', 'content': '6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\\nbe loaded via a network connection. Y ou can create a compressed TFRecord file by\\nsetting the options  argument:\\noptions = tf.io.TFRecordOptions (compression_type =\"GZIP\")\\nwith tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\\n  [...]\\nWhen reading a compressed TFRecord file, you need to specify the compression type:\\ndataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\\n                                  compression_type =\"GZIP\")\\nA Brief Introduction to Protocol Buffers\\nEven though each record can use any binary format you want, TFRecord files usually\\ncontain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\\nble and efficient binary format developed at Google back in 2001 and Open Sourced\\nin 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\\ndure call system. Protocol Buffers are defined using a simple language that looks like\\nthis:\\nsyntax = \"proto3\" ;\\nmessage Person {\\n  string name = 1;\\n  int32 id = 2;\\n  repeated  string email = 3;\\n}\\nThis definition says we are using the protobuf format version 3, and it specifies that\\neach Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\\nand zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\\nfield identifiers: they will be used in each record’s binary representation. Once you\\nhave a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\\nbuf compiler, to generate access classes in Python (or some other language). Note that\\nthe protobuf definitions we will use have already been compiled for you, and their\\nPython classes are part of TensorFlow, so you will not need to use protoc . All you\\nneed to know is how to use protobuf access classes in Python. To illustrate the basics,\\nlet’s look at a simple example that uses the access classes generated for the Person\\nprotobuf (the code is explained in the comments):\\n>>> from person_pb2  import Person  # import the generated access class\\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\\n>>> print(person)  # display the Person\\nThe TFRecord Format | 415', 'children': []}, {'id': 261, 'title': 'TensorFlow Protobufs', 'content': '7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\\nabout protobufs, please visit https://homl.info/protobuf .name: \"Al\"\\nid: 123\\nemail: \"a@b.com\"\\n>>> person.name  # read a field\\n\"Al\"\\n>>> person.name = \"Alice\"  # modify a field\\n>>> person.email[0]  # repeated fields can be accessed like arrays\\n\"a@b.com\"\\n>>> person.email.append(\"c@d.com\" )  # add an email address\\n>>> s = person.SerializeToString ()  # serialize the object to a byte string\\n>>> s\\nb\\'\\\\n\\\\x05Alice\\\\x10{\\\\x1a\\\\x07a@b.com\\\\x1a\\\\x07c@d.com\\'\\n>>> person2 = Person()  # create a new Person\\n>>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\\n27\\n>>> person == person2  # now they are equal\\nTrue\\nIn short, we import the Person  class generated by protoc , we create an instance and\\nwe play with it, visualizing it, reading and writing some fields, then we serialize it\\nusing the SerializeToString()  method. This is the binary data that is ready to be\\nsaved or transmitted over the network. When reading or receiving this binary data,\\nwe can parse it using the ParseFromString()  method, and we get a copy of the object\\nthat was serialized.7\\nWe could save the serialized Person  object to a TFRecord file, then we could load and\\nparse it: everything would work fine. However, SerializeToString()  and ParseFrom\\nString()  are not TensorFlow operations (and neither are the other operations in this\\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\\nthem in a tf.py_function()  operation, which would make the code slower and less\\nportable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\\ntobuf definitions for which it provides parsing operations.\\nTensorFlow Protobufs\\nThe main protobuf typically used in a TFRecord file is the Example  protobuf, which\\nrepresents one instance in a dataset. It contains a list of named features, where each\\nfeature can either be a list of byte strings, a list of floats or a list of integers. Here is the\\nprotobuf definition:\\nsyntax = \"proto3\" ;\\nmessage BytesList  { repeated  bytes value = 1; }\\nmessage FloatList  { repeated  float value = 1 [packed = true]; }\\nmessage Int64List  { repeated  int64 value = 1 [packed = true]; }\\n416 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 262, 'title': 'Loading and Parsing Examples', 'content': 'Now that we have a nice TFRecord file containing a serialized Example , let’s try to\\nload it.\\nLoading and Parsing Examples\\nTo load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\\nonce again, and we will parse each Example  using tf.io.parse_single_example() .\\nThis is a TensorFlow operation so it can be included in a TF Function. It requires at\\nleast two arguments: a string scalar tensor containing the serialized data, and a\\ndescription of each feature. The description is a dictionary that maps each feature\\nname to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\\ntype and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\\n(if the length may vary, such as for the \"emails\"  feature). For example:\\nfeature_description  = {\\n    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\\n    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\\n    \"emails\" : tf.io.VarLenFeature (tf.string),\\n}\\nfor serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\\n    parsed_example  = tf.io.parse_single_example (serialized_example ,\\n                                                feature_description )\\nThe fixed length features are parsed as regular tensors, but the variable length fea‐\\ntures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\\nusing tf.sparse.to_dense() , but in this case it is simpler to just access its values:\\n>>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\n>>> parsed_example [\"emails\" ].values\\n<tf.Tensor: [...] dtype=string, numpy=array([b\\'a@b.com\\', b\\'c@d.com\\'], [...])>\\nA BytesList  can contain any binary data you want, including any serialized object.\\nFor example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\\nformat, and put this binary data in a BytesList . Later, when your code reads the\\nTFRecord, it will start by parsing the Example , then you will need to call\\ntf.io.decode_jpeg()  to parse the data and get the original image (or you can use\\ntf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\\ncan also store any tensor you want in a BytesList  by serializing the tensor using\\ntf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\\nfeature. Later, when you parse the TFRecord, you can parse this data using\\ntf.io.parse_tensor() .\\nInstead of parsing examples one by one using tf.io.parse_single_example() , you\\nmay want to parse them batch by batch using tf.io.parse_example() :\\n418 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 263, 'title': 'Handling Lists of Lists Using the SequenceExample Protobuf', 'content': 'dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\\nfor serialized_examples  in dataset:\\n    parsed_examples  = tf.io.parse_example (serialized_examples ,\\n                                          feature_description )\\nAs you can see, the Example  proto will probably be sufficient for most use cases.\\nHowever, it may be a bit cumbersome to use when you are dealing with lists of lists.\\nFor example, suppose you want to classify text documents. Each document may be\\nrepresented as a list of sentences, where each sentence is represented as a list of\\nwords. And perhaps each document also has a list of comments, where each com‐\\nment is also represented as a list of words. Moreover, there may be some contextual\\ndata as well, such as the document’s author, title and publication date. TensorFlow’s\\nSequenceExample  protobuf is designed for such use cases.\\nHandling Lists of Lists Using the SequenceExample  Protobuf\\nHere is the definition of the SequenceExample  protobuf:\\nmessage FeatureList  { repeated  Feature feature = 1; };\\nmessage FeatureLists  { map<string, FeatureList > feature_list  = 1; };\\nmessage SequenceExample  {\\n    Features  context = 1;\\n    FeatureLists  feature_lists  = 2;\\n};\\nA SequenceExample  contains a Features  object for the contextual data and a Fea\\ntureLists  object which contains one or more named FeatureList  objects (e.g., a\\nFeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\\njust contains a list of Feature  objects, each of which may be a list of byte strings, a list\\nof 64-bit integers or a list of floats (in this example, each Feature  would represent a\\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\\nSequenceExample , serializing it and parsing it is very similar to building, serializing\\nand parsing an Example , but you must use tf.io.parse_single_sequence_exam\\nple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\\nparse a batch, and both functions return a tuple containing the context features (as a\\ndictionary) and the feature lists (also as a dictionary). If the feature lists contain\\nsequences of varying sizes (as in the example above), you may want to convert them\\nto ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\\nfull code):\\nparsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\\n    serialized_sequence_example , context_feature_descriptions ,\\n    sequence_feature_descriptions )\\nparsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\\nNow that you know how to efficiently store, load and parse data, the next step is to\\nprepare it so that it can be fed to a neural network. This means converting all features\\nThe TFRecord Format | 419', 'children': []}]}, {'id': 264, 'title': 'The Features API', 'content': 'into numerical features (ideally not too sparse), scaling them, and more. In particular,\\nif your data contains categorical features or text features, they need to be converted to\\nnumbers. For this, the Features API  can help.\\nThe Features API\\nPreprocessing your data can be performed in many ways: it can be done ahead of\\ntime when preparing your data files, using any tool you like. Or you can preprocess\\nyour data on the fly when loading it with the Data API (e.g., using the dataset’s map()\\nmethod, as we saw earlier). Or you can include a preprocessing layer directly in your\\nmodel. Whichever solution you prefer, the Features API can help you: it is a set of\\nfunctions available in the tf.feature_column  package, which let you define how\\neach feature (or group of features) in your data should be preprocessed (therefore you\\ncan think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\\nwill start by looking at the different types of columns available, and then we will look\\nat how to use them.\\nLet’s go back to the variant of the California housing dataset that we used in Chap‐\\nter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\\ncal column named \"housing_median_age\" :\\nhousing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\\nNumeric columns let you specify a normalization function using the normalizer_fn\\nargument. For example, let’s tweak the \"housing_median_age\"  column to define how\\nit should be scaled. Note that this requires computing ahead of time the mean and\\nstandard deviation of this feature in the training set:\\nage_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\\nhousing_median_age  = tf.feature_column .numeric_column (\\n    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\\nIn some cases, it might improve performance to bucketize some numerical features,\\neffectively transforming a numerical feature into a categorical feature. For example,\\nlet’s create a bucketized column based on the median_income  column, with 5 buckets:\\nless than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\\nyou specify 4 boundaries, there are actually 5 buckets):\\nmedian_income  = tf.feature_column .numeric_column (\"median_income\" )\\nbucketized_income  = tf.feature_column .bucketized_column (\\n    median_income , boundaries =[1.5, 3., 4.5, 6.])\\nIf the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\\nwill automatically be equal to 2 (i.e., the index of the corresponding income bucket).\\nChoosing the right boundaries can be somewhat of an art, but one approach is to just\\nuse percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\\na feature is multimodal , meaning it has separate peaks in its distribution, you may\\n420 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': [{'id': 265, 'title': 'Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 266, 'title': 'Crossed Categorical Features', 'content': 'want to define a bucket for each mode, placing the boundaries in between the peaks.\\nWhether you use the percentiles or the modes, you need to analyze the distribution of\\nyour data ahead of time, just like we had to measure the mean and standard deviation\\nahead of time to normalize the housing_median_age  column.\\nCategorical Features\\nFor categorical features such as ocean_proximity , there are several options. If it is\\nalready represented as a category ID (i.e., an integer from 0 to the max ID), then you\\ncan use the categorical_column_with_identity()  function (specifying the max\\nID). If not, and you know the list of all possible categories, then you can use categori\\ncal_column_with_vocabulary_list() :\\nocean_prox_vocab  = [\\'<1H OCEAN\\' , \\'INLAND\\' , \\'ISLAND\\' , \\'NEAR BAY\\' , \\'NEAR OCEAN\\' ]\\nocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\\n    \"ocean_proximity\" , ocean_prox_vocab )\\nIf you prefer to have TensorFlow load the vocabulary from a file, you can call catego\\nrical_column_with_vocabulary_file()  instead. As you might expect, these two\\nfunctions will simply map each category to its index in the vocabulary (e.g., NEAR\\nBAY  will be mapped to 3), and unknown categories will be mapped to -1.\\nFor categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\\nproducts, users, etc.), it may not be convenient to get the full list of possible cate‐\\ngories, or perhaps categories may be added or removed so frequently that using cate‐\\ngory indices would be too unreliable. In this case, you may prefer to use a\\ncategorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\\nwe could encode it like this:\\ncity_hash  = tf.feature_column .categorical_column_with_hash_bucket (\\n    \"city\", hash_bucket_size =1000)\\nThis feature will compute a hash for each category (i.e., for each city), modulo the\\nnumber of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\\nhigh enough to avoid getting too many collisions (i.e., different categories ending up\\nin the same bucket), but the higher you set it, the more RAM will be used (by the\\nembedding table, as we will see shortly).\\nCrossed Categorical Features\\nIf you suspect that two (or more) categorical features are more meaningful when used\\njointly, then you can create a crossed column . For example, suppose people are partic‐\\nularly fond of old houses inland and new houses near the ocean, then it might help to\\nThe Features API | 421', 'children': []}, {'id': 267, 'title': 'Encoding Categorical Features Using One-Hot Vectors', 'content': '9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\\nthe ocean_proximity  column. The crossed column will compute a hash of every age\\n& ocean proximity combination it comes across, modulo the hash_bucket_size , and\\nthis will give it the cross category ID. Y ou may then choose to use only this crossed\\ncolumn in your model, or also include the individual columns.\\nbucketized_age  = tf.feature_column .bucketized_column (\\n    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\\nage_and_ocean_proximity  = tf.feature_column .crossed_column (\\n    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\\nAnother common use case for crossed columns is to cross latitude and longitude into\\na single categorical feature: you start by bucketizing the latitude and longitude, for\\nexample into 20 buckets each, then you cross these bucketized features into a loca\\ntion  column. This will create a 20×20 grid over California, and each cell in the grid\\nwill correspond to one category:\\nlatitude  = tf.feature_column .numeric_column (\"latitude\" )\\nlongitude  = tf.feature_column .numeric_column (\"longitude\" )\\nbucketized_latitude  = tf.feature_column .bucketized_column (\\n    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\\nbucketized_longitude  = tf.feature_column .bucketized_column (\\n    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\\nlocation  = tf.feature_column .crossed_column (\\n    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\\nEncoding Categorical Features Using One-Hot Vectors\\nNo matter which option you choose to build a categorical feature (categorical col‐\\numns, bucketized columns or crossed columns), it must be encoded before you can\\nfeed it to a neural network. There are two options to encode a categorical feature:\\none-hot vectors or embeddings . For the first option, simply use the indicator_col\\numn()  function:\\nocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\\nA one-hot vector encoding has the size of the vocabulary length, which is fine if there\\nare just a few possible categories, but if the vocabulary is large, you will end up with\\ntoo many inputs fed to your neural network: it will have too many weights to learn\\nand it will probably not perform very well. In particular, this will typically be the case\\nwhen you use hash buckets. In this case, you should probably encode them using\\nembeddings  instead.\\n422 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 268, 'title': 'Encoding Categorical Features Using Embeddings', 'content': 'As a rule of thumb (but your mileage may vary!), if the number of\\ncategories is lower than 10, then one-hot encoding is generally the\\nway to go. If the number of categories is greater than 50 (which is\\noften the case when you use hash buckets), then embeddings are\\nusually preferable. In between 10 and 50 categories, you may want\\nto experiment with both options and see which one works best for\\nyour use case. Also, embeddings typically require more training\\ndata, unless you can reuse pretrained embeddings.\\nEncoding Categorical Features Using Embeddings\\nAn embedding is a trainable dense vector that represents a category. By default,\\nembeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\\nbe represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\\nOCEAN\"  category may be represented by another random vector such as [0.631,\\n0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\\nis a hyperparameter you can tweak). Since these embeddings are trainable, they will\\ngradually improve during training, and as they represent fairly similar categories,\\nGradient Descent will certainly end up pushing them closer together, while it will\\ntend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\\nIndeed, the better the representation, the easier it will be for the neural network to\\nmake accurate predictions, so training tends to make embeddings useful representa‐\\ntions of the categories. This is called representation learning  (we will see other types of\\nrepresentation learning in ???).\\nThe Features API | 423', 'children': []}, {'id': 269, 'title': 'Using Feature Columns for Parsing', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 270, 'title': 'Using Feature Columns in Your Models', 'content': 'If there are no pretrained embeddings that you can reuse for the\\ntask you are trying to tackle, and if you do not have enough train‐\\ning data to learn them, then you can try to learn them on some\\nauxiliary task for which it is easier to obtain plenty of training data.\\nAfter that, you can reuse the trained embeddings for your main\\ntask.\\nUsing Feature Columns for Parsing\\nLet’s suppose you have created feature columns for each of your input features, as well\\nas for the target. What can you do with them? Well, for one you can pass them to the\\nmake_parse_example_spec()  function to generate feature descriptions (so you don’t\\nhave to do it manually, as we did earlier):\\ncolumns = [bucketized_age , ....., median_house_value ] # all features + target\\nfeature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\\nY ou don’t always have to create a separate feature column for each\\nand every feature. For example, instead of having 2 numerical fea‐\\nture columns, you could choose to have a single 2D column: just\\nset shape=[2]  when calling numerical_column() .\\nY ou can then create a function that parses serialized examples using these feature\\ndescriptions, and separates the target column from the input features:\\ndef parse_examples (serialized_examples ):\\n    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\\n    targets = examples .pop(\"median_house_value\" ) # separate the targets\\n    return examples , targets\\nNext, you can create a TFRecordDataset  that will read batches of serialized examples\\n(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\\nate features):\\nbatch_size  = 32\\ndataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\\ndataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\\nUsing Feature Columns in Your Models\\nFeature columns can also be used directly in your model, to convert all your input\\nfeatures into a single dense vector which the neural network can then process. For\\nthis, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\\nin your model, passing it the list of feature columns (excluding the target column):\\ncolumns_without_target  = columns[:-1]\\nmodel = keras.models.Sequential ([\\n    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\\n426 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}]}, {'id': 271, 'title': 'TF Transform', 'content': 'TF Transform\\nIf preprocessing is computationally expensive, then handling it before training rather\\nthan on the fly may give you a significant speedup: the data will be preprocessed just\\nonce per instance before  training, rather than once per instance and per epoch during\\ntraining. Tools like Apache Beam let you run efficient data processing pipelines over\\nlarge amounts of data, even distributed across multiple servers, so why not use it to\\npreprocess all the training data? This works great and indeed can speed up training,\\nbut there is one problem: once your model is trained, suppose you want to deploy it\\nto a mobile app: you will need to write some code in your app to take care of prepro‐\\ncessing the data before it is fed to the model. And suppose you also want to deploy\\nthe model to TensorFlow.js so it runs in a web browser? Once again, you will need to\\nwrite some preprocessing code. This can become a maintenance nightmare: when‐\\never you want to change the preprocessing logic, you will need to update your Apache\\nBeam code, your mobile app code and your Javascript code. It is not only time con‐\\nsuming, but also error prone: you may end up with subtle differences between the\\npreprocessing operations performed before training and the ones performed in your\\napp or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\\nmance.\\nOne improvement would be to take the trained model (trained on data that was pre‐\\nprocessed by your Apache Beam code), and before deploying it to your app or the\\nbrowser, add an extra input layer to take care of preprocessing on the fly (either by\\nwriting a custom layer or by using a DenseFeatures  layer). That’s definitely better,\\nsince now you just have two versions of your preprocessing code: the Apache Beam\\ncode and the preprocessing layer’s code.\\nBut what if you could define your preprocessing operations just once? This is what\\nTF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\\nend platform for productionizing TensorFlow models. First, to use a TFX component,\\nsuch as TF Transform, you must install it, it does not come bundled with TensorFlow.\\nY ou define your preprocessing function just once (in Python), by using TF Transform\\nfunctions for scaling, bucketizing, crossing features, and more. Y ou can also use any\\nTensorFlow operation you need. Here is what this preprocessing function might look\\nlike if we just had two features:\\nimport tensorflow_transform  as tft\\ndef preprocess (inputs):  # inputs is a batch of input features\\n    median_age  = inputs[\"housing_median_age\" ]\\n    ocean_proximity  = inputs[\"ocean_proximity\" ]\\n    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\\n    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\\n    return {\\n        \"standardized_median_age\" : standardized_age ,\\n428 | Chapter 13: Loading and Preprocessing Data with TensorFlow', 'children': []}, {'id': 272, 'title': 'The TensorFlow Datasets (TFDS) Project', 'content': '11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\\nbut this will hopefully get resolved soon.\\n        \"ocean_proximity_id\" : ocean_proximity_id\\n    }\\nNext, TF Transform lets you apply this preprocess()  function to the whole training\\nset using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\\ncan use for this purpose in your Apache Beam pipeline). In the process, it will also\\ncompute all the necessary statistics over the whole training set: in this example, the\\nmean and standard deviation of the housing_median_age  feature, and the vocabulary\\nfor the ocean_proximity  feature. The components that compute these statistics are\\ncalled analyzers .\\nImportantly, TF Transform will also generate an equivalent TensorFlow Function that\\nyou can plug into the model you deploy. This TF Function contains all the necessary\\nstatistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\\nsimply included as constants.\\nAt the time of this writing, TF Transform only supports Tensor‐\\nFlow 1. Moreover, Apache Beam only has partial support for\\nPython 3. That said, both these limitations will likely be fixed by\\nthe time your read this.\\nWith the Data API, TFRecords, the Features API and TF Transform, you can build\\nhighly scalable input pipelines for training, and also benefit from fast and portable\\ndata preprocessing in production.\\nBut what if you just wanted to use a standard dataset? Well in that case, things are\\nmuch simpler: just use TFDS!\\nThe TensorFlow Datasets (TFDS) Project\\nThe TensorFlow Datasets  project makes it trivial to download common datasets, from\\nsmall ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\\nneed quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\\ning translation datasets), audio and video datasets, and more. Y ou can visit https://\\nhoml.info/tfds  to view the full list, along with a description of each dataset.\\nTFDS is not bundled with TensorFlow, so you need to install the tensorflow-\\ndatasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\\nfunction, and it will download the data you want (unless it was already downloaded\\nearlier), and return the data as a dictionary of Datasets  (typically one for training,\\nThe TensorFlow Datasets (TFDS) Project | 429', 'children': []}]}, {'id': 273, 'title': 'Chapter 14. Deep Computer Vision Using Convolutional Neural Networks', 'content': 'CHAPTER 14\\nDeep Computer Vision Using Convolutional\\nNeural Networks\\nWith Early Release ebooks, you get books in their earliest form—\\nthe author’s raw and unedited content as he or she writes—so you\\ncan take advantage of these technologies long before the official\\nrelease of these titles. The following will be Chapter 14 in the final\\nrelease of the book.\\nAlthough IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\\nparov back in 1996, it wasn’t until fairly recently that computers were able to reliably\\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\\nfact that perception largely takes place outside the realm of our consciousness, within\\nspecialized visual, auditory, and other sensory modules in our brains. By the time\\nsensory information reaches our consciousness, it is already adorned with high-level\\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\\nnot to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\\nognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\\nexperience: perception is not trivial at all, and to understand it we must look at how\\nthe sensory modules work.\\nConvolutional neural networks (CNNs) emerged from the study of the brain’s visual\\ncortex, and they have been used in image recognition since the 1980s. In the last few\\nyears, thanks to the increase in computational power, the amount of available training\\ndata, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\\naged to achieve superhuman performance on some complex visual tasks. They power\\nimage search services, self-driving cars, automatic video classification systems, and\\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\\n431', 'children': [{'id': 274, 'title': 'The Architecture of the Visual Cortex', 'content': '1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\\n2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\\n3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\\nhowever, we will focus on visual applications for now.\\nIn this chapter we will present where CNNs came from, what their building blocks\\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis‐\\ncuss some of the best CNN architectures, and discuss other visual tasks, including\\nobject detection  (classifying multiple objects in an image and placing bounding boxes\\naround them) and semantic segmentation  (classifying each pixel according to the class\\nof the object it belongs to).\\nThe Architecture of the Visual Cortex\\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\\nMedicine in 1981 for their work). In particular, they showed that many neurons in\\nthe visual cortex have a small local receptive field, meaning they react only to visual\\nstimuli located in a limited region of the visual field (see Figure 14-1 , in which the\\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\\nfields of different neurons may overlap, and together they tile the whole visual field.\\nMoreover, the authors showed that some neurons react only to images of horizontal\\nlines, while others react only to lines with different orientations (two neurons may\\nhave the same receptive field but react to different line orientations). They also\\nnoticed that some neurons have larger receptive fields, and they react to more com‐\\nplex patterns that are combinations of the lower-level patterns. These observations\\nled to the idea that the higher-level neurons are based on the outputs of neighboring\\nlower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\\nfew neurons from the previous layer). This powerful architecture is able to detect all\\nsorts of complex patterns in any area of the visual field.\\n432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 275, 'title': 'Convolutional Layer', 'content': '6A convolution is a mathematical operation that slides one function over another and measures the integral of\\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\\nsimilar to convolutions (see https://homl.info/76  for more details).\\nConvolutional Layer\\nThe most important building block of a CNN is the convolutional layer :6 neurons in\\nthe first convolutional layer are not connected to every single pixel in the input image\\n(like they were in previous chapters), but only to pixels in their receptive fields (see\\nFigure 14-2 ). In turn, each neuron in the second convolutional layer is connected\\nonly to neurons located within a small rectangle in the first layer. This architecture\\nallows the network to concentrate on small low-level features in the first hidden layer,\\nthen assemble them into larger higher-level features in the next hidden layer, and so\\non. This hierarchical structure is common in real-world images, which is one of the\\nreasons why CNNs work so well for image recognition.\\nFigure 14-2. CNN layers with rectangular local receptive fields\\nUntil now, all multilayer neural networks we looked at had layers\\ncomposed of a long line of neurons, and we had to flatten input\\nimages to 1D before feeding them to the neural network. Now each\\nlayer is represented in 2D, which makes it easier to match neurons\\nwith their corresponding inputs.\\nA neuron located in row i, column j of a given layer is connected to the outputs of the\\nneurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\\norder for a layer to have the same height and width as the previous layer, it is com‐\\n434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 276, 'title': 'Filters', 'content': 'Figure 14-4. Reducing dimensionality using a stride of 2\\nFilters\\nA neuron’s weights can be represented as a small image the size of the receptive field.\\nFor example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\\ntion kernels ). The first one is represented as a black square with a vertical white line in\\nthe middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\\n1s); neurons using these weights will ignore everything in their receptive field except\\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\\nones located in the central vertical line). The second filter is a black square with a\\nhorizontal white line in the middle. Once again, neurons using these weights will\\nignore everything in their receptive field except for the central horizontal line.\\nNow if all neurons in a layer use the same vertical line filter (and the same bias term),\\nand you feed the network the input image shown in Figure 14-5  (bottom image), the\\nlayer will output the top-left image. Notice that the vertical white lines get enhanced\\nwhile the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\\nrons use the same horizontal line filter; notice that the horizontal white lines get\\nenhanced while the rest is blurred out. Thus, a layer full of neurons using the same\\nfilter outputs a feature map , which highlights the areas in an image that activate the\\nfilter the most. Of course you do not have to define the filters manually: instead, dur‐\\ning training the convolutional layer will automatically learn the most useful filters for\\nits task, and the layers above will learn to combine them into more complex patterns.\\n436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 277, 'title': 'Stacking Multiple Feature Maps', 'content': 'Figure 14-5. Applying two different  filters  to get two feature maps\\nStacking Multiple Feature Maps\\nUp to now, for simplicity, I have represented the output of each convolutional layer as\\na thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\\nhow many), and it outputs one feature map per filter, so it is more accurately repre‐\\nsented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\\nmap, and all neurons within a given feature map share the same parameters (i.e., the\\nsame weights and bias term). However, neurons in different feature maps use differ‐\\nent parameters. A neuron’s receptive field is the same as described earlier, but it\\nextends across all the previous layers’ feature maps. In short, a convolutional layer\\nsimultaneously applies multiple trainable filters to its inputs, making it capable of\\ndetecting multiple features anywhere in its inputs.\\nThe fact that all neurons in a feature map share the same parame‐\\nters dramatically reduces the number of parameters in the model.\\nMoreover, once the CNN has learned to recognize a pattern in one\\nlocation, it can recognize it in any other location. In contrast, once\\na regular DNN has learned to recognize a pattern in one location, it\\ncan recognize it only in that particular location.\\nMoreover, input images are also composed of multiple sublayers: one per color chan‐\\nnel. There are typically three: red, green, and blue (RGB). Grayscale images have just\\nConvolutional Layer | 437', 'children': []}, {'id': 278, 'title': 'TensorFlow Implementation', 'content': 'It is a bit ugly due to all the different indices, but all it does is calculate the weighted\\nsum of all the inputs, plus the bias term.\\nEquation 14-1. Computing the output of a neuron in a convolutional layer\\nzi,j,k=bk+∑\\nu= 0fh− 1\\n∑\\nv= 0fw− 1\\n∑\\nk′= 0fn′− 1\\nxi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\\nj′=j×sw+v\\n•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\\nconvolutional layer (layer l).\\n•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\\nthe height and width of the receptive field, and fn′ is the number of feature maps\\nin the previous layer (layer l – 1).\\n•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\\nmap k′ (or channel k′ if the previous layer is the input layer).\\n•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\\ntweaks the overall brightness of the feature map k.\\n•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\\nl and its input located at row u, column v (relative to the neuron’s receptive field),\\nand feature map k′.\\nTensorFlow Implementation\\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\\n[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\\n[mini-batch size, height, width, channels] . The weights of a convolutional\\nlayer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\\nlutional layer are simply represented as a 1D tensor of shape [ fn].\\nLet’s look at a simple example. The following code loads two sample images, using\\nScikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\\nnese temple, and the other of a flower). The pixel intensities (for each color channel)\\nis represented as a byte from 0 to 255, so we scale these features simply by dividing by\\n255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\\nvertical white line in the middle, and the other with a horizontal white line in the\\nmiddle), and we apply them to both images using the tf.nn.conv2d()  function,\\nwhich is part of TensorFlow’s low-level Deep Learning API. In this example, we use\\nzero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\\nfeature maps (similar to the top-right image in Figure 14-5 ).\\nConvolutional Layer | 439', 'children': []}, {'id': 279, 'title': 'Memory Requirements', 'content': 'Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\\nIn this example, we manually defined the filters, but in a real CNN you would nor‐\\nmally define filters as trainable variables, so the neural net can learn which filters\\nwork best, as explained earlier. Instead of manually creating the variables, however,\\nyou can simply use the keras.layers.Conv2D  layer:\\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\\n                           padding=\"SAME\", activation =\"relu\")\\nThis code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\\nhorizontally and vertically), SAME padding, and applying the ReLU activation func‐\\ntion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\\nmeters: you must choose the number of filters, their height and width, the strides, and\\nthe padding type. As always, you can use cross-validation to find the right hyperpara‐\\nmeter values, but this is very time-consuming. We will discuss common CNN archi‐\\ntectures later, to give you some idea of what hyperparameter values work best in \\npractice.\\nMemory Requirements\\nAnother problem with CNNs is that the convolutional layers require a huge amount\\nof RAM. This is especially true during training, because the reverse pass of backpro‐\\npagation requires all the intermediate values computed during the forward pass.\\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\\nmaps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\\nConvolutional Layer | 441', 'children': []}]}, {'id': 280, 'title': 'Pooling Layer', 'content': '7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\\n× 1002 × 3 = 675 million parameters!\\n8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\\nRGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\\n= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\\nfully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\\nrons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\\n75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\\nare represented using 32-bit floats, then the convolutional layer’s output will occupy\\n200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\\ninstance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\\nof RAM!\\nDuring inference (i.e., when making a prediction for a new instance) the RAM occu‐\\npied by one layer can be released as soon as the next layer has been computed, so you\\nonly need as much RAM as required by two consecutive layers. But during training\\neverything computed during the forward pass needs to be preserved for the reverse\\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\\nall layers.\\nIf training crashes because of an out-of-memory error, you can try\\nreducing the mini-batch size. Alternatively, you can try reducing\\ndimensionality using a stride, or removing a few layers. Or you can\\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib‐\\nute the CNN across multiple devices.\\nNow let’s look at the second common building block of CNNs: the pooling layer .\\nPooling Layer\\nOnce you understand how convolutional layers work, the pooling layers are quite\\neasy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\\nreduce the computational load, the memory usage, and the number of parameters\\n(thereby limiting the risk of overfitting).\\nJust like in convolutional layers, each neuron in a pooling layer is connected to the\\noutputs of a limited number of neurons in the previous layer, located within a small\\nrectangular receptive field. Y ou must define its size, the stride, and the padding type,\\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\\ninputs using an aggregation function such as the max or mean. Figure 14-8  shows a\\nmax pooling layer , which is the most common type of pooling layer. In this example,\\n442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 281, 'title': 'TensorFlow Implementation', 'content': 'Figure 14-9. Invariance to small translations\\nBut max pooling has some downsides: firstly, it is obviously very destructive: even\\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\\ndirections (so its area will be four times smaller), simply dropping 75% of the input\\nvalues. And in some applications, invariance is not desirable, for example for seman‐\\ntic segmentation : this is the task of classifying each pixel in an image depending on the\\nobject that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\\nright, the output should also be translated by 1 pixel to the right. The goal in this case\\nis equivariance , not invariance: a small change to the inputs should lead to a corre‐\\nsponding small change in the output.\\nTensorFlow Implementation\\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\\ncreates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\\nV ALID padding (i.e., no padding at all):\\nmax_pool  = keras.layers.MaxPool2D (pool_size =2)\\nTo create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\\nmight expect, it works exactly like a max pooling layer, except it computes the mean\\nrather than the max. Average pooling layers used to be very popular, but people\\n444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}, {'id': 282, 'title': 'CNN Architectures', 'content': 'Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\\nDeep Learning API does: just use the tf.nn.max_pool()  function, and specify the\\nkernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\\ncates that the kernel size and stride along the batch, height and width dimensions\\nshoud be 1. The last value should be whatever kernel size and stride you want along\\nthe depth dimension, for example 3 (this must be a divisor of the input depth; for\\nexample, it will not work if the previous layer outputs 20 feature maps, since 20 is not\\na multiple of 3):\\noutput = tf.nn.max_pool (images,\\n                        ksize=(1, 1, 1, 3),\\n                        strides=(1, 1, 1, 3),\\n                        padding=\"VALID\")\\nIf you want to include this as a layer in your Keras models, you can simply wrap it in\\na Lambda  layer (or create a custom Keras layer):\\ndepth_pool  = keras.layers.Lambda(\\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\\n                             padding=\"VALID\"))\\nOne last type of pooling layer that you will often see in modern architectures is the\\nglobal average pooling  layer. It works very differently: all it does is compute the mean\\nof each entire feature map (it’s like an average pooling layer using a pooling kernel\\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin‐\\ngle number per feature map and per instance. Although this is of course extremely\\ndestructive (most of the information in the feature map is lost), it can be useful as the\\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\\nkeras.layers.GlobalAvgPool2D  class:\\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D ()\\nIt is actually equivalent to this simple Lamba  layer, which computes the mean over the\\nspatial dimensions (height and width):\\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\\nNow you know all the building blocks to create a convolutional neural network. Let’s\\nsee how to assemble them.\\nCNN Architectures\\nTypical CNN architectures stack a few convolutional layers (each one generally fol‐\\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\\n(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\\nwith more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\\ntop of the stack, a regular feedforward neural network is added, composed of a few\\n446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': [{'id': 283, 'title': 'LeNet-5', 'content': '10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\\n(1998).LeNet-5\\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\\nmentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\\nwritten digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\\nTable 14-1. LeNet-5 architecture\\nLayer Type Maps Size Kernel size Stride Activation\\nOut Fully Connected – 10 – – RBF\\nF6 Fully Connected – 84 – – tanh\\nC5 Convolution 120 1 × 1 5 × 5 1 tanh\\nS4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\\nC3 Convolution 16 10 × 10 5 × 5 1 tanh\\nS2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\\nC1 Convolution 6 28 × 28 5 × 5 1 tanh\\nIn Input 1 32 × 32 – – –\\nThere are a few extra details to be noted:\\n•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\\nnormalized before being fed to the network. The rest of the network does not use\\nany padding, which is why the size keeps shrinking as the image progresses\\nthrough the network.\\n•The average pooling layers are slightly more complex than usual: each neuron\\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi‐\\ncient (one per map) and adds a learnable bias term (again, one per map), then\\nfinally applies the activation function.\\n•Most neurons in C3 maps are connected to neurons in only three or four S2\\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\\ndetails.\\n•The output layer is a bit special: instead of computing the matrix multiplication\\nof the inputs and the weight vector, each neuron outputs the square of the Eucli‐\\ndian distance between its input vector and its weight vector. Each output meas‐\\nures how much the image belongs to a particular digit class. The cross entropy \\ncost function is now preferred, as it penalizes bad predictions much more, pro‐\\nducing larger gradients and converging faster.\\nCNN Architectures | 449', 'children': []}, {'id': 284, 'title': 'AlexNet', 'content': '11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \\ndigits.\\nAlexNet\\nThe AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\\nlarge margin: it achieved 17% top-5 error rate while the second best achieved only\\n26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\\nGeoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\\nwas the first to stack convolutional layers directly on top of each other, instead of\\nstacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\\narchitecture.\\nTable 14-2. AlexNet architecture\\nLayer Type Maps Size Kernel size Stride Padding Activation\\nOut Fully Connected – 1,000 – – – Softmax\\nF9 Fully Connected – 4,096 – – – ReLU\\nF8 Fully Connected – 4,096 – – – ReLU\\nC7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\\nC6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nC5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\\nS4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\\nC3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\\nS2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\\nC1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\\nIn Input 3 (RGB) 227 × 227 – – – –\\nTo reduce overfitting, the authors used two regularization techniques: first they\\napplied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\\nto the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\\ndomly shifting the training images by various offsets, flipping them horizontally, and\\nchanging the lighting conditions.\\nData Augmentation\\nData augmentation artificially increases the size of the training set by generating\\nmany realistic variants of each training instance. This reduces overfitting, making this\\na regularization technique. The generated instances should be as realistic as possible:\\n450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 285, 'title': 'GoogLeNet', 'content': '12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\\n13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\\nhence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\\n14-2  shows how to apply LRN.\\nEquation 14-2. Local response normalization\\nbi=aik+α∑\\nj=jlowjhigh\\naj2−β\\nwithjhigh= min i+r\\n2,fn− 1\\njlow= max 0,i−r\\n2\\n•bi is the normalized output of the neuron located in feature map i, at some row u\\nand column v (note that in this equation we consider only neurons located at this\\nrow and column, so u and v are not shown).\\n•ai is the activation of that neuron after the ReLU step, but before normalization.\\n•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\\nradius .\\n•fn is the number of feature maps.\\nFor example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\\nof the neurons located in the feature maps immediately above and below its own.\\nIn AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\\n= 1. This step can be implemented using the tf.nn.local_response_normaliza\\ntion()  function (which you can wrap in a Lambda  layer if you want to use it in a\\nKeras model).\\nA variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\\nand won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \\nhyperparameters (number of feature maps, kernel size, stride, etc.).\\nGoogLeNet\\nThe GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\\nResearch,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\\nbelow 7%. This great performance came in large part from the fact that the network\\nwas much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\\nsub-networks called inception modules ,13 which allow GoogLeNet to use parameters\\n452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 286, 'title': 'VGGNet', 'content': '14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\\n(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\\nup computations.\\n•Then comes the tall stack of nine inception modules, interleaved with a couple\\nmax pooling layers to reduce dimensionality and speed up the net.\\n•Next, the global average pooling layer simply outputs the mean of each feature\\nmap: this drops any remaining spatial information, which is fine since there was\\nnot much spatial information left at that point. Indeed, GoogLeNet input images\\nare typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\\ndividing the height and width by 2, the feature maps are down to 7 × 7. More‐\\nover, it is a classification task, not localization, so it does not matter where the\\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\\nno need to have several fully connected layers at the top of the CNN (like in\\nAlexNet), and this considerably reduces the number of parameters in the net‐\\nwork and limits the risk of overfitting.\\n•The last layers are self-explanatory: dropout for regularization, then a fully con‐\\nnected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\\nvation function to output estimated class probabilities.\\nThis diagram is slightly simplified: the original GoogLeNet architecture also included\\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules.\\nThey were both composed of one average pooling layer, one convolutional layer, two\\nfully connected layers, and a softmax activation layer. During training, their loss\\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\\ning gradients problem and regularize the network. However, it was later shown that\\ntheir effect was relatively minor.\\nSeveral variants of the GoogLeNet architecture were later proposed by Google\\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep‐\\ntion modules, and reaching even better performance.\\nVGGNet\\nThe runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\\nyan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\\nvolutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\\nlayer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\\nwork with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\\nfilters.\\n456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 287, 'title': 'ResNet', 'content': '15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\\nThe ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\\noped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\\n3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\\ntrend: models are getting deeper and deeper, with fewer and fewer parameters. The\\nkey to being able to train such a deep network is to use skip connections  (also called\\nshortcut connections ): the signal feeding into a layer is also added to the output of a\\nlayer located a bit higher up the stack. Let’s see why this is useful.\\nWhen training a neural network, the goal is to make it model a target function h(x).\\nIf you add the input x to the output of the network (i.e., you add a skip connection),\\nthen the network will be forced to model f(x) = h(x) – x rather than h(x). This is\\ncalled residual learning  (see Figure 14-15 ).\\nFigure 14-15. Residual learning\\nWhen you initialize a regular neural network, its weights are close to zero, so the net‐\\nwork just outputs values close to zero. If you add a skip connection, the resulting net‐\\nwork just outputs a copy of its inputs; in other words, it initially models the identity\\nfunction. If the target function is fairly close to the identity function (which is often\\nthe case), this will speed up training considerably.\\nMoreover, if you add many skip connections, the network can start making progress\\neven if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\\nconnections, the signal can easily make its way across the whole network. The deep\\nresidual network can be seen as a stack of residual units , where each residual unit is a\\nsmall neural network with a skip connection.\\nCNN Architectures | 457', 'children': []}, {'id': 288, 'title': 'Xception', 'content': '16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\\n(2016).\\n17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\\nconnection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\\nthe inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\\nnumber of output feature maps (see Figure 14-18 ).\\nFigure 14-18. Skip connection when changing feature map size and depth\\nResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\\nthe fully connected layer) containing three residual units that output 64 feature maps,\\n4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\\nment this architecture later in this chapter.\\nResNets deeper than that, such as ResNet-152, use slightly different residual units.\\nInstead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\\nconvolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\\nwith 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\\nmaps (4 times 64) that restores the original depth. ResNet-152 contains three such\\nRUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\\nmaps, and finally 3 RUs with 2,048 maps.\\nGoogle’s Inception-v416 architecture merged the ideas of GoogLe‐\\nNet and ResNet and achieved close to 3% top-5 error rate on\\nImageNet classification.\\nXception\\nAnother variant of the GoogLeNet architecture is also worth noting: Xception17\\n(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\\nCNN Architectures | 459', 'children': []}, {'id': 289, 'title': 'SENet', 'content': '19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\\n20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\\nall), plus a few max pooling layers and the usual final layers (a global average pooling\\nlayer, and a dense output layer).\\nY ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\\ntains no inception module at all? Well, as we discussed earlier, an Inception module\\ncontains convolutional layers with 1 × 1 filters: these look exclusively for cross-\\nchannel patterns. However, the convolution layers that sit on top of them are regular\\nconvolutional layers that look both for spatial and cross-channel patterns. So you can\\nthink of an Inception module as an intermediate between a regular convolutional\\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\\nrable convolutional layer (which considers them separately). In practice, it seems that\\nseparable convolutions generally perform better.\\nSeparable convolutions use less parameters, less memory and less\\ncomputations than regular convolutional layers, and in general\\nthey even perform better, so you should consider using them by\\ndefault (except after layers with few channels).\\nThe ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\\nversity of Hong Kong. They used an ensemble of many different techniques, includ‐\\ning a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\\nrate below 3%. Although this result is unquestionably impressive, the complexity of\\nthe solution contrasted with the simplicity of ResNets. Moreover, one year later\\nanother fairly simple architecture performed even better, as we will see now.\\nSENet\\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\\nExcitation Network  (SENet)20. This architecture extends existing architectures such as\\ninception networks or ResNets, and boosts their performance. This allowed SENet to\\nwin the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\\nsions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\\ntively. The boost comes from the fact that a SENet adds a small neural network, called\\na SE Block , to every unit in the original architecture (i.e., every inception module or\\nevery residual unit), as shown in Figure 14-20 .\\nCNN Architectures | 461', 'children': []}]}, {'id': 290, 'title': 'Implementing a ResNet-34 CNN Using Keras', 'content': 'As earlier, the global average pooling layer computes the mean activation for each fea‐\\nture map: for example, if its input contains 256 feature maps, it will output 256 num‐\\nbers representing the overall level of response for each filter. The next layer is where\\nthe “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\\nless than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\\npressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\\nrepresentation (i.e., an embedding) of the distribution of feature responses. This bot‐\\ntleneck step forces the SE Block to learn a general representation of the feature com‐\\nbinations (we will see this principle in action again when we discuss autoencoders\\nin ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\\ntor containing one number per feature map (e.g., 256), each between 0 and 1. The\\nfeature maps are then multiplied by this recalibration vector, so irrelevant features\\n(with a low recalibration score) get scaled down while relevant features (with a recali‐\\nbration score close to 1) are left alone.\\nImplementing a ResNet-34 CNN Using Keras\\nMost CNN architectures described so far are fairly straightforward to implement\\n(although generally you would load a pretrained network instead, as we will see). To\\nillustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\\ncreate a ResidualUnit  layer:\\nDefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\\n                        padding=\"SAME\", use_bias =False)\\nclass ResidualUnit (keras.layers.Layer):\\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\\n        super().__init__ (**kwargs)\\n        self.activation  = keras.activations .get(activation )\\n        self.main_layers  = [\\n            DefaultConv2D (filters, strides=strides),\\n            keras.layers.BatchNormalization (),\\n            self.activation ,\\n            DefaultConv2D (filters),\\n            keras.layers.BatchNormalization ()]\\n        self.skip_layers  = []\\n        if strides > 1:\\n            self.skip_layers  = [\\n                DefaultConv2D (filters, kernel_size =1, strides=strides),\\n                keras.layers.BatchNormalization ()]\\n    def call(self, inputs):\\n        Z = inputs\\n        for layer in self.main_layers :\\n            Z = layer(Z)\\n        skip_Z = inputs\\n        for layer in self.skip_layers :\\n464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 291, 'title': 'Using Pretrained Models From Keras', 'content': '            skip_Z = layer(skip_Z)\\n        return self.activation (Z + skip_Z)\\nAs you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\\ncreate all the layers we will need: the main layers are the ones on the right side of the\\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\\ngreater than 1). Then in the call()  method, we simply make the inputs go through\\nthe main layers, and the skip layers (if any), then we add both outputs and we apply\\nthe activation function.\\nNext, we can build the ResNet-34 simply using a Sequential  model, since it is really\\njust a long sequence of layers (we can treat each residual unit as a single layer now\\nthat we have the ResidualUnit  class):\\nmodel = keras.models.Sequential ()\\nmodel.add(DefaultConv2D (64, kernel_size =7, strides=2,\\n                        input_shape =[224, 224, 3]))\\nmodel.add(keras.layers.BatchNormalization ())\\nmodel.add(keras.layers.Activation (\"relu\"))\\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\\nprev_filters  = 64\\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\\n    strides = 1 if filters == prev_filters  else 2\\n    model.add(ResidualUnit (filters, strides=strides))\\n    prev_filters  = filters\\nmodel.add(keras.layers.GlobalAvgPool2D ())\\nmodel.add(keras.layers.Flatten())\\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" ))\\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\\nhave 128 filters, and so on. We then set the strides to 1 when the number of filters is\\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\\nand finally we update prev_filters .\\nIt is quite amazing that in less than 40 lines of code, we can build the model that won\\nthe ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\\nand the expressiveness of the Keras API. Implementing the other CNN architectures\\nis not much harder. However, Keras comes with several of these architectures built in,\\nso why not use them instead?\\nUsing Pretrained Models From Keras\\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet\\nmanually, since pretrained networks are readily available with a single line of code, in\\nthe keras.applications  package. For example:\\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\\nUsing Pretrained Models From Keras | 465', 'children': []}, {'id': 292, 'title': 'Pretrained Models for Transfer Learning', 'content': 'Image #1\\n  n04522168 - vase         46.83%\\n  n07930864 - cup          7.78%\\n  n11939491 - daisy        4.87%\\nThe correct classes (monastery and daisy) appear in the top 3 results for both images.\\nThat’s pretty good considering that the model had to choose among 1,000 classes.\\nAs you can see, it is very easy to create a pretty good image classifier using a pre‐\\ntrained model. Other vision models are available in keras.applications , including\\nseveral ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\\nVGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\\nmobile applications), and more.\\nBut what if you want to use an image classifier for classes of images that are not part\\nof ImageNet? In that case, you may still benefit from the pretrained models to per‐\\nform transfer learning.\\nPretrained Models for Transfer Learning\\nIf you want to build an image classifier, but you do not have enough training data,\\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\\ncussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\\nreusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\\nDatasets (see Chapter 13 ):\\nimport tensorflow_datasets  as tfds\\ndataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\\ndataset_size  = info.splits[\"train\"].num_examples  # 3670\\nclass_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\\nn_classes  = info.features [\"label\"].num_classes  # 5\\nNote that you can get information about the dataset by setting with_info=True . Here,\\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\\n\"train\"  dataset, no test set or validation set, so we need to split the training set. The\\nTF Datasets project provides an API for this. For example, let’s take the first 10% of\\nthe dataset for testing, the next 15% for validation, and the remaining 75% for train‐\\ning:\\ntest_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\\ntest_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\\nvalid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\\ntrain_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\\nPretrained Models for Transfer Learning | 467', 'children': []}, {'id': 293, 'title': 'Classification and Localization', 'content': 'optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\\n              metrics=[\"accuracy\" ])\\nhistory = model.fit(train_set ,\\n                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\\n                    validation_data =valid_set ,\\n                    validation_steps =int(0.15 * dataset_size  / batch_size ),\\n                    epochs=5)\\nThis will be very slow, unless you have a GPU. If you do not, then\\nyou should run this chapter’s notebook in Colab, using a GPU run‐\\ntime (it’s free!). See the instructions at https://github.com/ageron/\\nhandson-ml2 .\\nAfter training the model for a few epochs, its validation accuracy should reach about\\n75-80%, and stop making much progress. This means that the top layers are now\\npretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\\njust the top ones), and continue training (don’t forget to compile the model when you\\nfreeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\\naging the pretrained weights:\\nfor layer in base_model .layers:\\n    layer.trainable  = True\\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\\nmodel.compile(...)\\nhistory = model.fit(...)\\nIt will take a while, but this model should reach around 95% accuracy on the test set.\\nWith that, you can start training amazing image classifiers! But there’s more to com‐\\nputer vision than just classification. For example, what if you also want to know where\\nthe flower is in the picture? Let’s look at this now.\\nClassification  and Localization\\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\\nChapter 10 : to predict a bounding box around the object, a common approach is to\\npredict the horizontal and vertical coordinates of the object’s center, as well as its\\nheight and width. This means we have 4 numbers to predict. It does not require much\\nchange to the model, we just need to add a second dense output layer with 4 units\\n(typically on top of the global average pooling layer), and it can be trained using the\\nMSE loss:\\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\\n                                                  include_top =False)\\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\\nClassification  and Localization | 469', 'children': []}, {'id': 294, 'title': 'Object Detection', 'content': 'area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\\ntf.keras.metrics.MeanIoU  class.\\nFigure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\\nClassifying and localizing a single object is nice, but what if the images contain multi‐\\nple objects (as is often the case in the flowers dataset)?\\nObject Detection\\nThe task of classifying and localizing multiple objects in an image is called object\\ndetection . Until a few years ago, a common approach was to take a CNN that was\\ntrained to classify and locate a single object, then slide it across the image, as shown\\nin Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\\nshow a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\\nCNN was looking at the top left of the image, it detected part of the left-most rose,\\nand then it detected that same rose again when it was first shifted one step to the\\nright. At the next step, it started detecting part of the top-most rose, and then it detec‐\\nted it again once it was shifted one more step to the right. Y ou would then continue to\\nslide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\\nobjects can have varying sizes, you would also slide the CNN across regions of differ‐\\nent sizes. For example, once you are done with the 3 × 3 regions, you might want to\\nslide the CNN across all 4 × 4 regions as well.\\nObject Detection | 471', 'children': [{'id': 295, 'title': 'Fully Convolutional Networks (FCNs)', 'content': '23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\\n24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\\nsmaller than the kernel size.\\n•Third, repeat step two until there are no more bounding boxes to get rid of.\\nThis simple approach to object detection works pretty well, but it requires running\\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\\nslide a CNN across an image: using a Fully Convolutional Network .\\nFully Convolutional Networks (FCNs)\\nThe idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\\nsemantic segmentation (the task of classifying every pixel in an image according to\\nthe class of the object it belongs to). They pointed out that you could replace the\\ndense layers at the top of a CNN by convolutional layers. To understand this, let’s look\\nat an example: suppose a dense layer with 200 neurons sits on top of a convolutional\\nlayer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\\nthe kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\\ntions from the convolutional layer (plus a bias term). Now let’s see what happens if we\\nreplace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\\nV ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\\nis exactly the size of the input feature maps and we are using V ALID padding). In\\nother words, it will output 200 numbers, just like the dense layer did, and if you look\\nclosely at the computations performed by a convolutional layer, you will notice that\\nthese numbers will be precisely the same as the dense layer produced. The only differ‐\\nence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\\nconvolutional layer will output a tensor of shape [batch size, 1, 1, 200].\\nTo convert a dense layer to a convolutional layer, the number of fil‐\\nters in the convolutional layer must be equal to the number of units\\nin the dense layer, the filter size must be equal to the size of the\\ninput feature maps, and you must use V ALID padding. The stride\\nmay be set to 1 or more, as we will see shortly.\\nWhy is this important? Well, while a dense layer expects a specific input size (since it\\nhas one weight per input feature), a convolutional layer will happily process images of\\nany size24 (however, it does expect its inputs to have a specific number of channels,\\nsince each kernel contains a different set of weights for each input channel). Since an\\nFCN contains only convolutional layers (and pooling layers, which have the same\\nproperty), it can be trained and executed on images of any size!\\nObject Detection | 473', 'children': []}, {'id': 296, 'title': 'You Only Look Once (YOLO)', 'content': '26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\\n(2015).\\n27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\\n28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\\nFigure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\\nLarge One (right)\\nYou Only Look Once (YOLO)\\nYOLO is an extremely fast and accurate object detection architecture proposed by\\nJoseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\\n(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\\n(check out this nice demo ).\\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few\\nimportant differences:\\nObject Detection | 475', 'children': []}]}, {'id': 297, 'title': 'Semantic Segmentation', 'content': '29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\\n30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\\n(2015).with pretrained weights, and some have even been ported to TF Hub, making them\\nextremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\\nlar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\\nCNN is more complex: the image first goes through a CNN, and the output is passed\\nto a Region Proposal Network (RPN) which proposes bounding boxes that are most\\nlikely to contain an object, and a classifier is run for each bounding box, based on the\\ncropped output of the CNN.\\nThe choice of detection system depends on many factors: speed, accuracy, available\\npretrained models, training time, complexity, etc. The papers contain tables of met‐\\nrics, but there is quite a lot of variability in the testing environments, and the technol‐\\nogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\\nmost people and remain valid for more than a few months.\\nGreat! So we can locate objects by drawing bounding boxes around them. But per‐\\nhaps you might want to be a bit more precise. Let’s see how to go down to the pixel\\nlevel.\\nSemantic Segmentation\\nIn semantic segmentation , each pixel is classified according to the class of the object it\\nbelongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\\nthat different objects of the same class are not distinguished. For example, all the bicy‐\\ncles on the right side of the segmented image end up as one big lump of pixels. The\\nmain difficulty in this task is that when images go through a regular CNN, they grad‐\\nually lose their spatial resolution (due to the layers with strides greater than 1): so a\\nregular CNN may end up knowing that there’s a person in the image, somewhere in\\nthe bottom left of the image, but it will not be much more precise than that.\\n478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}, {'id': 298, 'title': 'Exercises', 'content': '32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\\nsorts of architectures popping out every year, all based on Convolutional Neural Net‐\\nworks. The progress made in just a few years has been astounding, and researchers\\nare now focusing on harder and harder problems, such as adversarial learning  (which\\nattempts to make the network more resistant to images designed to fool it), explaina‐\\nbility (understanding why the network makes a specific classification), realistic image\\ngeneration  (which we will come back to in ???), single-shot learning  (a system that can\\nrecognize an object after it has seen it just once), and much more. Some even explore\\ncompletely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\\nsented them in a couple videos , with the corresponding code in a notebook). Now on\\nto the next chapter, where we will look at how to process sequential data such as time\\nseries using Recurrent Neural Networks and Convolutional Neural Networks.\\nExercises\\n1.What are the advantages of a CNN over a fully connected DNN for image classi‐\\nfication?\\n2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\\na stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\\nimages of 200 × 300 pixels. What is the total number of parameters in the CNN?\\nIf we are using 32-bit floats, at least how much RAM will this network require\\nwhen making a prediction for a single instance? What about when training on a\\nmini-batch of 50 images?\\n3.If your GPU runs out of memory while training a CNN, what are five things you\\ncould try to solve the problem?\\n4.Why would you want to add a max pooling layer rather than a convolutional\\nlayer with the same stride?\\n5.When would you want to add a local response normalization  layer?\\n6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\\nabout the main innovations in GoogLeNet, ResNet, SENet and Xception?\\n7.What is a Fully Convolutional Network? How can you convert a dense layer into\\na convolutional layer?\\n8.What is the main technical difficulty of semantic segmentation?\\n9.Build your own CNN from scratch and try to achieve the highest possible accu‐\\nracy on MNIST.\\n482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks', 'children': []}]}]}, {'id': 299, 'title': 'About the Author', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}, {'id': 300, 'title': 'Colophon', 'content': 'About the Author\\nAurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\\nTube video classification team from 2013 to 2016. He was also a founder and CTO of\\nWifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\\nof Polyconseil in 2001, the firm that now manages the electric car sharing service\\nAutolib’ .\\nBefore this he worked as an engineer in a variety of domains: finance (JP Morgan and\\nSociété Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\\npublished a few technical books (on C++, WiFi, and internet architectures), and was\\na Computer Science lecturer in a French engineering school.\\nA few fun facts: he taught his three children to count in binary with their fingers (up\\nto 1023), he studied microbiology and evolutionary genetics before going into soft‐\\nware engineering, and his parachute didn’t open on the second jump.\\nColophon\\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\\nsorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\\nmost of Europe. Its black, glossy skin features large yellow spots on the head and\\nback, signaling the presence of alkaloid toxins. This is a possible source of this\\namphibian’s common name: contact with these toxins (which they can also spray\\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\\nor the moistness of the salamander’s skin (or both) led to a misguided belief that these\\ncreatures not only could survive being placed in fire but could extinguish it as well.\\nFire salamanders live in shaded forests, hiding in moist crevices and under logs near\\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\\nmost of their life on land, they give birth to their young in water. They subsist mostly\\non a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\\nin length, and in captivity, may live as long as 50 years.\\nThe fire salamander’s numbers have been reduced by destruction of their forest habi‐\\ntat and capture for the pet trade, but the greatest threat is the susceptibility of their\\nmoisture-permeable skin to pollutants and microbes. Since 2014, they have become\\nextinct in parts of the Netherlands and Belgium due to an introduced fungus.\\nMany of the animals on O’Reilly covers are endangered; all of them are important to\\nthe world. To learn more about how you can help, go to animals.oreilly.com .\\nThe cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\\nTypewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\\nis Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'children': []}]}: 'dict' object has no attribute 'children'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_bert_embeddings(tree_root, tokenizer, model):\n",
    "    node_to_embedding = {}\n",
    "\n",
    "    def traverse_tree(node):\n",
    "        # Log the node being processed\n",
    "        logging.info(f\"Processing node: {node}\")\n",
    "        \n",
    "        try:\n",
    "            if hasattr(node, 'content') and node.content:\n",
    "                inputs = tokenizer(node.content, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                if hasattr(node, 'id'):\n",
    "                    logging.info(f\"Node ID: {node.id}, Content: {node.content[:30]}...\")  # Log a portion of the content for readability\n",
    "                    node_to_embedding[node.id] = outputs.last_hidden_state.mean(dim=1).squeeze().detach().numpy()\n",
    "                else:\n",
    "                    logging.warning(f\"Node {node} does not have an 'id' attribute.\")\n",
    "            else:\n",
    "                logging.warning(f\"Node {node} does not have 'content' or content is empty.\")\n",
    "\n",
    "            for child in node.children:\n",
    "                traverse_tree(child)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing node {node}: {e}\")\n",
    "\n",
    "    traverse_tree(tree_root)\n",
    "    return node_to_embedding\n",
    "\n",
    "def generate_dpr_embeddings(tree_root, dpr_tokenizer, dpr_model):\n",
    "    node_to_embedding = {}\n",
    "\n",
    "    def traverse_tree(node):\n",
    "        # Log the node being processed\n",
    "        logging.info(f\"Processing node: {node}\")\n",
    "\n",
    "        try:\n",
    "            if hasattr(node, 'content') and node.content:\n",
    "                inputs = dpr_tokenizer(node.content, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "                with torch.no_grad():\n",
    "                    outputs = dpr_model(**inputs)\n",
    "                if hasattr(node, 'id'):\n",
    "                    logging.info(f\"Node ID: {node.id}, Content: {node.content[:30]}...\")  # Log a portion of the content for readability\n",
    "                    node_to_embedding[node.id] = outputs.pooler_output.squeeze().detach().numpy()\n",
    "                else:\n",
    "                    logging.warning(f\"Node {node} does not have an 'id' attribute.\")\n",
    "            else:\n",
    "                logging.warning(f\"Node {node} does not have 'content' or content is empty.\")\n",
    "\n",
    "            for child in node.children:\n",
    "                traverse_tree(child)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing node {node}: {e}\")\n",
    "\n",
    "    traverse_tree(tree_root)\n",
    "    return node_to_embedding\n",
    "\n",
    "# Example of setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Assuming tree_root, tokenizer, model, dpr_tokenizer, and dpr_model are defined elsewhere\n",
    "# bert_embeddings = generate_bert_embeddings(tree_root, tokenizer, model)\n",
    "# dpr_embeddings = generate_dpr_embeddings(tree_root, dpr_tokenizer, dpr_model)\n",
    "file_path=r\"hierarchical_tree.json\"\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize BERT model and tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Initialize DPR model and tokenizer\n",
    "dpr_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "dpr_model = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "# Generate embeddings\n",
    "bert_embeddings = generate_bert_embeddings(data, bert_tokenizer, bert_model)\n",
    "dpr_embeddings = generate_dpr_embeddings(data, dpr_tokenizer, dpr_model)\n",
    "\n",
    "# Verify if embeddings are generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path=r\"E:\\aaaaa\\bbot1\\hierarchical_tree.json\"\n",
    "with open(file_path, 'r',encoding='utf-8') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:0\n"
     ]
    }
   ],
   "source": [
    "id=data['id']\n",
    "print(f\"id:{id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 19:33:28,283 - INFO - Processing node with ID: 0\n",
      "2024-07-26 19:33:28,283 - INFO - Updated JSON data saved to updated_hierarchical_tree.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def replace_empty_content(tree_root, replacement_value=\"alpha\"):\n",
    "    def traverse_and_replace(node):\n",
    "        logging.info(f\"Processing node with ID: {node.get('id', 'No ID')}\")\n",
    "\n",
    "        try:\n",
    "            # Check if 'content' is empty or None\n",
    "            if not node.get('content'):\n",
    "                logging.info(f\"Node with ID {node.get('id', 'No ID')} has empty 'content'. Replacing with '{replacement_value}'.\")\n",
    "                node['content'] = \"No content\"\n",
    "            \n",
    "            # Traverse children if any\n",
    "            children = node.get('children', [])\n",
    "            for child in children:\n",
    "                traverse_and_replace(child)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing node with ID {node.get('id', 'No ID')}: {e}\")\n",
    "\n",
    "    traverse_and_replace(tree_root)\n",
    "\n",
    "# Load JSON data\n",
    "file_path = r'E:\\aaaaa\\bbot1\\hierarchical_tree.json'  # Update with your actual file path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Replace empty content\n",
    "replace_empty_content(data)\n",
    "\n",
    "# Save the updated JSON data\n",
    "output_file_path = 'updated_hierarchical_tree.json'  # Update with your desired output file path\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "logging.info(f\"Updated JSON data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_toc_from_file(file_path):\n",
    "    toc = []\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "                \n",
    "                # Split by comma and handle lines that might have extra commas or quotes\n",
    "                parts = line.split(',')\n",
    "                \n",
    "                if len(parts) < 3:  # Skip lines with fewer than 3 parts\n",
    "                    print(f\"Skipping invalid line (too few parts): {line}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract level, title, and page number\n",
    "                level_str = parts[0].strip()\n",
    "                title = ','.join(parts[1:-1]).strip()  # Join any additional commas in the title\n",
    "                page_num_str = parts[-1].strip()\n",
    "                \n",
    "                try:\n",
    "                    level = int(level_str)\n",
    "                    page_num = int(page_num_str)\n",
    "                    toc.append((level, title, page_num))\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error processing line '{line}': {e}\")\n",
    "                    continue\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    return toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid line (too few parts): 2,\n",
      "Skipping invalid line (too few parts): Chapter 14. Deep Computer Vision Using Convolutional Neural Networks,\n",
      "Skipping invalid line (too few parts): 457\n"
     ]
    }
   ],
   "source": [
    "toc_path = \"uploads/toc.txt\"\n",
    "\n",
    "toc = read_toc_from_file(toc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Cover', 1),\n",
       " (1, 'Copyright', 4),\n",
       " (1, 'Table of Contents', 5),\n",
       " (1, 'Preface', 13),\n",
       " (2, 'The Machine Learning Tsunami', 13),\n",
       " (2, 'Machine Learning in Your Projects', 13),\n",
       " (2, 'Objective and Approach', 14),\n",
       " (2, 'Prerequisites', 15),\n",
       " (2, 'Roadmap', 15),\n",
       " (2, 'Other Resources', 17),\n",
       " (2, 'Conventions Used in This Book', 18),\n",
       " (2, 'Code Examples', 19),\n",
       " (2, 'Using Code Examples', 20),\n",
       " (2, 'O’Reilly Safari', 20),\n",
       " (2, 'How to Contact Us', 20),\n",
       " (2, 'Changes in the Second Edition', 21),\n",
       " (2, 'Acknowledgments', 25),\n",
       " (1, 'Part I. The Fundamentals of Machine Learning', 27),\n",
       " (2, 'Chapter 1. The Machine Learning Landscape', 29),\n",
       " (3, 'What Is Machine Learning?', 30),\n",
       " (3, 'Why Use Machine Learning?', 30),\n",
       " (3, 'Types of Machine Learning Systems', 34),\n",
       " (4, 'Supervised/Unsupervised Learning', 34),\n",
       " (4, 'Batch and Online Learning', 41),\n",
       " (4, 'Instance-Based Versus Model-Based Learning', 44),\n",
       " (3, 'Main Challenges of Machine Learning', 50),\n",
       " (4, 'Insufficient Quantity of Training Data', 50),\n",
       " (4, 'Nonrepresentative Training Data', 52),\n",
       " (4, 'Poor-Quality Data', 53),\n",
       " (4, 'Irrelevant Features', 53),\n",
       " (4, 'Overfitting the Training Data', 54),\n",
       " (4, 'Underfitting the Training Data', 56),\n",
       " (4, 'Stepping Back', 56),\n",
       " (3, 'Testing and Validating', 57),\n",
       " (4, 'Hyperparameter Tuning and Model Selection', 58),\n",
       " (4, 'Data Mismatch', 59),\n",
       " (3, 'Exercises', 60),\n",
       " (2, 'Chapter 2. End-to-End Machine Learning Project', 63),\n",
       " (3, 'Working with Real Data', 64),\n",
       " (3, 'Look at the Big Picture', 65),\n",
       " (4, 'Frame the Problem', 65),\n",
       " (4, 'Select a Performance Measure', 68),\n",
       " (4, 'Check the Assumptions', 71),\n",
       " (3, 'Get the Data', 71),\n",
       " (4, 'Create the Workspace', 71),\n",
       " (4, 'Download the Data', 75),\n",
       " (4, 'Take a Quick Look at the Data Structure', 76),\n",
       " (4, 'Create a Test Set', 80),\n",
       " (3, 'Discover and Visualize the Data to Gain Insights', 84),\n",
       " (4, 'Visualizing Geographical Data', 85),\n",
       " (4, 'Looking for Correlations', 88),\n",
       " (4, 'Experimenting with Attribute Combinations', 91),\n",
       " (3, 'Prepare the Data for Machine Learning Algorithms', 92),\n",
       " (4, 'Data Cleaning', 93),\n",
       " (4, 'Handling Text and Categorical Attributes', 95),\n",
       " (4, 'Custom Transformers', 97),\n",
       " (4, 'Feature Scaling', 98),\n",
       " (4, 'Transformation Pipelines', 99),\n",
       " (3, 'Select and Train a Model', 101),\n",
       " (4, 'Training and Evaluating on the Training Set', 101),\n",
       " (4, 'Better Evaluation Using Cross-Validation', 102),\n",
       " (3, 'Fine-Tune Your Model', 105),\n",
       " (4, 'Grid Search', 105),\n",
       " (4, 'Randomized Search', 107),\n",
       " (4, 'Ensemble Methods', 108),\n",
       " (4, 'Analyze the Best Models and Their Errors', 108),\n",
       " (4, 'Evaluate Your System on the Test Set', 109),\n",
       " (3, 'Launch, Monitor, and Maintain Your System', 110),\n",
       " (3, 'Try It Out!', 111),\n",
       " (3, 'Exercises', 111),\n",
       " (2, 'Chapter 3. Classification', 113),\n",
       " (3, 'MNIST', 113),\n",
       " (3, 'Training a Binary Classifier', 116),\n",
       " (3, 'Performance Measures', 116),\n",
       " (4, 'Measuring Accuracy Using Cross-Validation', 117),\n",
       " (4, 'Confusion Matrix', 118),\n",
       " (4, 'Precision and Recall', 120),\n",
       " (4, 'Precision/Recall Tradeoff', 121),\n",
       " (4, 'The ROC Curve', 125),\n",
       " (3, 'Multiclass Classification', 128),\n",
       " (3, 'Error Analysis', 130),\n",
       " (3, 'Multilabel Classification', 134),\n",
       " (3, 'Multioutput Classification', 135),\n",
       " (3, 'Exercises', 136),\n",
       " (2, 'Chapter 4. Training Models', 139),\n",
       " (3, 'Linear Regression', 140),\n",
       " (4, 'The Normal Equation', 142),\n",
       " (4, 'Computational Complexity', 145),\n",
       " (3, 'Gradient Descent', 145),\n",
       " (4, 'Batch Gradient Descent', 149),\n",
       " (4, 'Stochastic Gradient Descent', 152),\n",
       " (4, 'Mini-batch Gradient Descent', 155),\n",
       " (3, 'Polynomial Regression', 156),\n",
       " (3, 'Learning Curves', 158),\n",
       " (3, 'Regularized Linear Models', 162),\n",
       " (4, 'Ridge Regression', 163),\n",
       " (4, 'Lasso Regression', 165),\n",
       " (4, 'Elastic Net', 168),\n",
       " (4, 'Early Stopping', 168),\n",
       " (3, 'Logistic Regression', 170),\n",
       " (4, 'Estimating Probabilities', 170),\n",
       " (4, 'Training and Cost Function', 171),\n",
       " (4, 'Decision Boundaries', 172),\n",
       " (4, 'Softmax Regression', 175),\n",
       " (3, 'Exercises', 179),\n",
       " (2, 'Chapter 5. Support Vector Machines', 181),\n",
       " (3, 'Linear SVM Classification', 181),\n",
       " (4, 'Soft Margin Classification', 182),\n",
       " (3, 'Nonlinear SVM Classification', 185),\n",
       " (4, 'Polynomial Kernel', 186),\n",
       " (4, 'Adding Similarity Features', 187),\n",
       " (4, 'Gaussian RBF Kernel', 188),\n",
       " (4, 'Computational Complexity', 189),\n",
       " (3, 'SVM Regression', 190),\n",
       " (3, 'Under the Hood', 192),\n",
       " (4, 'Decision Function and Predictions', 192),\n",
       " (4, 'Training Objective', 193),\n",
       " (4, 'Quadratic Programming', 195),\n",
       " (4, 'The Dual Problem', 196),\n",
       " (4, 'Kernelized SVM', 197),\n",
       " (4, 'Online SVMs', 200),\n",
       " (3, 'Exercises', 201),\n",
       " (2, 'Chapter 6. Decision Trees', 203),\n",
       " (3, 'Training and Visualizing a Decision Tree', 203),\n",
       " (3, 'Making Predictions', 205),\n",
       " (3, 'Estimating Class Probabilities', 207),\n",
       " (3, 'The CART Training Algorithm', 208),\n",
       " (3, 'Computational Complexity', 209),\n",
       " (3, 'Gini Impurity or Entropy?', 209),\n",
       " (3, 'Regularization Hyperparameters', 210),\n",
       " (3, 'Regression', 211),\n",
       " (3, 'Instability', 214),\n",
       " (3, 'Exercises', 215),\n",
       " (2, 'Chapter 7. Ensemble Learning and Random Forests', 217),\n",
       " (3, 'Voting Classifiers', 218),\n",
       " (3, 'Bagging and Pasting', 221),\n",
       " (4, 'Bagging and Pasting in Scikit-Learn', 222),\n",
       " (4, 'Out-of-Bag Evaluation', 223),\n",
       " (3, 'Random Patches and Random Subspaces', 224),\n",
       " (3, 'Random Forests', 225),\n",
       " (4, 'Extra-Trees', 226),\n",
       " (4, 'Feature Importance', 226),\n",
       " (3, 'Boosting', 227),\n",
       " (4, 'AdaBoost', 228),\n",
       " (4, 'Gradient Boosting', 231),\n",
       " (3, 'Stacking', 236),\n",
       " (3, 'Exercises', 239),\n",
       " (2, 'Chapter 8. Dimensionality Reduction', 241),\n",
       " (3, 'The Curse of Dimensionality', 242),\n",
       " (3, 'Main Approaches for Dimensionality Reduction', 244),\n",
       " (4, 'Projection', 244),\n",
       " (4, 'Manifold Learning', 246),\n",
       " (3, 'PCA', 248),\n",
       " (4, 'Preserving the Variance', 248),\n",
       " (4, 'Principal Components', 249),\n",
       " (4, 'Projecting Down to d Dimensions', 250),\n",
       " (4, 'Using Scikit-Learn', 250),\n",
       " (4, 'Explained Variance Ratio', 251),\n",
       " (4, 'Choosing the Right Number of Dimensions', 251),\n",
       " (4, 'PCA for Compression', 252),\n",
       " (4, 'Randomized PCA', 253),\n",
       " (4, 'Incremental PCA', 253),\n",
       " (3, 'Kernel PCA', 254),\n",
       " (4, 'Selecting a Kernel and Tuning Hyperparameters', 255),\n",
       " (3, 'LLE', 258),\n",
       " (3, 'Other Dimensionality Reduction Techniques', 260),\n",
       " (3, 'Exercises', 261),\n",
       " (2, 'Chapter 9. Unsupervised Learning Techniques', 263),\n",
       " (3, 'Clustering', 264),\n",
       " (4, 'K-Means', 266),\n",
       " (4, 'Limits of K-Means', 276),\n",
       " (4, 'Using clustering for image segmentation', 277),\n",
       " (4, 'Using Clustering for Preprocessing', 278),\n",
       " (4, 'Using Clustering for Semi-Supervised Learning', 280),\n",
       " (4, 'DBSCAN', 282),\n",
       " (4, 'Other Clustering Algorithms', 285),\n",
       " (3, 'Gaussian Mixtures', 286),\n",
       " (4, 'Anomaly Detection using Gaussian Mixtures', 292),\n",
       " (4, 'Selecting the Number of Clusters', 293),\n",
       " (4, 'Bayesian Gaussian Mixture Models', 296),\n",
       " (4, 'Other Anomaly Detection and Novelty Detection Algorithms', 300),\n",
       " (1, 'Part II. Neural Networks and Deep Learning', 301),\n",
       " (2, 'Chapter 10. Introduction to Artificial Neural Networks with Keras', 303),\n",
       " (3, 'From Biological to Artificial Neurons', 304),\n",
       " (4, 'Biological Neurons', 305),\n",
       " (4, 'Logical Computations with Neurons', 307),\n",
       " (4, 'The Perceptron', 307),\n",
       " (4, 'Multi-Layer Perceptron and Backpropagation', 312),\n",
       " (4, 'Regression MLPs', 315),\n",
       " (4, 'Classification MLPs', 316),\n",
       " (3, 'Implementing MLPs with Keras', 318),\n",
       " (4, 'Installing TensorFlow\\\\xa02', 319),\n",
       " (4, 'Building an Image Classifier Using the Sequential API', 320),\n",
       " (4, 'Building a Regression MLP Using the Sequential API', 329),\n",
       " (4, 'Building Complex Models Using the Functional API', 330),\n",
       " (4, 'Building Dynamic Models Using the Subclassing API', 335),\n",
       " (4, 'Saving and Restoring a Model', 337),\n",
       " (4, 'Using Callbacks', 337),\n",
       " (4, 'Visualization Using TensorBoard', 339),\n",
       " (3, 'Fine-Tuning Neural Network Hyperparameters', 341),\n",
       " (4, 'Number of Hidden Layers', 345),\n",
       " (4, 'Number of Neurons per Hidden Layer', 346),\n",
       " (4, 'Learning Rate, Batch Size and Other Hyperparameters', 346),\n",
       " (3, 'Exercises', 348),\n",
       " (2, 'Chapter 11. Training Deep Neural Networks', 351),\n",
       " (3, 'Vanishing/Exploding Gradients Problems', 352),\n",
       " (4, 'Glorot and He Initialization', 353),\n",
       " (4, 'Nonsaturating Activation Functions', 355),\n",
       " (4, 'Batch Normalization', 359),\n",
       " (4, 'Gradient Clipping', 364),\n",
       " (3, 'Reusing Pretrained Layers', 365),\n",
       " (4, 'Transfer Learning With Keras', 367),\n",
       " (4, 'Unsupervised Pretraining', 369),\n",
       " (4, 'Pretraining on an Auxiliary Task', 370),\n",
       " (3, 'Faster Optimizers', 370),\n",
       " (4, 'Momentum Optimization', 371),\n",
       " (4, 'Nesterov Accelerated Gradient', 372),\n",
       " (4, 'AdaGrad', 373),\n",
       " (4, 'RMSProp', 375),\n",
       " (4, 'Adam and Nadam Optimization', 375),\n",
       " (4, 'Learning Rate Scheduling', 378),\n",
       " (3, 'Avoiding Overfitting Through Regularization', 382),\n",
       " (4, 'ℓ1 and ℓ2 Regularization', 382),\n",
       " (4, 'Dropout', 383),\n",
       " (4, 'Max-Norm Regularization', 388),\n",
       " (3, 'Summary and Practical Guidelines', 389),\n",
       " (3, 'Exercises', 390),\n",
       " (2, 'Chapter 12. Custom Models and Training with TensorFlow', 393),\n",
       " (3, 'A Quick Tour of TensorFlow', 394),\n",
       " (3, 'Using TensorFlow like NumPy', 397),\n",
       " (4, 'Tensors and Operations', 397),\n",
       " (4, 'Tensors and NumPy', 399),\n",
       " (4, 'Type Conversions', 400),\n",
       " (4, 'Variables', 400),\n",
       " (4, 'Other Data Structures', 401),\n",
       " (3, 'Customizing Models and Training Algorithms', 402),\n",
       " (4, 'Custom Loss Functions', 402),\n",
       " (4, 'Saving and Loading Models That Contain Custom Components', 403),\n",
       " (4,\n",
       "  'Custom Activation Functions, Initializers, Regularizers, and Constraints',\n",
       "  405),\n",
       " (4, 'Custom Metrics', 406),\n",
       " (4, 'Custom Layers', 409),\n",
       " (4, 'Custom Models', 412),\n",
       " (4, 'Losses and Metrics Based on Model Internals', 414),\n",
       " (4, 'Computing Gradients Using Autodiff', 415),\n",
       " (4, 'Custom Training Loops', 419),\n",
       " (3, 'TensorFlow Functions and Graphs', 422),\n",
       " (4, 'Autograph and Tracing', 424),\n",
       " (4, 'TF Function Rules', 426),\n",
       " (2, 'Chapter 13. Loading and Preprocessing Data with TensorFlow', 429),\n",
       " (3, 'The Data API', 430),\n",
       " (4, 'Chaining Transformations', 431),\n",
       " (4, 'Shuffling the Data', 432),\n",
       " (4, 'Preprocessing the Data', 435),\n",
       " (4, 'Putting Everything Together', 436),\n",
       " (4, 'Prefetching', 437),\n",
       " (4, 'Using the Dataset With tf.keras', 439),\n",
       " (3, 'The TFRecord Format', 440),\n",
       " (4, 'Compressed TFRecord Files', 441),\n",
       " (4, 'A Brief Introduction to Protocol Buffers', 441),\n",
       " (4, 'TensorFlow Protobufs', 442),\n",
       " (4, 'Loading and Parsing Examples', 444),\n",
       " (4, 'Handling Lists of Lists Using the SequenceExample Protobuf', 445),\n",
       " (3, 'The Features API', 446),\n",
       " (4, 'Categorical Features', 447),\n",
       " (4, 'Crossed Categorical Features', 447),\n",
       " (4, 'Encoding Categorical Features Using One-Hot Vectors', 448),\n",
       " (4, 'Encoding Categorical Features Using Embeddings', 449),\n",
       " (4, 'Using Feature Columns for Parsing', 452),\n",
       " (4, 'Using Feature Columns in Your Models', 452),\n",
       " (3, 'TF Transform', 454),\n",
       " (3, 'The TensorFlow Datasets (TFDS) Project', 455),\n",
       " (3, 'The Architecture of the Visual Cortex', 458),\n",
       " (3, 'Convolutional Layer', 460),\n",
       " (4, 'Filters', 462),\n",
       " (4, 'Stacking Multiple Feature Maps', 463),\n",
       " (4, 'TensorFlow Implementation', 465),\n",
       " (4, 'Memory Requirements', 467),\n",
       " (3, 'Pooling Layer', 468),\n",
       " (4, 'TensorFlow Implementation', 470),\n",
       " (3, 'CNN Architectures', 472),\n",
       " (4, 'LeNet-5', 475),\n",
       " (4, 'AlexNet', 476),\n",
       " (4, 'GoogLeNet', 478),\n",
       " (4, 'VGGNet', 482),\n",
       " (4, 'ResNet', 483),\n",
       " (4, 'Xception', 485),\n",
       " (4, 'SENet', 487),\n",
       " (3, 'Implementing a ResNet-34 CNN Using Keras', 490),\n",
       " (3, 'Using Pretrained Models From Keras', 491),\n",
       " (3, 'Pretrained Models for Transfer Learning', 493),\n",
       " (3, 'Classification and Localization', 495),\n",
       " (3, 'Object Detection', 497),\n",
       " (4, 'Fully Convolutional Networks (FCNs)', 499),\n",
       " (4, 'You Only Look Once (YOLO)', 501),\n",
       " (3, 'Semantic Segmentation', 504),\n",
       " (3, 'Exercises', 508),\n",
       " (1, 'About the Author', 510),\n",
       " (1, 'Colophon', 510)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
