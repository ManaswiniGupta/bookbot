
# Example ToC (level, title, page_num)
toc=[(1, "Cover", 1),
 (1, "Copyright", 4),
 (1, "Table of Contents", 5),
 (1, "Preface", 13),
 (2, "The Machine Learning Tsunami", 13),
 (2, "Machine Learning in Your Projects", 13),
 (2, "Objective and Approach", 14),
 (2, "Prerequisites", 15),
 (2, "Roadmap", 15),
 (2, "Other Resources", 17),
 (2, "Conventions Used in This Book", 18),
 (2, "Code Examples", 19),
 (2, "Using Code Examples", 20),
 (2, "O'Reilly Safari", 20),
 (2, "How to Contact Us", 20),
 (2, "Changes in the Second Edition", 21),
 (2, "Acknowledgments", 25),
 (1, "Part I. The Fundamentals of Machine Learning", 27),
 (2, "Chapter 1. The Machine Learning Landscape", 29),
 (3, "What Is Machine Learning?", 30),
 (3, "Why Use Machine Learning?", 30),
 (3, "Types of Machine Learning Systems", 34),
 (4, "Supervised/Unsupervised Learning", 34),
 (4, "Batch and Online Learning", 41),
 (4, "Instance-Based Versus Model-Based Learning", 44),
 (3, "Main Challenges of Machine Learning", 50),
 (4, "Insufficient Quantity of Training Data", 50),
 (4, "Nonrepresentative Training Data", 52),
 (4, "Poor-Quality Data", 53),
 (4, "Irrelevant Features", 53),
 (4, "Overfitting the Training Data", 54),
 (4, "Underfitting the Training Data", 56),
 (4, "Stepping Back", 56),
 (3, "Testing and Validating", 57),
 (4, "Hyperparameter Tuning and Model Selection", 58),
 (4, "Data Mismatch", 59),
 (3, "Exercises", 60),
 (2, "Chapter 2. End-to-End Machine Learning Project", 63),
 (3, "Working with Real Data", 64),
 (3, "Look at the Big Picture", 65),
 (4, "Frame the Problem", 65),
 (4, "Select a Performance Measure", 68),
 (4, "Check the Assumptions", 71),
 (3, "Get the Data", 71),
 (4, "Create the Workspace", 71),
 (4, "Download the Data", 75),
 (4, "Take a Quick Look at the Data Structure", 76),
 (4, "Create a Test Set", 80),
 (3, "Discover and Visualize the Data to Gain Insights", 84),
 (4, "Visualizing Geographical Data", 85),
 (4, "Looking for Correlations", 88),
 (4, "Experimenting with Attribute Combinations", 91),
 (3, "Prepare the Data for Machine Learning Algorithms", 92),
 (4, "Data Cleaning", 93),
 (4, "Handling Text and Categorical Attributes", 95),
 (4, "Custom Transformers", 97),
 (4, "Feature Scaling", 98),
 (4, "Transformation Pipelines", 99),
 (3, "Select and Train a Model", 101),
 (4, "Training and Evaluating on the Training Set", 101),
 (4, "Better Evaluation Using Cross-Validation", 102),
 (3, "Fine-Tune Your Model", 105),
 (4, "Grid Search", 105),
 (4, "Randomized Search", 107),
 (4, "Ensemble Methods", 108),
 (4, "Analyze the Best Models and Their Errors", 108),
 (4, "Evaluate Your System on the Test Set", 109),
 (3, "Launch, Monitor, and Maintain Your System", 110),
 (3, "Try It Out!", 111),
 (3, "Exercises", 111),
 (2, "Chapter 3. Classification", 113),
 (3, "MNIST", 113),
 (3, "Training a Binary Classifier", 116),
 (3, "Performance Measures", 116),
 (4, "Measuring Accuracy Using Cross-Validation", 117),
 (4, "Confusion Matrix", 118),
 (4, "Precision and Recall", 120),
 (4, "Precision/Recall Tradeoff", 121),
 (4, "The ROC Curve", 125),
 (3, "Multiclass Classification", 128),
 (3, "Error Analysis", 130),
 (3, "Multilabel Classification", 134),
 (3, "Multioutput Classification", 135),
 (3, "Exercises", 136),
 (2, "Chapter 4. Training Models", 139),
 (3, "Linear Regression", 140),
 (4, "The Normal Equation", 142),
 (4, "Computational Complexity", 145),
 (3, "Gradient Descent", 145),
 (4, "Batch Gradient Descent", 149),
 (4, "Stochastic Gradient Descent", 152),
 (4, "Mini-batch Gradient Descent", 155),
 (3, "Polynomial Regression", 156),
 (3, "Learning Curves", 158),
 (3, "Regularized Linear Models", 162),
 (4, "Ridge Regression", 163),
 (4, "Lasso Regression", 165),
 (4, "Elastic Net", 168),
 (4, "Early Stopping", 168),
 (3, "Logistic Regression", 170),
 (4, "Estimating Probabilities", 170),
 (4, "Training and Cost Function", 171),
 (4, "Decision Boundaries", 172),
 (4, "Softmax Regression", 175),
 (3, "Exercises", 179),
 (2, "Chapter 5. Support Vector Machines", 181),
 (3, "Linear SVM Classification", 181),
 (4, "Soft Margin Classification", 182),
 (3, "Nonlinear SVM Classification", 185),
 (4, "Polynomial Kernel", 186),
 (4, "Adding Similarity Features", 187),
 (4, "Gaussian RBF Kernel", 188),
 (4, "Computational Complexity", 189),
 (3, "SVM Regression", 190),
 (3, "Under the Hood", 192),
 (4, "Decision Function and Predictions", 192),
 (4, "Training Objective", 193),
 (4, "Quadratic Programming", 195),
 (4, "The Dual Problem", 196),
 (4, "Kernelized SVM", 197),
 (4, "Online SVMs", 200),
 (3, "Exercises", 201),
 (2, "Chapter 6. Decision Trees", 203),
 (3, "Training and Visualizing a Decision Tree", 203),
 (3, "Making Predictions", 205),
 (3, "Estimating Class Probabilities", 207),
 (3, "The CART Training Algorithm", 208),
 (3, "Computational Complexity", 209),
 (3, "Gini Impurity or Entropy?", 209),
 (3, "Regularization Hyperparameters", 210),
 (3, "Regression", 211),
 (3, "Instability", 214),
 (3, "Exercises", 215),
 (2, "Chapter 7. Ensemble Learning and Random Forests", 217),
 (3, "Voting Classifiers", 218),
 (3, "Bagging and Pasting", 221),
 (4, "Bagging and Pasting in Scikit-Learn", 222),
 (4, "Out-of-Bag Evaluation", 223),
 (3, "Random Patches and Random Subspaces", 224),
 (3, "Random Forests", 225),
 (4, "Extra-Trees", 226),
 (4, "Feature Importance", 226),
 (3, "Boosting", 227),
 (4, "AdaBoost", 228),
 (4, "Gradient Boosting", 231),
 (3, "Stacking", 236),
 (3, "Exercises", 239),
 (2, "Chapter 8. Dimensionality Reduction", 241),
 (3, "The Curse of Dimensionality", 242),
 (3, "Main Approaches for Dimensionality Reduction", 244),
 (4, "Projection", 244),
 (4, "Manifold Learning", 246),
 (3, "PCA", 248),
 (4, "Preserving the Variance", 248),
 (4, "Principal Components", 249),
 (4, "Projecting Down to d Dimensions", 250),
 (4, "Using Scikit-Learn", 250),
 (4, "Explained Variance Ratio", 251),
 (4, "Choosing the Right Number of Dimensions", 251),
 (4, "PCA for Compression", 252),
 (4, "Randomized PCA", 253),
 (4, "Incremental PCA", 253),
 (3, "Kernel PCA", 254),
 (4, "Selecting a Kernel and Tuning Hyperparameters", 255),
 (3, "LLE", 258),
 (3, "Other Dimensionality Reduction Techniques", 260),
 (3, "Exercises", 261),
 (2, "Chapter 9. Unsupervised Learning Techniques", 263),
 (3, "Clustering", 264),
 (4, "K-Means", 266),
 (4, "Limits of K-Means", 276),
 (4, "Using clustering for image segmentation", 277),
 (4, "Using Clustering for Preprocessing", 278),
 (4, "Using Clustering for Semi-Supervised Learning", 280),
 (4, "DBSCAN", 282),
 (4, "Other Clustering Algorithms", 285),
 (3, "Gaussian Mixtures", 286),
 (4, "Anomaly Detection using Gaussian Mixtures", 292),
 (4, "Selecting the Number of Clusters", 293),
 (4, "Bayesian Gaussian Mixture Models", 296),
 (4, "Other Anomaly Detection and Novelty Detection Algorithms", 300),
 (1, "Part II. Neural Networks and Deep Learning", 301),
 (2, "Chapter 10. Introduction to Artificial Neural Networks with Keras", 303),
 (3, "From Biological to Artificial Neurons", 304),
 (4, "Biological Neurons", 305),
 (4, "Logical Computations with Neurons", 307),
 (4, "The Perceptron", 307),
 (4, "Multi-Layer Perceptron and Backpropagation", 312),
 (4, "Regression MLPs", 315),
 (4, "Classification MLPs", 316),
 (3, "Implementing MLPs with Keras", 318),
 (4, "Installing TensorFlow 2", 319),
 (4, "Building an Image Classifier Using the Sequential API", 320),
 (4, "Building a Regression MLP Using the Sequential API", 329),
 (4, "Building Complex Models Using the Functional API", 330),
 (4, "Building Dynamic Models Using the Subclassing API", 335),
 (4, "Saving and Restoring a Model", 337),
 (4, "Using Callbacks", 337),
 (4, "Visualization Using TensorBoard", 339),
 (3, "Fine-Tuning Neural Network Hyperparameters", 341),
 (4, "Number of Hidden Layers", 345),
 (4, "Number of Neurons per Hidden Layer", 346),
 (4, "Learning Rate, Batch Size and Other Hyperparameters", 346),
 (3, "Exercises", 348),
 (2, "Chapter 11. Training Deep Neural Networks", 351),
 (3, "Vanishing/Exploding Gradients Problems", 352),
 (4, "Glorot and He Initialization", 353),
 (4, "Nonsaturating Activation Functions", 355),
 (4, "Batch Normalization", 359),
 (4, "Gradient Clipping", 364),
 (3, "Reusing Pretrained Layers", 365),
 (4, "Transfer Learning With Keras", 367),
 (4, "Unsupervised Pretraining", 369),
 (4, "Pretraining on an Auxiliary Task", 370),
 (3, "Faster Optimizers", 370),
 (4, "Momentum Optimization", 371),
 (4, "Nesterov Accelerated Gradient", 372),
 (4, "AdaGrad", 373),
 (4, "RMSProp", 375),
 (4, "Adam and Nadam Optimization", 375),
 (4, "Learning Rate Scheduling", 378),
 (3, "Avoiding Overfitting Through Regularization", 382),
 (4, "l1 and l2 Regularization", 382),
 (4, "Dropout", 383),
 (4, "Monte-Carlo (MC) Dropout", 386),
 (4, "Max-Norm Regularization", 388),
 (3, "Summary and Practical Guidelines", 389),
 (3, "Exercises", 390),
 (2, "Chapter 12. Custom Models and Training with TensorFlow", 393),
 (3, "A Quick Tour of TensorFlow", 394),
 (3, "Using TensorFlow like NumPy", 397),
 (4, "Tensors and Operations", 397),
 (4, "Tensors and NumPy", 399),
 (4, "Type Conversions", 400),
 (4, "Variables", 400),
 (4, "Other Data Structures", 401),
 (3, "Customizing Models and Training Algorithms", 402),
 (4, "Custom Loss Functions", 402),
 (4, "Saving and Loading Models That Contain Custom Components", 403),
 (4,"Custom Activation Functions, Initializers, Regularizers, and Constraints",405),
 (4, "Custom Metrics", 406),
 (4, "Custom Layers", 409),
 (4, "Custom Models", 412),
 (4, "Losses and Metrics Based on Model Internals", 414),
 (4, "Computing Gradients Using Autodiff", 415),
 (4, "Custom Training Loops", 419),
 (3, "TensorFlow Functions and Graphs", 422),
 (4, "Autograph and Tracing", 424),
 (4, "TF Function Rules", 426),
 (2, "Chapter 13. Loading and Preprocessing Data with TensorFlow", 429),
 (3, "The Data API", 430),
 (4, "Chaining Transformations", 431),
 (4, "Shuffling the Data", 432),
 (4, "Preprocessing the Data", 435),
 (4, "Putting Everything Together", 436),
 (4, "Prefetching", 437),
 (4, "Using the Dataset With tf.keras", 439),
 (3, "The TFRecord Format", 440),
 (4, "Compressed TFRecord Files", 441),
 (4, "A Brief Introduction to Protocol Buffers", 441),
 (4, "TensorFlow Protobufs", 442),
 (4, "Loading and Parsing Examples", 444),
 (4, "Handling Lists of Lists Using the SequenceExample Protobuf", 445),
 (3, "The Features API", 446),
 (4, "Categorical Features", 447),
 (4, "Crossed Categorical Features", 447),
 (4, "Encoding Categorical Features Using One-Hot Vectors", 448),
 (4, "Encoding Categorical Features Using Embeddings", 449),
 (4, "Using Feature Columns for Parsing", 452),
 (4, "Using Feature Columns in Your Models", 452),
 (3, "TF Transform", 454),
 (3, "The TensorFlow Datasets (TFDS) Project", 455),
 (2,
  "Chapter 14. Deep Computer Vision Using Convolutional Neural Networks",
  457),
 (3, "The Architecture of the Visual Cortex", 458),
 (3, "Convolutional Layer", 460),
 (4, "Filters", 462),
 (4, "Stacking Multiple Feature Maps", 463),
 (4, "TensorFlow Implementation", 465),
 (4, "Memory Requirements", 467),
 (3, "Pooling Layer", 468),
 (4, "TensorFlow Implementation", 470),
 (3, "CNN Architectures", 472),
 (4, "LeNet-5", 475),
 (4, "AlexNet", 476),
 (4, "GoogLeNet", 478),
 (4, "VGGNet", 482),
 (4, "ResNet", 483),
 (4, "Xception", 485),
 (4, "SENet", 487),
 (3, "Implementing a ResNet-34 CNN Using Keras", 490),
 (3, "Using Pretrained Models From Keras", 491),
 (3, "Pretrained Models for Transfer Learning", 493),
 (3, "Classification and Localization", 495),
 (3, "Object Detection", 497),
 (4, "Fully Convolutional Networks (FCNs)", 499),
 (4, "You Only Look Once (YOLO)", 501),
 (3, "Semantic Segmentation", 504),
 (3, "Exercises", 508),
 (1, "About the Author", 510),
 (1, "Colophon", 510)]